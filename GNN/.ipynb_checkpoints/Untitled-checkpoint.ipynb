{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def Diffusion(A, F):\n",
    "    \"\"\"finds the diffusion of F via A\"\"\"\n",
    "    #F can be a batched signal\n",
    "    return tf.batch_matmul(A, F)\n",
    "\n",
    "def Diag(A, F):\n",
    "    \"\"\"multiplies F by diagonal vector\"\"\"\n",
    "    diag_matrix = tf.expand_dims(tf.reduce_sum(Adj, 1), 1)\n",
    "    return tf.mul(diag_matrix, F)\n",
    "\n",
    "def Diffusion_normed(A, F):\n",
    "    \"\"\"D^-1W\"\"\"\n",
    "    return tf.mul(tf.div(1.0, Diag(A, F)), tf.batch_matmul(A, F))\n",
    "\n",
    "\n",
    "def balanced_stochastic_blockmodel(big_community, small_community, p_in=1.0, p_out=0.0, seed=None):\n",
    "    \"\"\"gives dense adjacency matrix representaiton of randomly generated SBM with balanced community size\"\"\"\n",
    "\n",
    "    G = nx.random_partition_graph([big_community, small_community], p_in=p_in, p_out=p_out, seed=seed)\n",
    "    A = nx.adjacency_matrix(G).todense()\n",
    "    \n",
    "    return A\n",
    "\n",
    "def batch_vm2(x, m):\n",
    "    [input_size, output_size] = m.get_shape().as_list()\n",
    "\n",
    "    input_shape = tf.shape(x)\n",
    "    batch_rank = input_shape.get_shape()[0].value - 1\n",
    "    batch_shape = input_shape[:batch_rank]\n",
    "    output_shape = tf.concat(0, [batch_shape, [output_size]])\n",
    "\n",
    "    x = tf.reshape(x, [-1, input_size])\n",
    "    y = tf.matmul(x, m)\n",
    "\n",
    "    y = tf.reshape(y, output_shape)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 0]\n",
      " [1 0 1 1 0 0 0]\n",
      " [1 1 0 1 0 0 0]\n",
      " [1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 1]\n",
      " [0 0 0 0 1 0 1]\n",
      " [0 0 0 0 1 1 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.,  0.]])]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = balanced_stochastic_blockmodel(4, 3, 1.0, 0.0)\n",
    "print a\n",
    "DATA1 = [np.asarray(balanced_stochastic_blockmodel(4, 2, p, 0.1*p)).astype(np.double) for p in np.linspace(0.3, 0.3, 10)]\n",
    "DATA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "###################a\n",
    "\n",
    "\n",
    "\n",
    "def GNN(Size=10, signal_dim = 10, batch_size = 2, big_community=30, small_community=10, \n",
    "    p_min = 0.5, p_max = 0.5, Mean = 1, l_rate = 0.0000001, datapoints=10):\n",
    "\n",
    "\n",
    "    \"\"\" First implement of GNN\"\"\"\n",
    "    dim = big_community+small_community\n",
    "\n",
    "    DATA1 = [np.asarray(balanced_stochastic_blockmodel(big_community, small_community, p, 0.1*p, seed=None)).astype(np.double) for p in np.linspace(p_min, p_max, datapoints)]\n",
    "\n",
    "    DATA = DATA1*(Size//datapoints)\n",
    "    np.random.shuffle(DATA)\n",
    "\n",
    "    \n",
    "    TRUE_A = np.append(np.ones([batch_size, big_community], dtype=float),np.zeros([batch_size, small_community], dtype=float), axis = 1)\n",
    "\n",
    "    Adj = tf.placeholder(dtype=tf.float32, shape=[None, dim, dim])\n",
    "    Adj_mod = tf.reshape(tf.transpose(Adj, perm = [1,0,2]), [dim, batch_size*dim])#preparing it to be multiplied by F to broadcast\n",
    "    \n",
    "    F = tf.placeholder(dtype=tf.float32, shape = [10, big_community+small_community])\n",
    "\n",
    "\n",
    "    #first diffusion step without cascading (unnormed version)\n",
    "    Diff_1 = tf.reshape(tf.transpose(tf.matmul(F, Adj_mod)), shape=[batch_size, dim, signal_dim]) #shape=[batch_size, signal_dim, dim])\n",
    "\n",
    "    diag_inv = tf.div(1.0, tf.reduce_sum(Adj, 2))\n",
    "    diag_inv_batch = tf.matrix_diag(diag_inv) #to use in subsequent layers\n",
    "    Diag_1 = tf.mul(tf.expand_dims(diag_inv, 1), F)\n",
    "\n",
    "\n",
    "    C_a = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "    C_b = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "\n",
    "    #treat this as the new Adj_mod\n",
    "    A1 = tf.matmul(C_a, tf.reshape(tf.transpose(Diag_1, perm=[1, 0,2]), [signal_dim, batch_size*dim]))\n",
    "    B1 = tf.matmul(C_b, tf.reshape(tf.transpose(Diff_1, perm=[2, 0,1]), [signal_dim, batch_size*dim]))\n",
    "\n",
    "    #transform it back into the 3-D tensor it is\n",
    "    #Psi_1 = tf.transpose(tf.reshape(A1 + B1, shape = [signal_dim, batch_size, dim]), perm=[1,0,2])\n",
    "    #relu also added\n",
    "    Psi_1 = tf.transpose(tf.reshape(tf.nn.relu(A1 + B1), shape = [signal_dim, batch_size, dim]), perm=[1,2,0])\n",
    "\n",
    "\n",
    " ###################\n",
    "    ###################\n",
    "    Diff_2 = tf.batch_matmul(Adj, Psi_1)\n",
    "    Diag_2 = tf.batch_matmul(diag_inv_batch, Psi_1)\n",
    "    #we change the constants for now but let's keep these the same for another model\n",
    "    C_a_1 = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "    C_b_1 = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "\n",
    "    A2 = tf.matmul(C_a_1, tf.reshape(tf.transpose(Diag_2, perm=[2, 0,1]), [signal_dim, batch_size*dim]))\n",
    "    B2 = tf.matmul(C_b_1, tf.reshape(tf.transpose(Diff_2, perm=[2, 0,1]), [signal_dim, batch_size*dim]))\n",
    "\n",
    "    Psi_2 = tf.transpose(tf.reshape(tf.nn.relu(A2 + B2), shape = [signal_dim, batch_size, dim]), perm=[1,2,0])\n",
    "\n",
    "    ##################\n",
    "    ##################\n",
    "    Diff_3 = tf.batch_matmul(Adj, Psi_2)\n",
    "    Diag_3 = tf.batch_matmul(diag_inv_batch, Psi_2)\n",
    "\n",
    "    C_a_2 = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "    C_b_2 = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "\n",
    "    A3 = tf.matmul(C_a_2, tf.reshape(tf.transpose(Diag_3, perm=[2, 0,1]), [signal_dim, batch_size*dim]))\n",
    "    B3 = tf.matmul(C_b_2, tf.reshape(tf.transpose(Diff_3, perm=[2, 0,1]), [signal_dim, batch_size*dim]))\n",
    "\n",
    "    Psi_3 = tf.transpose(tf.reshape(tf.nn.relu(A3 + B3), shape = [signal_dim, batch_size, dim]), perm=[1,2,0])\n",
    "\n",
    "    ##################\n",
    "    ##################\n",
    "    Diff_4 = tf.batch_matmul(Adj, Psi_3)\n",
    "    Diag_4 = tf.batch_matmul(diag_inv_batch, Psi_3)\n",
    "\n",
    "    C_a_3 = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "    C_b_3 = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "\n",
    "    A4 = tf.matmul(C_a_3, tf.reshape(tf.transpose(Diag_4, perm=[2, 0,1]), [signal_dim, batch_size*dim]))\n",
    "    B4 = tf.matmul(C_b_3, tf.reshape(tf.transpose(Diff_4, perm=[2, 0,1]), [signal_dim, batch_size*dim]))\n",
    "\n",
    "    Psi_4 = tf.transpose(tf.reshape(tf.nn.relu(A4 + B4), shape = [signal_dim, batch_size, dim]), perm=[1,2,0])\n",
    "\n",
    "\n",
    "    ###################\n",
    "    ###################\n",
    "\n",
    "    B_reduce = tf.Variable(tf.random_normal([signal_dim, 2], stddev=1.0, mean=0.0))\n",
    "\n",
    "    Y_hat = tf.nn.relu(batch_vm2(Psi_4, B_reduce))\n",
    "    test = tf.nn.softmax(Y_hat)\n",
    "    ##################\n",
    "    ################## TRUE ASSIGNMENTlton\n",
    "\n",
    "    #true_assignment_a = tf.expand_dims(tf.concat(0, [tf.zeros([group_size], dtype=tf.float32),\n",
    "     #                                     tf.ones([group_size], dtype=tf.float32)]), 1)\n",
    "    #true_assignment_b = tf.expand_dims(tf.concat(0, [tf.ones([group_size], dtype=tf.float32),\n",
    "     #                                     tf.zeros([group_size], dtype=tf.float32)]), 1)\n",
    "\n",
    "    true_assignment_a= tf.placeholder(dtype=tf.int32, shape = [batch_size, dim])\n",
    "    a = tf.nn.sparse_softmax_cross_entropy_with_logits(Y_hat, true_assignment_a)\n",
    "    \n",
    "    \n",
    "    loss = tf.reduce_sum(a)\n",
    "    optimizer = tf.train.AdamOptimizer(l_rate)\n",
    "    train = optimizer.minimize(loss, var_list=[C_a, C_b, C_a_1, C_a_2, C_a_3, C_b_1, C_b_2, C_b_3, B_reduce])\n",
    "\n",
    "    ##################\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    #init_F = tf.initialize_variables(var_list=[F])\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        iterations = Size/batch_size\n",
    "        \n",
    "        sess.run(init)\n",
    "        loss_lst = [None]*iterations\n",
    "        #Psi_6_lst = []\n",
    "        variable_lst = []\n",
    "        #Cb_list = []\n",
    "        \n",
    "        for i in xrange(iterations):\n",
    "            #sess.run(init_F)\n",
    "            sess.run(train, feed_dict={Adj: DATA[i:i+batch_size], F: Signal, true_assignment_a: TRUE_A})\n",
    "            loss_printed= sess.run(loss, feed_dict={Adj: DATA[i:i+batch_size], F: Signal, true_assignment_a: TRUE_A})\n",
    "            loss_lst[i]=loss_printed\n",
    "\n",
    "            if i%20==0:\n",
    "                print i, \"loss\", loss_printed\n",
    "                d = {\"loss\": loss_lst}\n",
    "                d = pd.DataFrame(d)\n",
    "                d.to_csv(\"~/Desktop/clusternet/GNN/imbalanced_block_data/DATApoints{}Size{}l_rate{}batch_size{}p_min{}p_max{}.csv\".format(datapoints, Size, l_rate, batch_size, p_min, p_max))\n",
    "               \n",
    "            if i==iterations-1:\n",
    "                print \"these are the variables after training:\"\n",
    "                #print \"mean:\", m, \"l rate:\", l, \"Mean_signal:\", n, \n",
    "\n",
    "                f, a, b, a1, a2, a3, b1, b2, b3, b_reduce = sess.run([F, C_a, C_b, C_a_1, C_a_2, C_a_3, C_b_1, C_b_2, C_b_3, B_reduce], \n",
    "                    feed_dict={Adj: DATA[i:i+batch_size], F: Signal, true_assignment_a: TRUE_A})\n",
    "\n",
    "                variable_lst = variable_lst+[f, a, b, a1, a2, a3, b1, b2, b3, b_reduce]\n",
    "                d_var = {\"vars_a_b_ai_bi_b_reduce\": variable_lst, \"header\": [\"F\", \"Ca\", \"Cb\", \"Ca1\", \"Ca2\", \"Ca3\", \n",
    "                \"Cb1\", \"Cb2\", \"Cb3\", \"B_reduce\"]}\n",
    "\n",
    "                d_var = pd.DataFrame(d_var)\n",
    "                d_var.to_csv(\"~/Desktop/clusternet/GNN/imbalanced_block_data//ModelPARAMS_DATApoints{}Size{}l_rate{}batch_size{}p_min{}p_max{}.csv\".format(datapoints, Size, l_rate, batch_size, p_min, p_max))\n",
    "           \n",
    "                #print d_var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "iterated_list = [(b, l, p, d, sizes) for b in [1] for \n",
    "     l in [10**-i for i in [3]] for #xrange(6, 14, 4)] for \n",
    "     p in [0.4] for \n",
    "     d in [2] for\n",
    "     sizes in [(25,15), (30,10), (35, 5)]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.001, 0.4, 2, (25, 15))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterated_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (10, 20) for Tensor u'Placeholder_73:0', which has shape '(10, 40)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-c2d474d2fef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterated_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mSignal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mGNN_seq_2000\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#loss, a, B4, A4, Psi_4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-c2d474d2fef3>\u001b[0m in \u001b[0;36mGNN_seq_2000\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     return GNN(Size=1000, batch_size = v[0], l_rate = v[1], p_min = v[2], p_max = v[2], \n\u001b[0;32m----> 6\u001b[0;31m                datapoints=v[3], big_community=v[4][0], small_community=v[4][1])\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-4ab19bd70004>\u001b[0m in \u001b[0;36mGNN\u001b[0;34m(Size, signal_dim, batch_size, big_community, small_community, p_min, p_max, Mean, l_rate, datapoints)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m#sess.run(init_F)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mAdj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_assignment_a\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTRUE_A\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mloss_printed\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mAdj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_assignment_a\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTRUE_A\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mloss_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_printed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    895\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (10, 20) for Tensor u'Placeholder_73:0', which has shape '(10, 40)'"
     ]
    }
   ],
   "source": [
    "k = np.random.randint(1, 1000)\n",
    "\n",
    "def GNN_seq_2000(v):\n",
    "    \n",
    "    return GNN(Size=1000, batch_size = v[0], l_rate = v[1], p_min = v[2], p_max = v[2], \n",
    "               datapoints=v[3], big_community=v[4][0], small_community=v[4][1])\n",
    "\n",
    "\n",
    "for i in iterated_list:\n",
    "    Signal = np.random.randn(10, 40)\n",
    "    GNN_seq_2000(i)      \n",
    "#loss, a, B4, A4, Psi_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
