{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def Diffusion(A, F):\n",
    "    \"\"\"finds the diffusion of F via A\"\"\"\n",
    "    #F can be a batched signal\n",
    "    return tf.batch_matmul(A, F)\n",
    "\n",
    "def Diag(A, F):\n",
    "    \"\"\"multiplies F by diagonal vector\"\"\"\n",
    "    diag_matrix = tf.expand_dims(tf.reduce_sum(Adj, 1), 1)\n",
    "    return tf.mul(diag_matrix, F)\n",
    "\n",
    "def Diffusion_normed(A, F):\n",
    "    \"\"\"D^-1W\"\"\"\n",
    "    return tf.mul(tf.div(1.0, Diag(A, F)), tf.batch_matmul(A, F))\n",
    "\n",
    "\n",
    "def balanced_stochastic_blockmodel(big_community, small_community, p_in=1.0, p_out=0.0, seed=None):\n",
    "    \"\"\"gives dense adjacency matrix representaiton of randomly generated SBM with balanced community size\"\"\"\n",
    "\n",
    "    G = nx.random_partition_graph([big_community, small_community], p_in=p_in, p_out=p_out, seed=seed)\n",
    "    A = nx.adjacency_matrix(G).todense()\n",
    "    \n",
    "    return A\n",
    "\n",
    "def batch_vm2(x, m):\n",
    "    [input_size, output_size] = m.get_shape().as_list()\n",
    "\n",
    "    input_shape = tf.shape(x)\n",
    "    batch_rank = input_shape.get_shape()[0].value - 1\n",
    "    batch_shape = input_shape[:batch_rank]\n",
    "    output_shape = tf.concat(0, [batch_shape, [output_size]])\n",
    "\n",
    "    x = tf.reshape(x, [-1, input_size])\n",
    "    y = tf.matmul(x, m)\n",
    "\n",
    "    y = tf.reshape(y, output_shape)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 0]\n",
      " [1 0 1 1 0 0 0]\n",
      " [1 1 0 1 0 0 0]\n",
      " [1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 1]\n",
      " [0 0 0 0 1 0 1]\n",
      " [0 0 0 0 1 1 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.]]),\n",
       " array([[ 0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.,  0.]])]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = balanced_stochastic_blockmodel(4, 3, 1.0, 0.0)\n",
    "print a\n",
    "DATA1 = [np.asarray(balanced_stochastic_blockmodel(4, 2, p, 0.1*p)).astype(np.double) for p in np.linspace(0.3, 0.3, 10)]\n",
    "DATA1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "###################a\n",
    "\n",
    "\n",
    "\n",
    "def GNN(Size=10, signal_dim = 10, batch_size = 2, big_community=30, small_community=10, \n",
    "    p_min = 0.5, p_max = 0.5, Mean = 1, l_rate = 0.0000001, datapoints=10):\n",
    "\n",
    "\n",
    "    \"\"\" First implement of GNN\"\"\"\n",
    "    dim = big_community+small_community\n",
    "\n",
    "    DATA1 = [np.asarray(balanced_stochastic_blockmodel(big_community, small_community, p, 0.1*p, seed=None)).astype(np.double) for p in np.linspace(p_min, p_max, datapoints)]\n",
    "\n",
    "    DATA = DATA1*(Size//datapoints)\n",
    "    np.random.shuffle(DATA)\n",
    "\n",
    "    \n",
    "    TRUE_A = np.append(np.ones([batch_size, big_community], dtype=float),np.zeros([batch_size, small_community], dtype=float), axis = 1)\n",
    "\n",
    "    Adj = tf.placeholder(dtype=tf.float32, shape=[None, dim, dim])\n",
    "    Adj_mod = tf.reshape(tf.transpose(Adj, perm = [1,0,2]), [dim, batch_size*dim])#preparing it to be multiplied by F to broadcast\n",
    "    \n",
    "    F = tf.placeholder(dtype=tf.float32, shape = [10, big_community+small_community])\n",
    "\n",
    "\n",
    "    #first diffusion step without cascading (unnormed version)\n",
    "    Diff_1 = tf.reshape(tf.transpose(tf.matmul(F, Adj_mod)), shape=[batch_size, dim, signal_dim]) #shape=[batch_size, signal_dim, dim])\n",
    "\n",
    "    diag_inv = tf.div(1.0, tf.reduce_sum(Adj, 2))\n",
    "    diag_inv_batch = tf.matrix_diag(diag_inv) #to use in subsequent layers\n",
    "    Diag_1 = tf.mul(tf.expand_dims(diag_inv, 1), F)\n",
    "\n",
    "\n",
    "    C_a = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "    C_b = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "\n",
    "    #treat this as the new Adj_mod\n",
    "    A1 = tf.matmul(C_a, tf.reshape(tf.transpose(Diag_1, perm=[1, 0,2]), [signal_dim, batch_size*dim]))\n",
    "    B1 = tf.matmul(C_b, tf.reshape(tf.transpose(Diff_1, perm=[2, 0,1]), [signal_dim, batch_size*dim]))\n",
    "\n",
    "    #transform it back into the 3-D tensor it is\n",
    "    #Psi_1 = tf.transpose(tf.reshape(A1 + B1, shape = [signal_dim, batch_size, dim]), perm=[1,0,2])\n",
    "    #relu also added\n",
    "    Psi_1 = tf.transpose(tf.reshape(tf.nn.relu(A1 + B1), shape = [signal_dim, batch_size, dim]), perm=[1,2,0])\n",
    "\n",
    "\n",
    " ###################\n",
    "    ###################\n",
    "    Diff_2 = tf.batch_matmul(Adj, Psi_1)\n",
    "    Diag_2 = tf.batch_matmul(diag_inv_batch, Psi_1)\n",
    "    #we change the constants for now but let's keep these the same for another model\n",
    "    C_a_1 = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "    C_b_1 = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "\n",
    "    A2 = tf.matmul(C_a_1, tf.reshape(tf.transpose(Diag_2, perm=[2, 0,1]), [signal_dim, batch_size*dim]))\n",
    "    B2 = tf.matmul(C_b_1, tf.reshape(tf.transpose(Diff_2, perm=[2, 0,1]), [signal_dim, batch_size*dim]))\n",
    "\n",
    "    Psi_2 = tf.transpose(tf.reshape(tf.nn.relu(A2 + B2), shape = [signal_dim, batch_size, dim]), perm=[1,2,0])\n",
    "\n",
    "    ##################\n",
    "    ##################\n",
    "    Diff_3 = tf.batch_matmul(Adj, Psi_2)\n",
    "    Diag_3 = tf.batch_matmul(diag_inv_batch, Psi_2)\n",
    "\n",
    "    C_a_2 = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "    C_b_2 = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "\n",
    "    A3 = tf.matmul(C_a_2, tf.reshape(tf.transpose(Diag_3, perm=[2, 0,1]), [signal_dim, batch_size*dim]))\n",
    "    B3 = tf.matmul(C_b_2, tf.reshape(tf.transpose(Diff_3, perm=[2, 0,1]), [signal_dim, batch_size*dim]))\n",
    "\n",
    "    Psi_3 = tf.transpose(tf.reshape(tf.nn.relu(A3 + B3), shape = [signal_dim, batch_size, dim]), perm=[1,2,0])\n",
    "\n",
    "    ##################\n",
    "    ##################\n",
    "    Diff_4 = tf.batch_matmul(Adj, Psi_3)\n",
    "    Diag_4 = tf.batch_matmul(diag_inv_batch, Psi_3)\n",
    "\n",
    "    C_a_3 = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "    C_b_3 = tf.Variable(tf.random_normal([signal_dim, signal_dim], stddev=1.0, mean=Mean))\n",
    "\n",
    "    A4 = tf.matmul(C_a_3, tf.reshape(tf.transpose(Diag_4, perm=[2, 0,1]), [signal_dim, batch_size*dim]))\n",
    "    B4 = tf.matmul(C_b_3, tf.reshape(tf.transpose(Diff_4, perm=[2, 0,1]), [signal_dim, batch_size*dim]))\n",
    "\n",
    "    Psi_4 = tf.transpose(tf.reshape(tf.nn.relu(A4 + B4), shape = [signal_dim, batch_size, dim]), perm=[1,2,0])\n",
    "\n",
    "\n",
    "    ###################\n",
    "    ###################\n",
    "\n",
    "    B_reduce = tf.Variable(tf.random_normal([signal_dim, 2], stddev=1.0, mean=0.0))\n",
    "\n",
    "    Y_hat = tf.nn.relu(batch_vm2(Psi_4, B_reduce))\n",
    "    test = tf.nn.softmax(Y_hat)\n",
    "    ##################\n",
    "    ################## TRUE ASSIGNMENTlton\n",
    "\n",
    "    #true_assignment_a = tf.expand_dims(tf.concat(0, [tf.zeros([group_size], dtype=tf.float32),\n",
    "     #                                     tf.ones([group_size], dtype=tf.float32)]), 1)\n",
    "    #true_assignment_b = tf.expand_dims(tf.concat(0, [tf.ones([group_size], dtype=tf.float32),\n",
    "     #                                     tf.zeros([group_size], dtype=tf.float32)]), 1)\n",
    "\n",
    "    true_assignment_a= tf.placeholder(dtype=tf.int32, shape = [batch_size, dim])\n",
    "    a = tf.nn.sparse_softmax_cross_entropy_with_logits(Y_hat, true_assignment_a)\n",
    "    \n",
    "    \n",
    "    loss = tf.reduce_sum(a)\n",
    "    optimizer = tf.train.AdamOptimizer(l_rate)\n",
    "    train = optimizer.minimize(loss, var_list=[C_a, C_b, C_a_1, C_a_2, C_a_3, C_b_1, C_b_2, C_b_3, B_reduce])\n",
    "\n",
    "    ##################\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    #init_F = tf.initialize_variables(var_list=[F])\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        iterations = Size/batch_size\n",
    "        \n",
    "        sess.run(init)\n",
    "        loss_lst = [None]*iterations\n",
    "        #Psi_6_lst = []\n",
    "        variable_lst = []\n",
    "        #Cb_list = []\n",
    "        \n",
    "        for i in xrange(iterations):\n",
    "            #sess.run(init_F)\n",
    "            sess.run(train, feed_dict={Adj: DATA[i:i+batch_size], F: Signal, true_assignment_a: TRUE_A})\n",
    "            loss_printed= sess.run(loss, feed_dict={Adj: DATA[i:i+batch_size], F: Signal, true_assignment_a: TRUE_A})\n",
    "            loss_lst[i]=loss_printed\n",
    "\n",
    "            if i%20==0:\n",
    "                print i, \"loss\", loss_printed\n",
    "                d = {\"loss\": loss_lst}\n",
    "                d = pd.DataFrame(d)\n",
    "                d.to_csv(\"~/Desktop/clusternet/GNN/imbalanced_block_data/DATApoints{}Size{}l_rate{}batch_size{}p_min{}p_max{}.csv\".format(datapoints, Size, l_rate, batch_size, p_min, p_max))\n",
    "               \n",
    "            if i==iterations-1:\n",
    "                print \"these are the variables after training:\"\n",
    "                #print \"mean:\", m, \"l rate:\", l, \"Mean_signal:\", n, \n",
    "\n",
    "                f, a, b, a1, a2, a3, b1, b2, b3, b_reduce = sess.run([F, C_a, C_b, C_a_1, C_a_2, C_a_3, C_b_1, C_b_2, C_b_3, B_reduce], \n",
    "                    feed_dict={Adj: DATA[i:i+batch_size], F: Signal, true_assignment_a: TRUE_A})\n",
    "\n",
    "                variable_lst = variable_lst+[f, a, b, a1, a2, a3, b1, b2, b3, b_reduce]\n",
    "                d_var = {\"vars_a_b_ai_bi_b_reduce\": variable_lst, \"header\": [\"F\", \"Ca\", \"Cb\", \"Ca1\", \"Ca2\", \"Ca3\", \n",
    "                \"Cb1\", \"Cb2\", \"Cb3\", \"B_reduce\"]}\n",
    "\n",
    "                d_var = pd.DataFrame(d_var)\n",
    "                d_var.to_csv(\"~/Desktop/clusternet/GNN/imbalanced_block_data//ModelPARAMS_DATApoints{}Size{}l_rate{}batch_size{}p_min{}p_max{}.csv\".format(datapoints, Size, l_rate, batch_size, p_min, p_max))\n",
    "           \n",
    "                #print d_var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "iterated_list = [(b, l, p, d, sizes) for b in [1] for \n",
    "     l in [10**-i for i in [3]] for #xrange(6, 14, 4)] for \n",
    "     p in [0.4] for \n",
    "     d in [2] for\n",
    "     sizes in [(25,15), (30,10), (35, 5)]]\n",
    "\n",
    "iterated_list[0]\n",
    "\n",
    "k = np.random.randint(1, 1000)\n",
    "\n",
    "def GNN_seq_2000(v):\n",
    "    \n",
    "    return GNN(Size=1000, batch_size = v[0], l_rate = v[1], p_min = v[2], p_max = v[2], \n",
    "               datapoints=v[3], big_community=v[4][0], small_community=v[4][1])\n",
    "\n",
    "Signal = np.random.randn(10, 40)\n",
    "for i in iterated_list:\n",
    "    GNN_seq_2000(i)      \n",
    "#loss, a, B4, A4, Psi_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.001, 0.4, 2, (25, 15))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 3.90371e+08\n",
      "20 loss 6.85771e+07\n",
      "40 loss 1.28012e+07\n",
      "60 loss 9.28033e+06\n",
      "80 loss 4.13099e+06\n",
      "100 loss 151236.0\n",
      "120 loss 139546.0\n",
      "140 loss 783082.0\n",
      "160 loss 199759.0\n",
      "180 loss 372523.0\n",
      "200 loss 33230.5\n",
      "220 loss 339870.0\n",
      "240 loss 393917.0\n",
      "260 loss 0.0\n",
      "280 loss 164393.0\n",
      "300 loss 378506.0\n",
      "320 loss 346153.0\n",
      "340 loss 613961.0\n",
      "360 loss 853348.0\n",
      "380 loss 17214.8\n",
      "400 loss 6531.0\n",
      "420 loss 104882.0\n",
      "440 loss 0.0\n",
      "460 loss 978991.0\n",
      "480 loss 0.0\n",
      "500 loss 0.0\n",
      "520 loss 23089.8\n",
      "540 loss 27466.0\n",
      "560 loss 4911.0\n",
      "580 loss 26523.2\n",
      "600 loss 9709.0\n",
      "620 loss 17241.0\n",
      "640 loss 0.0\n",
      "660 loss 0.0\n",
      "680 loss 45268.5\n",
      "700 loss 0.0\n",
      "720 loss 1001.0\n",
      "740 loss 1627.0\n",
      "760 loss 38530.5\n",
      "780 loss 0.0\n",
      "800 loss 20211.2\n",
      "820 loss 20015.0\n",
      "840 loss 20420.8\n",
      "860 loss 18421.8\n",
      "880 loss 0.0\n",
      "900 loss 0.0\n",
      "920 loss 21524.0\n",
      "940 loss 0.0\n",
      "960 loss 18044.8\n",
      "980 loss 5250.0\n",
      "these are the variables after training:\n",
      "0 loss 1.4957e+07\n",
      "20 loss 1.47794e+06\n",
      "40 loss 27.7259\n",
      "60 loss 27.7259\n",
      "80 loss 27.7259\n",
      "100 loss 27.7259\n",
      "120 loss 27.7259\n",
      "140 loss 27.7259\n",
      "160 loss 27.7259\n",
      "180 loss 27.7259\n",
      "200 loss 27.7259\n",
      "220 loss 27.7259\n",
      "240 loss 27.7259\n",
      "260 loss 27.7259\n",
      "280 loss 27.7259\n",
      "300 loss 27.7259\n",
      "320 loss 27.7259\n",
      "340 loss 27.7259\n",
      "360 loss 27.7259\n",
      "380 loss 27.7259\n",
      "400 loss 27.7259\n",
      "420 loss 27.7259\n",
      "440 loss 27.7259\n",
      "460 loss 27.7259\n",
      "480 loss 27.7259\n",
      "500 loss 27.7259\n",
      "520 loss 27.7259\n",
      "540 loss 27.7259\n",
      "560 loss 27.7259\n",
      "580 loss 27.7259\n",
      "600 loss 27.7259\n",
      "620 loss 27.7259\n",
      "640 loss 27.7259\n",
      "660 loss 27.7259\n",
      "680 loss 27.7259\n",
      "700 loss 27.7259\n",
      "720 loss 27.7259\n",
      "740 loss 27.7259\n",
      "760 loss 27.7259\n",
      "780 loss 27.7259\n",
      "800 loss 27.7259\n",
      "820 loss 27.7259\n",
      "840 loss 27.7259\n",
      "860 loss 27.7259\n",
      "880 loss 27.7259\n",
      "900 loss 27.7259\n",
      "920 loss 27.7259\n",
      "940 loss 27.7259\n",
      "960 loss 27.7259\n",
      "980 loss 27.7259\n",
      "these are the variables after training:\n",
      "0 loss 7.93275e+07\n",
      "20 loss 6.34515e+07\n",
      "40 loss 4.91769e+07\n",
      "60 loss 1.88886e+07\n",
      "80 loss 1.21547e+07\n",
      "100 loss 1.21102e+07\n",
      "120 loss 1.06933e+06\n",
      "140 loss 27.7259\n",
      "160 loss 14038.8\n",
      "180 loss 27.7259\n",
      "200 loss 13519.8\n",
      "220 loss 27.7259\n",
      "240 loss 27.7259\n",
      "260 loss 27.7259\n",
      "280 loss 27.7259\n",
      "300 loss 27.7259\n",
      "320 loss 27.7259\n",
      "340 loss 12748.5\n",
      "360 loss 12631.0\n",
      "380 loss 27.7259\n",
      "400 loss 27.7259\n",
      "420 loss 12230.1\n",
      "440 loss 27.7259\n",
      "460 loss 11937.5\n",
      "480 loss 11739.6\n",
      "500 loss 27.7259\n",
      "520 loss 27.7259\n",
      "540 loss 27.7259\n",
      "560 loss 11097.6\n",
      "580 loss 10926.8\n",
      "600 loss 10733.8\n",
      "620 loss 27.7259\n",
      "640 loss 10403.0\n",
      "660 loss 10230.4\n",
      "680 loss 27.7259\n",
      "700 loss 27.7259\n",
      "720 loss 9647.32\n",
      "740 loss 27.7259\n",
      "760 loss 27.7259\n",
      "780 loss 27.7259\n",
      "800 loss 8742.41\n",
      "820 loss 27.7259\n",
      "840 loss 8250.67\n",
      "860 loss 7987.75\n",
      "880 loss 27.7259\n",
      "900 loss 27.7259\n",
      "920 loss 7163.8\n",
      "940 loss 27.7259\n",
      "960 loss 27.7259\n",
      "980 loss 6377.42\n",
      "these are the variables after training:\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
