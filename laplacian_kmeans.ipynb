{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#we try to implement powermethod with kmeans: here\n",
    "http://jmlr.org/proceedings/papers/v37/boutsidis15.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import  networkx as nx\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48034069  0.1725045 ]\n",
      " [ 0.47344008  0.84283829]] [[ 0.48034069  0.17250449]\n",
      " [ 0.47344006  0.84283828]]\n"
     ]
    }
   ],
   "source": [
    "#get diagonal matrix from adjacency matrix\n",
    "x = np.random.rand(2,2)\n",
    "\n",
    "x_tensor = tf.cast(x, tf.float32)\n",
    "\n",
    "degree_m_inverse = tf.diag(tf.inv(tf.reduce_sum(x_tensor,0))) #takes the degree of each vertex and makes diagonal matrix out of it\n",
    "\n",
    "laplacian = tf.matmul(degree_m_inverse, x_tensor)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    a, b = sess.run([laplacian, degree_m_inverse])\n",
    "\n",
    "#double check we do the same thing in np.\n",
    "\n",
    "degree_m = np.diag(np.reciprocal(np.ndarray.sum(np.array(x, dtype = np.float32), 0)))\n",
    "laplacian_np = np.dot(degree_m, x)\n",
    "\n",
    "print a, laplacian_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def joint_permutation(A):\n",
    "    #takes adjacency matrix and relabels, gives out permutated adjacency matrix of same relationship\n",
    "    random_shuffle = np.random.permutation(len(A))\n",
    "\n",
    "    A_shuffle = A[random_shuffle]\n",
    "    A_shuffle = np.transpose(A_shuffle)\n",
    "    A_shuffle = A_shuffle[random_shuffle]\n",
    "\n",
    "    return A_shuffle, random_shuffle\n",
    "\n",
    "def balanced_stochastic_blockmodel(communities=2, groupsize=3, p_in=1.0, p_out=0.0):\n",
    "    #gives dense adjacency matrix representaiton of randomly generated SBM with balanced community size\n",
    "\n",
    "    G = nx.planted_partition_graph(l=communities, k=groupsize, p_in=p_in, p_out =p_out)\n",
    "    A = nx.adjacency_matrix(G).todense()\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 0]\n",
      " [1 0 1 0 0 0]\n",
      " [1 1 0 0 0 0]\n",
      " [0 0 0 0 1 1]\n",
      " [0 0 0 1 0 1]\n",
      " [0 0 0 1 1 0]] (matrix([[0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 1, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 0, 0],\n",
      "        [0, 1, 1, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 1],\n",
      "        [1, 0, 0, 0, 1, 0]]), array([1, 5, 3, 4, 0, 2]))\n"
     ]
    }
   ],
   "source": [
    "A = balanced_stochastic_blockmodel(communities=2, groupsize=3, p_in=1.0, p_out=0.0)\n",
    "B = joint_permutation(A)\n",
    "print A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.57735026,  0.        ],\n",
      "       [ 0.        ,  0.57735026]], dtype=float32), array([[ 0.57735026,  0.        ],\n",
      "       [ 0.        ,  0.57735026]], dtype=float32), array([1, 1, 1, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "x = A\n",
    "dim_graph = len(x)\n",
    "k = communities \n",
    "\n",
    "x_tensor = tf.cast(x, tf.float32)\n",
    " #takes the degree of each vertex and makes diagonal matrix out of it\n",
    "laplacian = tf.matmul(tf.diag(tf.inv(tf.reduce_sum(x_tensor,0))),\n",
    "                      x_tensor)\n",
    "#the laplacian is symmetric, we wish to get the k largest eigenvalues\n",
    "\n",
    "eigenval, eigenvec = tf.self_adjoint_eig(laplacian) #seems to be sorted for me\n",
    "Y = tf.slice(eigenvec, [0, dim_graph-k], [dim_graph, k]) #pick the top k eigenvectors\n",
    "\n",
    "\n",
    "#now we do K-means clustering on the rows of Y, which are the top k eignvectors of the laplacian above, or the bottom k of the normalized laplacian\n",
    "\n",
    "#find k random centroides\n",
    "\n",
    "centroides = tf.Variable(tf.slice(tf.random_shuffle(Y),[0,0],[k,-1]))\n",
    "\n",
    "expanded_Y = tf.expand_dims(Y, 0)\n",
    "expanded_centroides = tf.expand_dims(centroides, 1)\n",
    "\n",
    "diff = tf.sub(expanded_Y, expanded_centroides) #will get difference between eacnh centroide and all of thw points\n",
    "\n",
    "sqr = tf.square(diff) #sqr diff\n",
    "\n",
    "distances = tf.reduce_sum(sqr, 2)\n",
    "assignments = tf.argmin(distances, 0) #these are the clustering assignments based on current centroides\n",
    "\n",
    "means = tf.concat(0, \n",
    "                  [tf.reduce_mean(\n",
    "            tf.gather(\n",
    "                Y, tf.reshape(\n",
    "                    tf.where( \n",
    "                        tf.equal(assignments, c)),[1,-1])),\n",
    "            reduction_indices=[1]) for c in xrange(k)])\n",
    "\n",
    "#these new means, calculated by group, will be the new centroides\n",
    "update_centroides = tf.assign(centroides, means)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in xrange(1000):\n",
    "        _, centroid_values, assigment_values = sess.run([centroides, update_centroides, assignments])\n",
    "    print sess.run([centroides, update_centroides, assignments])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#not bad, seems to be insensitive to permutations, which is what we want. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals: \n",
    "\n",
    "So far we have not implemented non linearities.  We have also not included much of a learning component.  \n",
    "The only hope was to change parameters of how to add laplacian together, but so far it is working....\n",
    "\n",
    "If we feed it communities, depending on how the communities are structured (more strong in degree, more strong out degree) it will change whether we want to just optimize on adjacency matrix, versus optimizing on the laplacian...that is one possible way it can learn\n",
    "\n",
    "\n",
    "It does not need to learn throug kmeans\n",
    "\n",
    "\n",
    "Finally, we can include belief propagation level.  \n",
    "\n",
    "##let's try to make a version where it can decide to take take a vote between the adjaency matrix version versus the laplacian matrix version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = 4\n",
    "k = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 0, 0, 0, 0, 0, 1, 1]), array([0, 0, 0, 0, 0, 0, 0, 0]), 0.25, 0.5]\n"
     ]
    }
   ],
   "source": [
    "A = balanced_stochastic_blockmodel(communities=l, groupsize=k, p_in=1.0, p_out=0.0)\n",
    "x = A\n",
    "dim_graph = len(x)\n",
    "k = communities \n",
    "\n",
    "x_tensor = tf.cast(x, tf.float32)\n",
    "\n",
    "\n",
    "#Laplcian Branch\n",
    "laplacian = tf.matmul(tf.diag(tf.inv(tf.reduce_sum(x_tensor,0))),x_tensor)\n",
    "eigenval, eigenvec = tf.self_adjoint_eig(laplacian)\n",
    "Y = tf.slice(eigenvec, [0, dim_graph-k], [dim_graph, k])\n",
    "\n",
    "#kmeans\n",
    "\n",
    "centroides = tf.Variable(tf.slice(tf.random_shuffle(Y),[0,0],[k,-1]))\n",
    "\n",
    "expanded_Y = tf.expand_dims(Y, 0)\n",
    "expanded_centroides = tf.expand_dims(centroides, 1)\n",
    "\n",
    "assignments = tf.argmin(tf.reduce_sum(tf.square(tf.sub(expanded_Y, expanded_centroides)), 2), 0) #these are the clustering assignments based on current centroides\n",
    "means = tf.concat(0, [tf.reduce_mean(tf.gather(Y, tf.reshape(tf.where( tf.equal(assignments, c)),[1,-1])), reduction_indices=[1]) for c in xrange(k)])\n",
    "\n",
    "update_centroides = tf.assign(centroides, means)\n",
    "\n",
    "\n",
    "#Adjacnecy Branch\n",
    "\n",
    "Adj_eigenval, Adj_eigenvec = tf.self_adjoint_eig(x_tensor)\n",
    "Y_adj = tf.slice(Adj_eigenvec, [0, dim_graph-k], [dim_graph, k])\n",
    "\n",
    "#kmeans \n",
    "\n",
    "centroides_adj = tf.Variable(tf.slice(tf.random_shuffle(Y_adj),[0,0],[k,-1]))\n",
    "\n",
    "expanded_Y_adj = tf.expand_dims(Y_adj, 0)\n",
    "expanded_centroides_adj = tf.expand_dims(centroides_adj, 1)\n",
    "\n",
    "assignments_adj = tf.argmin(tf.reduce_sum(tf.square(tf.sub(expanded_Y_adj, expanded_centroides_adj)), 2), 0) #these are the clustering assignments based on current centroides\n",
    "means_adj = tf.concat(0, [tf.reduce_mean(tf.gather(Y_adj, tf.reshape(tf.where( tf.equal(assignments_adj, c)),[1,-1])), reduction_indices=[1]) for c in xrange(k)])\n",
    "\n",
    "update_centroides_adj = tf.assign(centroides_adj, means_adj)\n",
    "\n",
    "\n",
    "#optimization\n",
    "#a = tf.Variable(tf.random.uniform[1], -1, 1, dtype = tf.float32)\n",
    "#Pooled_assignment = \n",
    "\n",
    "true_assignment_a = tf.concat(0, [tf.zeros([l], dtype=tf.float32), tf.ones([l], dtype=tf.float32)])\n",
    "true_assignment_b = tf.concat(0, [tf.ones([l], dtype=tf.float32), tf.zeros([l], dtype=tf.float32)])         \n",
    "\n",
    "laplace_assignment = tf.cast(assignments, dtype = tf.float32)\n",
    "adj_assignment = tf.cast(assignments_adj, dtype = tf.float32)\n",
    "\n",
    "loss_laplacian = tf.minimum(tf.reduce_sum(tf.square(tf.sub(true_assignment_a, laplace_assignment))), \n",
    "                            tf.reduce_sum(tf.square(tf.sub(true_assignment_b, laplace_assignment))))\n",
    "\n",
    "loss_adj = tf.minimum(tf.reduce_sum(tf.square(tf.sub(true_assignment_a, adj_assignment))),\n",
    "                      tf.reduce_sum(tf.square(tf.sub(true_assignment_b, adj_assignment))))\n",
    "\n",
    "error_laplacian = tf.div(loss_laplacian, dim_graph)\n",
    "error_adj = tf.div(loss_adj, dim_graph)\n",
    "\n",
    "#either we make them vote on which groups they should be in, or backprop to see whcih to listen to, adjacency matrix or laplacian\n",
    "\n",
    "\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in xrange(100):\n",
    "        _, centroid_values, assigment_values = sess.run([centroides, update_centroides, assignments])\n",
    "    print sess.run([assignments, assignments_adj, error_laplacian, error_adj])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `tf.mat.min` not found.\n"
     ]
    }
   ],
   "source": [
    "?tf.mat.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#eigenspace_power = tf.matmul(laplacian, tf.transpose(laplacian)) #x will be symmetric, so so will L, can be made more effcient here\n",
    "#eigenspace_power = tf.matmul(eigenspace_power, eigenspace_power)\n",
    "#here we used p = 2 iterations, we can also try to learn this.  \n",
    "#the paper reference above used p = some function of the eigenvalues...\n",
    "#but this may be learned\n",
    "\n",
    "#S = tf.Variable(tf.random_normal(shape=[dim_graph,k],mean = 0.0,stddev = 1.0))\n",
    "#B = tf.matmul(eigenspace_power, S)\n",
    "#now we extract the singular vectors of B (left) to get our eigenspace approximation\n",
    "#Y_hat = tf.svd(B, full_matrices=True) #we want to use tf.svd, however it does not work in the newest version and building from source does not give me all that I want...\n",
    "\n",
    "\n",
    "#the laplacian is symmetric, we wish to get the 2 largest eigenvalues\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
