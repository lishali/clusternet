{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import  networkx as nx\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non backtacking matrix is similiar to message passing algorithms used to find clusters.  This works much better in the sparse case (especially since adjacency matrices in that case are very not full rank, so finding their eigenvectors...)\n",
    "\n",
    "Definition:\n",
    "\n",
    "Let M be the number of edges, then the matrix B, our non backtracking matrix will be a 2M by 2M matrix such that B_(ei-ej) = 1 if ei and ej are adjacent and are not just one edge, and 0 otherwise (i.e the diagonal should be zero). \n",
    "\n",
    "\n",
    "An interesting point is that the Bethe Hessian matrix should have similar performance, however is defined from the laplacian, with a regularizing r...that can be learned.  https://papers.nips.cc/paper/5520-spectral-clustering-of-graphs-with-the-bethe-hessian.pdf\n",
    "\n",
    "the Bethe Hessian Matrix is the deformed Laplacian is is simply:\n",
    "H(r): = (r^2 -1)1 - rA +D\n",
    "\n",
    "\n",
    "for the stochastic block model, optimal r's have been show to equal root(c) where c is the average degree of the graph.\n",
    "\n",
    "\n",
    "Compute eigenvectors associated with negative eigenvalues of both H(r_c) and H(-r_c)\n",
    "\n",
    "supposedly, the negative eigenvalues of H(r_c) reveal the assortative aspects, whereas H(-r_c) reviews the disassortative ones.\n",
    "\n",
    "Tests:\n",
    "\n",
    "Give a large dataset of stochastic block models with the same average degree, will it learn the correct r_c?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 1 0 1]\n",
      " [0 1 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 1 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 1 0 ..., 0 1 0]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'laplacian' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-f6097960bd00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0massignment_laplacian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_laplacian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentroides_laplacian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_means_spectral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlaplacian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommunities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mupdate_centroides_laplacian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroides_laplacian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans_laplacian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mloss_laplacian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_laplacian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massignment_laplacian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'laplacian' is not defined"
     ]
    }
   ],
   "source": [
    "communities = 2 #number of communities, chance to \n",
    "group_size = 20 #number of nodes in each communitites (balanced so far)\n",
    "dim_graph = communities*group_size\n",
    "A = balanced_stochastic_blockmodel(communities=communities, groupsize=group_size, p_in=0.29, p_out=0.1)\n",
    "\n",
    "print A\n",
    "\n",
    "Adj = tf.cast(A, tf.float32)\n",
    "Diag = tf.diag(tf.reduce_sum(Adj,0)) #just the diagonal matrix of degrees of Adj\n",
    "r = tf.Variable(tf.random_normal(shape=[1], mean=0.0,\n",
    "                                 stddev=10.0, dtype=tf.float32,\n",
    "                                 seed=None, name=None))\n",
    "Bethe_Hesse = (tf.square(r)-1)*tf.diag(tf.ones(shape=[dim_graph]))-tf.mul(r, Adj)+Diag \n",
    "\n",
    "\n",
    "assignment_laplacian, means_laplacian, centroides_laplacian = k_means_spectral(laplacian, communities, group_size)\n",
    "update_centroides_laplacian = tf.assign(centroides_laplacian, means_laplacian)\n",
    "loss_laplacian, error_laplacian = cluster_error(assignment_laplacian, group_size)\n",
    "\n",
    "assignment_adj, means_adj, centroides_adj = k_means_spectral(Adj, communities, group_size)\n",
    "update_centroides_adj = tf.assign(centroides_adj, means_adj)\n",
    "loss_adj, error_adj = cluster_error(assignment_adj, group_size)\n",
    "\n",
    "\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print sess.run(Bethe_Hesse)\n",
    "    \n",
    "    \n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) #, feed_dict = {x: A})\n",
    "    for step in xrange(10):\n",
    "        sess.run([update_centroides_adj, update_centroides_laplacian, assignment_adj, assignment_laplacian])\n",
    "    a, b, c, d = sess.run([assignment_laplacian, assignment_adj, error_adj, error_laplacian])\n",
    "    print 'Using the Laplacian, the assignment of clusters for each node is {}, with error rate of {}.'.format(a, d)\n",
    "    print 'Using the adjacency matrix, the assignment of clusters for each node is {}, with error rate of {}'.format(b, c)\n",
    "    print \"Note that a random guess of a balanced partition will on average overlap with the correct clustering by 50%,but we are taking the min of two binomial's--since we wanted it to be invariant under labellings, so randomly we will do a little better than 50% error, min of two binomial 1/2.  But for bigger and bigger vectors this will approach 1/2. \"\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ÃŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "communities = 2 #number of communities, chance to \n",
    "group_size = 3 \n",
    "def get_eigenvectors(A, communities=communities, group_size=group_size):\n",
    "    \"\"\"gets first k eigenvalues of matrix\"\"\"\n",
    "    dim_graph = communities*group_size\n",
    "    eigenval, eigenvec = tf.self_adjoint_eig(A)\n",
    "    return tf.slice(eigenvec, [0, dim_graph-communities], [dim_graph, communities])\n",
    "\n",
    "def k_means_spectral(A, communities=communities, group_size=group_size):\n",
    "    \"\"\"takes in matrix, does k means on first k eignvectors \n",
    "    Y where rows are the data points\"\"\"    \n",
    "\n",
    "    Y = get_eigenvectors(A, communities, group_size)\n",
    "    centroides = tf.Variable(tf.slice(tf.random_shuffle(Y),[0,0],[communities,-1]))\n",
    "    expanded_Y = tf.expand_dims(Y, 0)\n",
    "    expanded_centroides = tf.expand_dims(centroides, 1)\n",
    "    assignments = tf.argmin(tf.reduce_sum(tf.square(tf.sub(expanded_Y, expanded_centroides)), 2), 0) #these are the clustering assignments based on current centroides\n",
    "    means = tf.concat(0, [tf.reduce_mean(tf.gather(Y, tf.reshape(tf.where( tf.equal(assignments, c)),[1,-1])),\n",
    "                                     reduction_indices=[1]) for c in xrange(communities)])\n",
    "    \n",
    "    return assignments, means, centroides\n",
    "\n",
    "def cluster_error(assignment, group_size=group_size, communities=communities):\n",
    "    \"\"\"Takes in assignments and compares to the balanced two cluster\n",
    "    model of the random graph above\"\"\"\n",
    "    dim_graph = communities*group_size\n",
    "    true_assignment_a = tf.concat(0, [tf.zeros([group_size], dtype=tf.float32),\n",
    "                                      tf.ones([group_size], dtype=tf.float32)])\n",
    "    true_assignment_b = tf.concat(0, [tf.ones([group_size], dtype=tf.float32),\n",
    "                                      tf.zeros([group_size], dtype=tf.float32)])         \n",
    "    assignment = tf.cast(assignment, dtype = tf.float32)\n",
    "    loss = tf.minimum(tf.reduce_sum(tf.square(tf.sub(true_assignment_a, assignment))),\n",
    "                      tf.reduce_sum(tf.square(tf.sub(true_assignment_b, assignment))))\n",
    "    error = tf.div(loss, dim_graph)\n",
    "    \n",
    "    return loss, error\n",
    "\n",
    "\n",
    "## the following functions allows us to verify that permuted adjacency matrices shoudl have no effect on the network\n",
    "#of course, which no power method, this is just true by construction but good to have anyways \n",
    "\n",
    "def joint_permutation(A):\n",
    "    #takes adjacency matrix and relabels, gives out permutated adjacency matrix of same relationship\n",
    "    random_shuffle = np.random.permutation(len(A))\n",
    "\n",
    "    A_shuffle = A[random_shuffle]\n",
    "    A_shuffle = np.transpose(A_shuffle)\n",
    "    A_shuffle = A_shuffle[random_shuffle]\n",
    "\n",
    "    return A_shuffle, random_shuffle\n",
    "\n",
    "def balanced_stochastic_blockmodel(communities=2, groupsize=3, p_in=1.0, p_out=0.0):\n",
    "    #gives dense adjacency matrix representaiton of randomly generated SBM with balanced community size\n",
    "\n",
    "    G = nx.planted_partition_graph(l=communities, k=groupsize, p_in=p_in, p_out =p_out)\n",
    "    A = nx.adjacency_matrix(G).todense()\n",
    "    \n",
    "    return A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ((None, <tensorflow.python.ops.variables.Variable object at 0x1170db050>), (None, <tensorflow.python.ops.variables.Variable object at 0x1170dbbd0>), (None, <tensorflow.python.ops.variables.Variable object at 0x1177d9550>), (None, <tensorflow.python.ops.variables.Variable object at 0x1177d9a90>), (None, <tensorflow.python.ops.variables.Variable object at 0x115bbe210>), (None, <tensorflow.python.ops.variables.Variable object at 0x117ec1cd0>))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-e07ee132b60c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/envs/tfnighly/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    196\u001b[0m         grad_loss=grad_loss)\n\u001b[1;32m    197\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[0;32m--> 198\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m   def compute_gradients(self, loss, var_list=None,\n",
      "\u001b[0;32m/Users/xiangli/anaconda/envs/tfnighly/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       raise ValueError(\"No gradients provided for any variable: %s\" %\n\u001b[0;32m--> 298\u001b[0;31m                        (grads_and_vars,))\n\u001b[0m\u001b[1;32m    299\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: ((None, <tensorflow.python.ops.variables.Variable object at 0x1170db050>), (None, <tensorflow.python.ops.variables.Variable object at 0x1170dbbd0>), (None, <tensorflow.python.ops.variables.Variable object at 0x1177d9550>), (None, <tensorflow.python.ops.variables.Variable object at 0x1177d9a90>), (None, <tensorflow.python.ops.variables.Variable object at 0x115bbe210>), (None, <tensorflow.python.ops.variables.Variable object at 0x117ec1cd0>))"
     ]
    }
   ],
   "source": [
    "communities = 2 #number of communities, chance to \n",
    "group_size = 4 #number of nodes in each communitites (balanced so far)\n",
    "dim_graph = communities*group_size\n",
    "A = balanced_stochastic_blockmodel(communities=communities, groupsize=group_size, p_in=0.15, p_out=0.01)\n",
    "\n",
    "Adj = tf.cast(A, tf.float32)\n",
    "Diag = tf.diag(tf.reduce_sum(Adj,0)) #just the diagonal matrix of degrees of Adj\n",
    "r = tf.Variable(tf.random_normal(shape=[1], mean=0.0,\n",
    "                                 stddev=1.0, dtype=tf.float32,\n",
    "                                 seed=None, name=None))\n",
    "\n",
    "\n",
    "Bethe_Hesse_neg = (tf.square(r)-1)*tf.diag(tf.ones(shape=[dim_graph]))-tf.mul(r, Adj)+Diag \n",
    "\n",
    "def get_neg_eigenvectors(A, dim_graph=dim_graph):\n",
    "    \"\"\"gets neg eigenvalues of matrix, \n",
    "    this may not be a differentiable opeartor\n",
    "    so let's set it to take the last k eigenvalues as usual\n",
    "    \"\"\"\n",
    "    dim_graph = communities*group_size\n",
    "    eigenval, eigenvec = tf.self_adjoint_eig(A)\n",
    "    return eigenval, tf.slice(eigenvec, [0, 0], [dim_graph, group_size])\n",
    "\n",
    "def k_means_spectral(A, dim_graph=dim_graph, communities=communities):\n",
    "    \"\"\"takes in matrix, does k means on first k eignvectors \n",
    "    Y where rows are the data points\"\"\"    \n",
    "\n",
    "    Y = get_neg_eigenvectors(A, dim_graph)[1]\n",
    "    centroides = tf.Variable(tf.slice(tf.random_shuffle(Y),[0,0],[communities,-1]))\n",
    "    expanded_Y = tf.expand_dims(Y, 0)\n",
    "    expanded_centroides = tf.expand_dims(centroides, 1)\n",
    "    assignments = tf.argmin(tf.reduce_sum(tf.square(tf.sub(expanded_Y, expanded_centroides)), 2), 0) #these are the clustering assignments based on current centroides\n",
    "    means = tf.concat(0, [tf.reduce_mean(tf.gather(Y, tf.reshape(tf.where( tf.equal(assignments, c)),[1,-1])),\n",
    "                                     reduction_indices=[1]) for c in xrange(communities)])\n",
    "    \n",
    "    return assignments, means, centroides\n",
    "\n",
    "\n",
    "def cluster_error(assignment, group_size=group_size, communities=communities):\n",
    "    \"\"\"Takes in assignments and compares to the balanced two cluster\n",
    "    model of the random graph above\"\"\"\n",
    "    dim_graph = communities*group_size\n",
    "    true_assignment_a = tf.concat(0, [tf.zeros([group_size], dtype=tf.float32),\n",
    "                                      tf.ones([group_size], dtype=tf.float32)])\n",
    "    true_assignment_b = tf.concat(0, [tf.ones([group_size], dtype=tf.float32),\n",
    "                                      tf.zeros([group_size], dtype=tf.float32)])         \n",
    "    assignment = tf.cast(assignment, dtype = tf.float32)\n",
    "    loss = tf.minimum(tf.reduce_sum(tf.square(tf.sub(true_assignment_a, assignment))),\n",
    "                      tf.reduce_sum(tf.square(tf.sub(true_assignment_b, assignment))))\n",
    "    error = tf.div(loss, dim_graph)\n",
    "    \n",
    "    return loss, error\n",
    "\n",
    "\n",
    "assignment_adj, means_adj, centroides_adj = k_means_spectral(Bethe_Hesse_neg, communities, group_size)\n",
    "update_centroides_adj = tf.assign(centroides_adj, means_adj)\n",
    "loss_adj, error_adj = cluster_error(assignment_adj, group_size, communities)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss_adj)\n",
    "\n",
    "\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for steps in xrange(10):\n",
    "        sess.run(train)#, feed_dict = {x: A})\n",
    "        for step in xrange(10):\n",
    "            sess.run([update_centroides_adj, assignment_adj, error_adj])\n",
    "            a, b = sess.run([assignment_adj, error_adj])\n",
    "            print 'Using the Besse Hessian Matrix, the assignment of clusters for each node is {}, with error rate of {}'.format(a, b)\n",
    "  \n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "print sess.run([Y, centroides, assignments, means, tmp])\n",
    "\n",
    "    \n",
    "\n",
    "init = tf.initialize_all_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tfnighly]",
   "language": "python",
   "name": "conda-env-tfnighly-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
