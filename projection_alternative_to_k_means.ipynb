{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import linalg_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "import networkx as nx\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ops.RegisterGradient(\"gradient_no_unitary_adjustment\")\n",
    "def _test1(op, grad_e, grad_v):\n",
    "    \"\"\"Gradient for SelfAdjointEigV2 derived with Joan with no adjustment for subspace\"\"\"\n",
    "    e = op.outputs[0]\n",
    "    v = op.outputs[1]\n",
    "    #dim = v.get_shape()\n",
    "    with ops.control_dependencies([grad_e.op, grad_v.op]):\n",
    "        if grad_v is not None:  \n",
    "            E = array_ops.diag(e)\n",
    "            #v_proj = arrary.ops.slice(v, [0,0], [])\n",
    "            grad_grassman = grad_v# - math_ops.batch_matmul(math_ops.batch_matmul(v, array_ops.transpose(grad_v)), v)\n",
    "            grad_a = math_ops.batch_matmul(grad_grassman, math_ops.batch_matmul(E, array_ops.transpose(grad_v)))+math_ops.batch_matmul(grad_v, math_ops.batch_matmul(E, array_ops.transpose(grad_grassman)))\n",
    "    return grad_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ops.RegisterGradient(\"grassman_with_2d\")\n",
    "def _test1(op, grad_e, grad_v):\n",
    "    \"\"\"Gradient for SelfAdjointEigV2 derived with Joan with no adjustment for subspace\"\"\"\n",
    "    e = op.outputs[0]\n",
    "    v = op.outputs[1]\n",
    "    #dim = v.get_shape()\n",
    "    with ops.control_dependencies([grad_e.op, grad_v.op]):\n",
    "        if grad_v is not None:  \n",
    "            E = array_ops.diag(e)\n",
    "            v_proj = array_ops.slice(v, [0,0], [20,2])\n",
    "            grad_grassman = grad_v - math_ops.batch_matmul(math_ops.batch_matmul(v_proj, array_ops.transpose(v_proj)), grad_v)\n",
    "            grad_a = math_ops.batch_matmul(grad_grassman, math_ops.batch_matmul(E, array_ops.transpose(grad_v)))+math_ops.batch_matmul(grad_v, math_ops.batch_matmul(E, array_ops.transpose(grad_grassman)))\n",
    "    return grad_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balanced_stochastic_blockmodel(communities=2, groupsize=3, p_in=0.5, p_out=0.1, seed=None):\n",
    "    #gives dense adjacency matrix representaiton of randomly generated SBM with balanced community size\n",
    "\n",
    "    G = nx.planted_partition_graph(l=communities, k=groupsize, p_in=p_in, p_out =p_out, seed=seed)\n",
    "    A = nx.adjacency_matrix(G).todense()\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_subspace(adj, groupsize, communities, diag, dim_proj):\n",
    "    normalizer = tf.cast(2.0*groupsize*communities, dtype=tf.float64)\n",
    "    total_degree = tf.cast(tf.reduce_sum(adj), dtype=tf.float64)\n",
    "    r = tf.sqrt(total_degree/normalizer)\n",
    "    BH_op = (tf.square(r)-1)*tf.diag(tf.ones(shape=[communities*groupsize], dtype=tf.float64))-r*adj+diag \n",
    "    val, vec = tf.self_adjoint_eig(BH_op) #this is already normalized so no need to normalize\n",
    "    subspace = tf.slice(vec, [0,0], [communities*groupsize, dim_proj])\n",
    "    return r, subspace\n",
    "\n",
    "def proj_magnitude(space, vector):\n",
    "    projection_op = tf.matmul(space, tf.transpose(space))\n",
    "    projection = tf.matmul(projection_op, vector)\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(projection))) #tf.reduce_sum(tf.abs(projection))#\n",
    "\n",
    "\n",
    "def rnd_vec_normed(communities, groupsize, seed=None):\n",
    "    rnd_vec1 = tf.Variable(tf.random_normal(shape=[communities*groupsize,1], mean=0.0,stddev=1.0,\n",
    "                                                    dtype=tf.float64,\n",
    "                                                    seed=seed))\n",
    "    return normalize_vec(rnd_vec1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_svm_cluster(communities = 2, group_size = 10, seed=1, seed_r=1, p=0.8, q=0.05, name='test1', projection_dim=2, iterations=100, \n",
    "                     print_ratio=10, l_rate=0.1, mean=2.0, sd=0.4):\n",
    "    \"\"\"testing to see if the loss will decrease backproping through very simple function\"\"\"\n",
    "    B = np.asarray(balanced_stochastic_blockmodel(communities, group_size, p, q, seed)).astype(np.double)\n",
    "    B = tf.cast(B, dtype = tf.float64)\n",
    "    \n",
    "    Diag = tf.diag(tf.reduce_sum(B,0))\n",
    "    Diag = tf.cast(Diag, tf.float64)\n",
    "\n",
    "    r =  tf.Variable(tf.random_normal(shape=[1], mean=mean,\n",
    "                                 stddev=sd, dtype=tf.float64,\n",
    "                                 seed=seed_r, name=None))\n",
    "\n",
    "    \n",
    "    BH = (tf.square(r)-1)*tf.diag(tf.ones(shape=[communities*group_size], dtype=tf.float64))-tf.mul(r, B)+Diag \n",
    "    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        g = tf.get_default_graph()\n",
    "        \n",
    "        with g.gradient_override_map({'SelfAdjointEigV2': name}):\n",
    "            eigenval, eigenvec = tf.self_adjoint_eig(BH)\n",
    "            #we try to do svm in this subspace \n",
    "            #or we can project it down to 1 dimensions, do the clustering there via some threshold and check if it makes sense \n",
    "            #by computing the loss, if it is too big, we change the angle we project down to...\n",
    "            \n",
    "            \n",
    "            eigenvec_proj = tf.slice(eigenvec, [0,0], [communities*group_size, projection_dim])\n",
    "            \n",
    "            \n",
    "            \n",
    "            true_assignment_a = tf.concat(0, [-1*tf.ones([group_size], dtype=tf.float64),\n",
    "                                      tf.ones([group_size], dtype=tf.float64)])\n",
    "            true_assignment_b = -1*true_assignment_a\n",
    "            true_assignment_a = tf.expand_dims(true_assignment_a, 1)\n",
    "            true_assignment_b = tf.expand_dims(true_assignment_b, 1)\n",
    "\n",
    "            \n",
    "            projected_a = tf.matmul(tf.matmul(eigenvec_proj, tf.transpose(eigenvec_proj)), true_assignment_a)#tf.transpose(true_assignment_a))\n",
    "            projected_b = tf.matmul(tf.matmul(eigenvec_proj, tf.transpose(eigenvec_proj)), true_assignment_b)#tf.transpose(true_assignment_b))\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss = tf.minimum(tf.reduce_sum(tf.square(tf.sub(projected_a, true_assignment_a))),\n",
    "                              tf.reduce_sum(tf.square(tf.sub(projected_b, true_assignment_b))))\n",
    "            \n",
    "            optimizer = tf.train.GradientDescentOptimizer(l_rate)\n",
    "            \n",
    "            train = optimizer.minimize(loss, var_list=[r])\n",
    "\n",
    "            eigenvec_grad = tf.gradients(eigenvec, r)\n",
    "            loss_grad = tf.gradients(loss, r)\n",
    "            \n",
    "            \n",
    "            \n",
    "            r_op, target = target_subspace(adj=B, groupsize=group_size, communities=communities, diag=Diag, dim_proj=projection_dim)  \n",
    "            \n",
    "            r_op_projection_a = tf.matmul(tf.matmul(target, tf.transpose(target)), true_assignment_a)\n",
    "            r_op_projection_b = tf.matmul(tf.matmul(target, tf.transpose(target)), true_assignment_b)\n",
    "            r_op_loss = tf.minimum(tf.reduce_sum(tf.square(tf.sub(r_op_projection_a, true_assignment_a))),\n",
    "                              tf.reduce_sum(tf.square(tf.sub(r_op_projection_b, true_assignment_b))))\n",
    "            \n",
    "            init = tf.initialize_all_variables()\n",
    "            \n",
    "            \n",
    "            sess.run(init)\n",
    "            a,b,c,d= sess.run([r, r_op, r_op_loss, tf.transpose(r_op_projection_a)])\n",
    "            print \"initial r: {}. r_op = sqrt(average degree) : {} . Loss associated with r_op: {}. r_op assignments {}.\".format(a, b, c, d)\n",
    "            for i in range(iterations):           \n",
    "                if i%print_ratio==0:\n",
    "                    print i\n",
    "                    a,b,c,d = sess.run([r, loss, tf.gradients(loss, r), tf.transpose(projected_a)])\n",
    "                    print \"current r: {}, current loss: {}, gradient of loss/r is {} and current assignments (up to sign) {}.\".format(a,b,c,d)\n",
    "                    sess.run(train)\n",
    "                    \n",
    "                \n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 2.09713102]. r_op = sqrt(average degree) : 1.96214168703 . Loss associated with r_op: 0.239352945504. r_op assignments [[-0.81008947 -0.89831384 -1.06456991 -1.0936426  -1.05592124 -0.92168635\n",
      "  -0.88318609 -1.01724697 -1.0936426  -1.05086035  0.98362854  0.98397103\n",
      "   1.03054178  0.80692791  1.13586066  0.80914196  1.03176237  1.10198787\n",
      "   1.13586066  0.85180488]].\n",
      "0\n",
      "current r: [ 2.09713102], current loss: 0.244082145495, gradient of loss/r is [array([-870.75382315])] and current assignments (up to sign) [[-0.80940175 -0.9017661  -1.06406728 -1.09702595 -1.05503979 -0.91305556\n",
      "  -0.88571072 -1.0136581  -1.09702595 -1.05125222  0.97767908  0.97803705\n",
      "   1.02975475  0.80666566  1.14222167  0.80900335  1.02415991  1.10372799\n",
      "   1.14222167  0.85444331]].\n",
      "5000\n",
      "current r: [ 2.1842064], current loss: 0.247151590827, gradient of loss/r is [array([-970.77787562])] and current assignments (up to sign) [[-0.80898766 -0.90375093 -1.06375077 -1.09897039 -1.05450329 -0.90810577\n",
      "  -0.88716406 -1.01156044 -1.09897039 -1.05144598  0.97423025  0.97459683\n",
      "   1.02925282  0.80647574  1.14589764  0.80888358  1.01976638  1.10469783\n",
      "   1.14589764  0.85594003]].\n",
      "10000\n",
      "current r: [ 2.28128418], current loss: 0.250545077235, gradient of loss/r is [array([-1084.74543071])] and current assignments (up to sign) [[-0.80855164 -0.90577337 -1.06340744 -1.10095103 -1.05393414 -0.90307167\n",
      "  -0.88864636 -1.00939739 -1.10095103 -1.05161955  0.97069496  0.97107007\n",
      "   1.02870375  0.80625236  1.14965726  0.80873098  1.01527287  1.10566245\n",
      "   1.14965726  0.85744937]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='gradient_no_unitary_adjustment', l_rate=0.0001, seed_r=1, seed = 1, print_ratio=5000, iterations=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 2.09713102]. r_op = sqrt(average degree) : 1.96214168703 . Loss associated with r_op: 0.239352945504. r_op assignments [[-0.81008947 -0.89831384 -1.06456991 -1.0936426  -1.05592124 -0.92168635\n",
      "  -0.88318609 -1.01724697 -1.0936426  -1.05086035  0.98362854  0.98397103\n",
      "   1.03054178  0.80692791  1.13586066  0.80914196  1.03176237  1.10198787\n",
      "   1.13586066  0.85180488]].\n",
      "0\n",
      "current r: [ 2.09713102], current loss: 0.244082145495, gradient of loss/r is [array([-870.75382315])] and current assignments (up to sign) [[-0.80940175 -0.9017661  -1.06406728 -1.09702595 -1.05503979 -0.91305556\n",
      "  -0.88571072 -1.0136581  -1.09702595 -1.05125222  0.97767908  0.97803705\n",
      "   1.02975475  0.80666566  1.14222167  0.80900335  1.02415991  1.10372799\n",
      "   1.14222167  0.85444331]].\n",
      "5000\n",
      "current r: [ 2.1842064], current loss: 0.247151590827, gradient of loss/r is [array([-970.77787562])] and current assignments (up to sign) [[-0.80898766 -0.90375093 -1.06375077 -1.09897039 -1.05450329 -0.90810577\n",
      "  -0.88716406 -1.01156044 -1.09897039 -1.05144598  0.97423025  0.97459683\n",
      "   1.02925282  0.80647574  1.14589764  0.80888358  1.01976638  1.10469783\n",
      "   1.14589764  0.85594003]].\n",
      "10000\n",
      "current r: [ 2.28128418], current loss: 0.250545077235, gradient of loss/r is [array([-1084.74543071])] and current assignments (up to sign) [[-0.80855164 -0.90577337 -1.06340744 -1.10095103 -1.05393414 -0.90307167\n",
      "  -0.88864636 -1.00939739 -1.10095103 -1.05161955  0.97069496  0.97107007\n",
      "   1.02870375  0.80625236  1.14965726  0.80873098  1.01527287  1.10566245\n",
      "   1.14965726  0.85744937]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='grassman_with_2d', l_rate=0.0001, seed_r=1, seed = 1, print_ratio=5000, iterations=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 2.09713102]. r_op = sqrt(average degree) : 1.96214168703 . Loss associated with r_op: 0.239352945504. r_op assignments [[-0.81008947 -0.89831384 -1.06456991 -1.0936426  -1.05592124 -0.92168635\n",
      "  -0.88318609 -1.01724697 -1.0936426  -1.05086035  0.98362854  0.98397103\n",
      "   1.03054178  0.80692791  1.13586066  0.80914196  1.03176237  1.10198787\n",
      "   1.13586066  0.85180488]].\n",
      "0\n",
      "current r: [ 2.09713102], current loss: 0.244082145495, gradient of loss/r is [array([ 0.03527779])] and current assignments (up to sign) [[-0.80940175 -0.9017661  -1.06406728 -1.09702595 -1.05503979 -0.91305556\n",
      "  -0.88571072 -1.0136581  -1.09702595 -1.05125222  0.97767908  0.97803705\n",
      "   1.02975475  0.80666566  1.14222167  0.80900335  1.02415991  1.10372799\n",
      "   1.14222167  0.85444331]].\n",
      "5000\n",
      "current r: [ 2.09712749], current loss: 0.244082021042, gradient of loss/r is [array([ 0.03527778])] and current assignments (up to sign) [[-0.80940177 -0.90176601 -1.0640673  -1.09702587 -1.05503982 -0.91305577\n",
      "  -0.88571066 -1.01365819 -1.09702587 -1.05125221  0.97767922  0.9780372\n",
      "   1.02975477  0.80666567  1.14222151  0.80900336  1.0241601   1.10372795\n",
      "   1.14222151  0.85444325]].\n",
      "10000\n",
      "current r: [ 2.09712396], current loss: 0.24408189659, gradient of loss/r is [array([ 0.03527778])] and current assignments (up to sign) [[-0.80940178 -0.90176593 -1.06406731 -1.09702579 -1.05503984 -0.91305598\n",
      "  -0.88571059 -1.01365828 -1.09702579 -1.0512522   0.97767937  0.97803734\n",
      "   1.02975479  0.80666568  1.14222136  0.80900336  1.02416028  1.10372791\n",
      "   1.14222136  0.85444318]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='SelfAdjointEigV2', l_rate=0.0001, seed_r=1, seed = 1, print_ratio=5000, iterations=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 0.09713102]. r_op = sqrt(average degree) : 1.96214168703 . Loss associated with r_op: 0.239352945504. r_op assignments [[-0.81008947 -0.89831384 -1.06456991 -1.0936426  -1.05592124 -0.92168635\n",
      "  -0.88318609 -1.01724697 -1.0936426  -1.05086035  0.98362854  0.98397103\n",
      "   1.03054178  0.80692791  1.13586066  0.80914196  1.03176237  1.10198787\n",
      "   1.13586066  0.85180488]].\n",
      "0\n",
      "current r: [ 0.09713102], current loss: 13.4317194769, gradient of loss/r is [array([-35.38462474])] and current assignments (up to sign) [[-0.0704722  -0.00936025 -0.0777787  -0.05508386 -0.07880056 -1.32332547\n",
      "  -0.04820161 -0.0327515  -0.05508386 -0.02054714  1.12260788  1.10486448\n",
      "   0.19324849  0.1558531   0.14156616  0.15051068  1.45895059  0.19786074\n",
      "   0.14156616  0.12984709]].\n",
      "5000\n",
      "current r: [ 0.45097726], current loss: 1.35213560743, gradient of loss/r is [array([ nan])] and current assignments (up to sign) [[-0.76517874 -0.64865546 -0.97596476 -0.84888474 -0.98337332 -1.55159973\n",
      "  -0.70128906 -1.07012367 -0.84888474 -0.88712714  1.25174917  1.2487866\n",
      "   0.92204746  0.70567144  0.78882589  0.69568715  1.42072437  0.90515929\n",
      "   0.78882589  0.63930578]].\n",
      "10000\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Self Adjoint Eigen decomposition was not successful. The input might not be valid.\n\t [[Node: SelfAdjointEigV2_70 = SelfAdjointEigV2[T=DT_DOUBLE, _gradient_op_type=\"SelfAdjointEigV2\", compute_v=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add_70)]]\nCaused by op u'SelfAdjointEigV2_70', defined at:\n  File \"/Users/xiangli/anaconda/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/xiangli/anaconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2831, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-54-f7a3ebeb9739>\", line 1, in <module>\n    test_svm_cluster(name='SelfAdjointEigV2', l_rate=0.01, seed_r=1, seed = 1, print_ratio=5000, iterations=10001, mean = 0)\n  File \"<ipython-input-46-8fbb99d36fa0>\", line 22, in test_svm_cluster\n    eigenval, eigenvec = tf.self_adjoint_eig(BH)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/linalg_ops.py\", line 161, in self_adjoint_eig\n    e, v = gen_linalg_ops._self_adjoint_eig_v2(tensor, compute_v=True, name=name)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_linalg_ops.py\", line 519, in _self_adjoint_eig_v2\n    compute_v=compute_v, name=name)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-f7a3ebeb9739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_svm_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SelfAdjointEigV2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-8fbb99d36fa0>\u001b[0m in \u001b[0;36mtest_svm_cluster\u001b[0;34m(communities, group_size, seed, seed_r, p, q, name, projection_dim, iterations, print_ratio, l_rate, mean, sd)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mprint_ratio\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojected_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"current r: {}, current loss: {}, gradient of loss/r is {} and current assignments (up to sign) {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Self Adjoint Eigen decomposition was not successful. The input might not be valid.\n\t [[Node: SelfAdjointEigV2_70 = SelfAdjointEigV2[T=DT_DOUBLE, _gradient_op_type=\"SelfAdjointEigV2\", compute_v=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add_70)]]\nCaused by op u'SelfAdjointEigV2_70', defined at:\n  File \"/Users/xiangli/anaconda/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/xiangli/anaconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2831, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-54-f7a3ebeb9739>\", line 1, in <module>\n    test_svm_cluster(name='SelfAdjointEigV2', l_rate=0.01, seed_r=1, seed = 1, print_ratio=5000, iterations=10001, mean = 0)\n  File \"<ipython-input-46-8fbb99d36fa0>\", line 22, in test_svm_cluster\n    eigenval, eigenvec = tf.self_adjoint_eig(BH)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/linalg_ops.py\", line 161, in self_adjoint_eig\n    e, v = gen_linalg_ops._self_adjoint_eig_v2(tensor, compute_v=True, name=name)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_linalg_ops.py\", line 519, in _self_adjoint_eig_v2\n    compute_v=compute_v, name=name)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='SelfAdjointEigV2', l_rate=0.01, seed_r=1, seed = 1, print_ratio=5000, iterations=10001, mean = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 2.20383543]. r_op = sqrt(average degree) : 1.97484176581 . Loss associated with r_op: 0.662781930024. r_op assignments [[-0.84520184 -1.04351574 -1.09783797 -1.0543809  -1.01460308 -0.94833992\n",
      "  -0.83218621 -0.8432221  -0.98416898 -1.00625452  1.0952479   0.95841708\n",
      "   1.12949094  0.9512771   0.32414146  1.14211636  1.0952479   0.92739904\n",
      "   0.87003141  1.17413761]].\n",
      "0\n",
      "current r: [ 2.20383543], current loss: 0.666961367887, gradient of loss/r is [array([ 0.02081512])] and current assignments (up to sign) [[-0.84855647 -1.05652956 -1.09976991 -1.04980963 -1.00161017 -0.93801109\n",
      "  -0.84630963 -0.839108   -0.97165024 -1.01364658  1.09876804  0.9595762\n",
      "   1.12354594  0.93764335  0.32311382  1.15104672  1.09876804  0.928076\n",
      "   0.87132582  1.17617342]].\n",
      "5000\n",
      "current r: [ 2.20383335], current loss: 0.66696132456, gradient of loss/r is [array([ 0.02081508])] and current assignments (up to sign) [[-0.84855645 -1.05652945 -1.09976989 -1.04980967 -1.00161027 -0.93801117\n",
      "  -0.84630952 -0.83910804 -0.97165035 -1.01364652  1.09876801  0.95957619\n",
      "   1.12354599  0.93764346  0.32311383  1.15104665  1.09876801  0.92807599\n",
      "   0.87132581  1.17617341]].\n",
      "10000\n",
      "current r: [ 2.20383127], current loss: 0.666961281234, gradient of loss/r is [array([ 0.02081505])] and current assignments (up to sign) [[-0.84855642 -1.05652935 -1.09976988 -1.04980971 -1.00161038 -0.93801126\n",
      "  -0.8463094  -0.83910807 -0.97165045 -1.01364646  1.09876798  0.95957618\n",
      "   1.12354604  0.93764357  0.32311383  1.15104658  1.09876798  0.92807599\n",
      "   0.8713258   1.17617339]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='SelfAdjointEigV2', l_rate=0.0001, seed_r=100, seed =100, print_ratio=5000, iterations=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 2.1347376]. r_op = sqrt(average degree) : 1.97484176581 . Loss associated with r_op: 0.662781930024. r_op assignments [[-0.84520184 -1.04351574 -1.09783797 -1.0543809  -1.01460308 -0.94833992\n",
      "  -0.83218621 -0.8432221  -0.98416898 -1.00625452  1.0952479   0.95841708\n",
      "   1.12949094  0.9512771   0.32414146  1.14211636  1.0952479   0.92739904\n",
      "   0.87003141  1.17413761]].\n",
      "0\n",
      "current r: [ 2.1347376], current loss: 0.665565074398, gradient of loss/r is [array([-1612.51172712])] and current assignments (up to sign) [[-0.84765907 -1.0529035  -1.09927242 -1.05112167 -1.00523673 -0.94090613\n",
      "  -0.84234876 -0.84028683 -0.97514447 -1.01161039  1.09780201  0.95925908\n",
      "   1.12521453  0.94142082  0.32339089  1.14855368  1.09780201  0.9279009\n",
      "   0.87097998  1.17562105]].\n",
      "5000\n",
      "current r: [ 2.29598877], current loss: 0.668938341386, gradient of loss/r is [array([-1929.84889544])] and current assignments (up to sign) [[-0.84962699 -1.06101813 -1.1003416  -1.04814432 -0.99711386 -0.93440902\n",
      "  -0.8512409  -0.83761388 -0.96731799 -1.01614153  1.09994822  0.95996263\n",
      "   1.12146742  0.93298953  0.32278108  1.1541384   1.09994822  0.92827835\n",
      "   0.87173821  1.17684137]].\n",
      "10000\n",
      "current r: [ 2.48897366], current loss: 0.673319842502, gradient of loss/r is [array([-2310.9432781])] and current assignments (up to sign) [[-0.85148744 -1.06931695 -1.10126871 -1.04494481 -0.98877913 -0.9276953\n",
      "  -0.86044271 -0.83474938 -0.9592873  -1.02067844  1.10208521  0.96065973\n",
      "   1.11758641  0.92444808  0.32219661  1.15987238  1.10208521  0.92861063\n",
      "   0.87245532  1.17803042]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='grassman_with_2d', l_rate=0.0001, seed_r=1000, seed = 100, print_ratio=5000, iterations=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [-1.8652624]. r_op = sqrt(average degree) : 1.97484176581 . Loss associated with r_op: 0.662781930024. r_op assignments [[-0.84520184 -1.04351574 -1.09783797 -1.0543809  -1.01460308 -0.94833992\n",
      "  -0.83218621 -0.8432221  -0.98416898 -1.00625452  1.0952479   0.95841708\n",
      "   1.12949094  0.9512771   0.32414146  1.14211636  1.0952479   0.92739904\n",
      "   0.87003141  1.17413761]].\n",
      "0\n",
      "current r: [-1.8652624], current loss: 19.9780034437, gradient of loss/r is [array([-135.54116955])] and current assignments (up to sign) [[  7.62607883e-03  -1.51394537e-02   8.81541803e-03   2.86532468e-03\n",
      "    3.67478703e-03   1.75515683e-02  -3.10144305e-05  -2.96726668e-02\n",
      "    1.84014076e-02  -2.50765873e-02  -3.51752344e-02   8.10403541e-03\n",
      "    5.12098757e-02  -7.82659851e-02   6.90828779e-02  -2.96589109e-03\n",
      "   -3.51752344e-02   4.32792318e-02  -3.16514231e-02   2.25691657e-02]].\n",
      "5000\n",
      "current r: [-0.50985073], current loss: 19.8773870427, gradient of loss/r is [array([-676.23377394])] and current assignments (up to sign) [[-0.00065309  0.02000473 -0.01972799  0.0620368  -0.08819673 -0.1559837\n",
      "   0.00930973  0.07473428 -0.01232187  0.03826393  0.04262194 -0.02498327\n",
      "  -0.08691096  0.2303182  -0.09062296 -0.00811971  0.04262194 -0.05606416\n",
      "   0.03450129 -0.03328325]].\n",
      "10000\n",
      "current r: [ 6.25248686], current loss: 0.731286927033, gradient of loss/r is [array([ 4307.10427689])] and current assignments (up to sign) [[-0.85997616 -1.12755712 -1.10278195 -1.0179569  -0.92930375 -0.87850936\n",
      "  -0.92852352 -0.81082188 -0.90197873 -1.04949392  1.11550805  0.96497245\n",
      "   1.08889832  0.86651136  0.31936886  1.20102596  1.11550805  0.92942199\n",
      "   0.87588028  1.18471446]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='grassman_with_2d', l_rate=0.01, seed_r=1000, seed = 100, mean=-2.0, print_ratio=5000, iterations=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [-1.8652624]. r_op = sqrt(average degree) : 1.97484176581 . Loss associated with r_op: 0.662781930024. r_op assignments [[-0.84520184 -1.04351574 -1.09783797 -1.0543809  -1.01460308 -0.94833992\n",
      "  -0.83218621 -0.8432221  -0.98416898 -1.00625452  1.0952479   0.95841708\n",
      "   1.12949094  0.9512771   0.32414146  1.14211636  1.0952479   0.92739904\n",
      "   0.87003141  1.17413761]].\n",
      "0\n",
      "current r: [-1.8652624], current loss: 19.9780034437, gradient of loss/r is [array([-135.54116955])] and current assignments (up to sign) [[  7.62607883e-03  -1.51394537e-02   8.81541803e-03   2.86532468e-03\n",
      "    3.67478703e-03   1.75515683e-02  -3.10144305e-05  -2.96726668e-02\n",
      "    1.84014076e-02  -2.50765873e-02  -3.51752344e-02   8.10403541e-03\n",
      "    5.12098757e-02  -7.82659851e-02   6.90828779e-02  -2.96589109e-03\n",
      "   -3.51752344e-02   4.32792318e-02  -3.16514231e-02   2.25691657e-02]].\n",
      "5000\n",
      "current r: [-0.50985073], current loss: 19.8773870427, gradient of loss/r is [array([-676.23377394])] and current assignments (up to sign) [[-0.00065309  0.02000473 -0.01972799  0.0620368  -0.08819673 -0.1559837\n",
      "   0.00930973  0.07473428 -0.01232187  0.03826393  0.04262194 -0.02498327\n",
      "  -0.08691096  0.2303182  -0.09062296 -0.00811971  0.04262194 -0.05606416\n",
      "   0.03450129 -0.03328325]].\n",
      "10000\n",
      "current r: [ 6.25248686], current loss: 0.731286927033, gradient of loss/r is [array([ 4307.10427689])] and current assignments (up to sign) [[-0.85997616 -1.12755712 -1.10278195 -1.0179569  -0.92930375 -0.87850936\n",
      "  -0.92852352 -0.81082188 -0.90197873 -1.04949392  1.11550805  0.96497245\n",
      "   1.08889832  0.86651136  0.31936886  1.20102596  1.11550805  0.92942199\n",
      "   0.87588028  1.18471446]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='gradient_no_unitary_adjustment', l_rate=0.01, seed_r=1000, seed = 100, mean=-2.0, print_ratio=5000, iterations=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 0.1347376]. r_op = sqrt(average degree) : 1.97484176581 . Loss associated with r_op: 0.662781930024. r_op assignments [[-0.84520184 -1.04351574 -1.09783797 -1.0543809  -1.01460308 -0.94833992\n",
      "  -0.83218621 -0.8432221  -0.98416898 -1.00625452  1.0952479   0.95841708\n",
      "   1.12949094  0.9512771   0.32414146  1.14211636  1.0952479   0.92739904\n",
      "   0.87003141  1.17413761]].\n",
      "0\n",
      "current r: [ 0.1347376], current loss: 12.4597011854, gradient of loss/r is [array([-14743.81304199])] and current assignments (up to sign) [[-0.1036862  -0.14970453 -0.24773421 -0.31349935 -1.48332304 -0.59194844\n",
      "  -0.11930684 -0.28088319 -1.46729067 -0.18289341  0.06520975  0.14465215\n",
      "   0.26570022  1.53480505  0.08151308  0.10921135  0.06520975  0.1305858\n",
      "   0.04960254  0.15353925]].\n",
      "10000\n",
      "current r: [ 1.60911887], current loss: 0.661699232372, gradient of loss/r is [array([-644.48109844])] and current assignments (up to sign) [[-0.83653327 -1.01485487 -1.09218053 -1.06311159 -1.04301477 -0.9704901\n",
      "  -0.8019307  -0.85114742 -1.01154325 -0.98918302  1.08696109  0.95564671\n",
      "   1.14215833  0.98209496  0.32670739  1.12258023  1.08696109  0.92542806\n",
      "   0.86664843  1.16912497]].\n",
      "20000\n",
      "current r: [ 1.67356698], current loss: 0.661002146511, gradient of loss/r is [array([-754.93511377])] and current assignments (up to sign) [[-0.83848387 -1.020833   -1.09351687 -1.06144163 -1.03710986 -0.96593777\n",
      "  -0.80815017 -0.84962286 -1.00585401 -0.99283049  1.08875193  0.95625064\n",
      "   1.13956465  0.97557244  0.32614017  1.12664384  1.08875193  0.92589437\n",
      "   0.86741588  1.17023149]].\n",
      "30000\n",
      "current r: [ 1.74906049], current loss: 0.660796133442, gradient of loss/r is [array([-887.70668331])] and current assignments (up to sign) [[-0.84049962 -1.02726566 -1.09486353 -1.05955586 -1.03074392 -0.96099936\n",
      "  -0.81489495 -0.84790678 -0.99972054 -0.99670507  1.09064173  0.95688474\n",
      "   1.13674511  0.96861085  0.32554805  1.13102209  1.09064173  0.92636342\n",
      "   0.86820488  1.17138597]].\n",
      "40000\n",
      "current r: [ 1.83783115], current loss: 0.661198334804, gradient of loss/r is [array([-1048.2167797])] and current assignments (up to sign) [[-0.84255781 -1.03414103 -1.09619727 -1.05743835 -1.02392544 -0.95567536\n",
      "  -0.82216576 -0.84598588 -0.99315102 -1.00078768  1.09261963  0.95754498\n",
      "   1.13369884  0.96123364  0.32493691  1.13570946  1.09261963  0.92682757\n",
      "   0.86900657  1.17257883]].\n",
      "50000\n",
      "current r: [ 1.94265283], current loss: 0.662327255022, gradient of loss/r is [array([-1243.2052591])] and current assignments (up to sign) [[-0.8446312  -1.04143464 -1.0974916  -1.05507662 -1.01667515 -0.94997591\n",
      "  -0.82995092 -0.84385006 -0.98616542 -1.00505103  1.09467122  0.95822622\n",
      "   1.13043045  0.95347735  0.32431416  1.14069237  1.09467122  0.92727826\n",
      "   0.86981065  1.17379829]].\n",
      "60000\n",
      "current r: [ 2.06697335], current loss: 0.664294908901, gradient of loss/r is [array([-1480.78264342])] and current assignments (up to sign) [[-0.84668832 -1.04910631 -1.09871755 -1.05246385 -1.00902914 -0.94392362\n",
      "  -0.83822236 -0.84149434 -0.97879848 -1.00945855  1.09677819  0.9589222\n",
      "   1.12695182  0.94539407  0.32368878  1.14594705  1.09677819  0.92770635\n",
      "   0.87060558  1.17503032]].\n",
      "70000\n",
      "current r: [ 2.21505161], current loss: 0.667195815667, gradient of loss/r is [array([-1770.07438544])] and current assignments (up to sign) [[-0.84869411 -1.05709631 -1.09984479 -1.04960188 -1.00104287 -0.93755739\n",
      "  -0.84693055 -0.83892148 -0.97110365 -1.01396318  1.09891801  0.95962536\n",
      "   1.12328429  0.93705439  0.32307118  1.15143674  1.09891801  0.92810243\n",
      "   0.87137884  1.17625873]].\n",
      "80000\n",
      "current r: [ 2.39205905], current loss: 0.671092119354, gradient of loss/r is [array([-2119.88890896])] and current assignments (up to sign) [[-0.85061093 -1.06532082 -1.10084344 -1.04650505 -0.99279614 -0.93093693\n",
      "  -0.85599787 -0.83614529 -0.96315779 -1.0185062   1.10106344  0.96032684\n",
      "   1.11946143  0.928551    0.32247299  1.15710827  1.10106344  0.92845739\n",
      "   0.87211733  1.17746527]].\n",
      "90000\n",
      "current r: [ 2.60404793], current loss: 0.675994323956, gradient of loss/r is [array([-2535.30122927])] and current assignments (up to sign) [[-0.85240027 -1.07366657 -1.10168655 -1.04320504 -0.98439892 -0.92414828\n",
      "  -0.86531068 -0.83319488 -0.95506685 -1.02301621  1.10318219  0.9610163\n",
      "   1.11553239  0.92000293  0.32190668  1.16288778  1.10318219  0.92876316\n",
      "   0.87280785  1.17862993]].\n",
      "100000\n",
      "current r: [ 2.85757805], current loss: 0.681837936555, gradient of loss/r is [array([-3010.26405411])] and current assignments (up to sign) [[-0.85402507 -1.08198548 -1.10235343 -1.03975639 -0.97599759 -0.91730977\n",
      "  -0.87471043 -0.83011936 -0.94697187 -1.02740856  1.10523661  0.96168188\n",
      "   1.11156553  0.91155971  0.32138482  1.16867622  1.10523661  0.92901357\n",
      "   0.87343787  1.1797313 ]].\n",
      "110000\n",
      "current r: [ 3.15860445], current loss: 0.688458239186, gradient of loss/r is [array([-3513.86332802])] and current assignments (up to sign) [[-0.85545292 -1.09009036 -1.10283359 -1.03624202 -0.96778025 -0.9105772\n",
      "  -0.88398553 -0.82699259 -0.93905407 -1.0315859   1.10718381  0.9623102\n",
      "   1.10765156  0.9034045   0.32091913  1.17434524  1.10718381  0.92920541\n",
      "   0.8739963   1.18074738]].\n",
      "120000\n",
      "current r: [ 3.50999077], current loss: 0.695569064228, gradient of loss/r is [array([-3969.88657764])] and current assignments (up to sign) [[-0.85665971 -1.09775462 -1.10313088 -1.03277689 -0.95997824 -0.90414584\n",
      "  -0.89286745 -0.82391624 -0.93153638 -1.03544071  1.10897648  0.96288662\n",
      "   1.10390498  0.89575382  0.3205192   1.17973574  1.10897648  0.92933952\n",
      "   0.87447456  1.18165659]].\n",
      "130000\n",
      "current r: [ 3.90697942], current loss: 0.702757081344, gradient of loss/r is [array([-4239.63492713])] and current assignments (up to sign) [[-0.85763334 -1.10472015 -1.10326647 -1.02950665 -0.95285907 -0.89824498\n",
      "  -0.90103728 -0.82101844 -0.92467656 -1.03886128  1.11056518  0.96339594\n",
      "   1.10046093  0.88884962  0.32019121  1.18466222  1.11056518  0.92942147\n",
      "   0.87486754  1.18243941]].\n",
      "140000\n",
      "current r: [ 4.3309429], current loss: 0.709509617608, gradient of loss/r is [array([-4141.6292574])] and current assignments (up to sign) [[-0.85837642 -1.1107204  -1.1032787  -1.02659605 -0.9467033  -0.8931183\n",
      "  -0.90815241 -0.81844352 -0.91874498 -1.04174285  1.1119031   0.96382382\n",
      "   1.09746378  0.88293779  0.31993674  1.18892867  1.1119031   0.92946151\n",
      "   0.87517443  1.1830805 ]].\n",
      "150000\n",
      "current r: [ 4.74510582], current loss: 0.71529688413, gradient of loss/r is [array([-3556.80635521])] and current assignments (up to sign) [[-0.85890677 -1.11552639 -1.10321758 -1.02420157 -0.94175634 -0.88898231\n",
      "  -0.91390491 -0.816328   -0.91397814 -1.04400646  1.11295447  0.96415943\n",
      "   1.09504248  0.87822528  0.31975225  1.19236215  1.11295447  0.92947313\n",
      "   0.87539922  1.18357176]].\n",
      "160000\n",
      "current r: [ 5.10078644], current loss: 0.719716150114, gradient of loss/r is [array([-2589.23353386])] and current assignments (up to sign) [[-0.85925554 -1.11901897 -1.10313306 -1.02242585 -0.93815173 -0.88595974\n",
      "  -0.91811607 -0.81476071 -0.91050474 -1.04562616  1.11370727  0.96439942\n",
      "   1.09327114  0.87481279  0.31962923  1.19486686  1.11370727  0.92947012\n",
      "   0.87555088  1.18391633]].\n",
      "170000\n",
      "current r: [ 5.35970979], current loss: 0.722653152217, gradient of loss/r is [array([-1574.55314916])] and current assignments (up to sign) [[-0.85946331 -1.12126445 -1.10306071 -1.02126822 -0.9358298  -0.88400883\n",
      "  -0.92083749 -0.81373965 -0.90826733 -1.0466561   1.11418629  0.96455199\n",
      "   1.09212701  0.87262403  0.31955514  1.19648161  1.11418629  0.92946308\n",
      "   0.87564319  1.18413232]].\n",
      "180000\n",
      "current r: [ 5.5171651], current loss: 0.724334920016, gradient of loss/r is [array([-817.32367362])] and current assignments (up to sign) [[-0.85957427 -1.122525   -1.10301388 -1.02061286 -0.9345248  -0.88291101\n",
      "  -0.92237007 -0.81316184 -0.90700981 -1.04723032  1.1144535   0.96463705\n",
      "   1.09148289  0.87139709  0.31951529  1.19738964  1.1144535   0.92945737\n",
      "   0.87569324  1.18425167]].\n",
      "190000\n",
      "current r: [ 5.59889747], current loss: 0.725178765042, gradient of loss/r is [array([-380.08233475])] and current assignments (up to sign) [[-0.85962783 -1.12315086 -1.10298895 -1.020286   -0.93387644 -0.88236524\n",
      "  -0.92313229 -0.81287372 -0.90638505 -1.04751435  1.11458572  0.96467913\n",
      "   1.0911626   0.87078838  0.31949598  1.19784089  1.11458572  0.92945407\n",
      "   0.87571761  1.18431042]].\n",
      "200000\n",
      "current r: [ 5.6369057], current loss: 0.725564638413, gradient of loss/r is [array([-166.15869629])] and current assignments (up to sign) [[-0.85965187 -1.1234356  -1.10297725 -1.02013696 -0.93358137 -0.88211677\n",
      "  -0.92347937 -0.81274236 -0.90610071 -1.04764335  1.11464577  0.96469824\n",
      "   1.09101677  0.87051154  0.31948729  1.19804629  1.11464577  0.92945246\n",
      "   0.87572859  1.18433704]].\n",
      "210000\n",
      "current r: [ 5.65352157], current loss: 0.725732046805, gradient of loss/r is [array([-70.4962132])] and current assignments (up to sign) [[-0.85966221 -1.12355886 -1.10297211 -1.02007238 -0.93345362 -0.88200919\n",
      "  -0.92362966 -0.81268544 -0.90597762 -1.04769914  1.11467175  0.9647065\n",
      "   1.09095363  0.87039172  0.31948355  1.19813522  1.11467175  0.92945175\n",
      "   0.87573333  1.18434854]].\n",
      "220000\n",
      "current r: [ 5.66057119], current loss: 0.725802839456, gradient of loss/r is [array([-29.51311792])] and current assignments (up to sign) [[-0.85966656 -1.12361093 -1.10296992 -1.02004509 -0.93339965 -0.88196373\n",
      "  -0.92369317 -0.81266139 -0.90592561 -1.0477227   1.11468272  0.96470999\n",
      "   1.09092694  0.8703411   0.31948197  1.19817279  1.11468272  0.92945144\n",
      "   0.87573533  1.18435339]].\n",
      "230000\n",
      "current r: [ 5.6635225], current loss: 0.725832435459, gradient of loss/r is [array([-12.28530464])] and current assignments (up to sign) [[-0.85966838 -1.12363269 -1.10296901 -1.02003368 -0.9333771  -0.88194473\n",
      "  -0.92371971 -0.81265134 -0.90590388 -1.04773254  1.1146873   0.96471145\n",
      "   1.09091579  0.87031995  0.31948132  1.19818849  1.1146873   0.92945131\n",
      "   0.87573616  1.18435542]].\n",
      "240000\n",
      "current r: [ 5.66475103], current loss: 0.725844748105, gradient of loss/r is [array([-5.10170686])] and current assignments (up to sign) [[-0.85966914 -1.12364174 -1.10296863 -1.02002894 -0.93336772 -0.88193683\n",
      "  -0.92373075 -0.81264716 -0.90589484 -1.04773664  1.11468921  0.96471206\n",
      "   1.09091116  0.87031116  0.31948104  1.19819502  1.11468921  0.92945126\n",
      "   0.87573651  1.18435626]].\n",
      "250000\n",
      "current r: [ 5.6652612], current loss: 0.72584985993, gradient of loss/r is [array([-2.1164645])] and current assignments (up to sign) [[-0.85966945 -1.1236455  -1.10296847 -1.02002697 -0.93336382 -0.88193355\n",
      "  -0.92373533 -0.81264542 -0.90589108 -1.04773834  1.11469     0.96471231\n",
      "   1.09090923  0.8703075   0.31948093  1.19819773  1.11469     0.92945124\n",
      "   0.87573665  1.18435661]].\n",
      "260000\n",
      "current r: [ 5.66547285], current loss: 0.72585198038, gradient of loss/r is [array([-0.87765959])] and current assignments (up to sign) [[-0.85966958 -1.12364706 -1.1029684  -1.02002615 -0.93336221 -0.88193219\n",
      "  -0.92373723 -0.8126447  -0.90588953 -1.04773904  1.11469033  0.96471241\n",
      "   1.09090843  0.87030599  0.31948088  1.19819886  1.11469033  0.92945123\n",
      "   0.87573671  1.18435676]].\n",
      "270000\n",
      "current r: [ 5.66556061], current loss: 0.725852859655, gradient of loss/r is [array([-0.36388686])] and current assignments (up to sign) [[-0.85966964 -1.12364771 -1.10296838 -1.02002581 -0.93336154 -0.88193163\n",
      "  -0.92373802 -0.8126444  -0.90588888 -1.04773934  1.11469046  0.96471246\n",
      "   1.0909081   0.87030536  0.31948086  1.19819932  1.11469046  0.92945122\n",
      "   0.87573673  1.18435682]].\n",
      "280000\n",
      "current r: [ 5.665597], current loss: 0.725853224205, gradient of loss/r is [array([-0.15086052])] and current assignments (up to sign) [[-0.85966966 -1.12364797 -1.10296837 -1.02002567 -0.93336126 -0.88193139\n",
      "  -0.92373835 -0.81264428 -0.90588861 -1.04773946  1.11469052  0.96471247\n",
      "   1.09090796  0.8703051   0.31948085  1.19819952  1.11469052  0.92945122\n",
      "   0.87573674  1.18435684]].\n",
      "290000\n",
      "current r: [ 5.66561209], current loss: 0.72585337534, gradient of loss/r is [array([-0.06254203])] and current assignments (up to sign) [[-0.85966967 -1.12364808 -1.10296836 -1.02002561 -0.93336114 -0.8819313\n",
      "  -0.92373848 -0.81264423 -0.9058885  -1.04773951  1.11469054  0.96471248\n",
      "   1.09090791  0.87030499  0.31948085  1.1981996   1.11469054  0.92945122\n",
      "   0.87573675  1.18435685]].\n",
      "300000\n",
      "current r: [ 5.66561834], current loss: 0.725853437995, gradient of loss/r is [array([-0.02592764])] and current assignments (up to sign) [[-0.85966967 -1.12364813 -1.10296836 -1.02002559 -0.9333611  -0.88193126\n",
      "  -0.92373854 -0.8126442  -0.90588846 -1.04773953  1.11469055  0.96471248\n",
      "   1.09090788  0.87030495  0.31948085  1.19819963  1.11469055  0.92945122\n",
      "   0.87573675  1.18435686]].\n",
      "310000\n",
      "current r: [ 5.66562094], current loss: 0.72585346397, gradient of loss/r is [array([-0.0107486])] and current assignments (up to sign) [[-0.85966967 -1.12364815 -1.10296836 -1.02002558 -0.93336108 -0.88193124\n",
      "  -0.92373856 -0.8126442  -0.90588844 -1.04773954  1.11469056  0.96471249\n",
      "   1.09090787  0.87030493  0.31948085  1.19819964  1.11469056  0.92945122\n",
      "   0.87573675  1.18435686]].\n",
      "320000\n",
      "current r: [ 5.66562201], current loss: 0.725853474738, gradient of loss/r is [array([-0.00445594])] and current assignments (up to sign) [[-0.85966967 -1.12364816 -1.10296836 -1.02002557 -0.93336107 -0.88193123\n",
      "  -0.92373857 -0.81264419 -0.90588843 -1.04773954  1.11469056  0.96471249\n",
      "   1.09090787  0.87030492  0.31948085  1.19819965  1.11469056  0.92945122\n",
      "   0.87573675  1.18435686]].\n",
      "330000\n",
      "current r: [ 5.66562246], current loss: 0.725853479202, gradient of loss/r is [array([-0.00184726])] and current assignments (up to sign) [[-0.85966967 -1.12364816 -1.10296836 -1.02002557 -0.93336107 -0.88193123\n",
      "  -0.92373858 -0.81264419 -0.90588843 -1.04773954  1.11469056  0.96471249\n",
      "   1.09090787  0.87030492  0.31948085  1.19819965  1.11469056  0.92945122\n",
      "   0.87573675  1.18435686]].\n",
      "340000\n",
      "current r: [ 5.66562264], current loss: 0.725853481053, gradient of loss/r is [array([-0.0007658])] and current assignments (up to sign) [[-0.85966968 -1.12364816 -1.10296836 -1.02002557 -0.93336106 -0.88193123\n",
      "  -0.92373858 -0.81264419 -0.90588842 -1.04773954  1.11469056  0.96471249\n",
      "   1.09090787  0.87030492  0.31948085  1.19819965  1.11469056  0.92945122\n",
      "   0.87573675  1.18435686]].\n",
      "350000\n",
      "current r: [ 5.66562272], current loss: 0.72585348182, gradient of loss/r is [array([-0.00031747])] and current assignments (up to sign) [[-0.85966968 -1.12364816 -1.10296836 -1.02002557 -0.93336106 -0.88193123\n",
      "  -0.92373858 -0.81264419 -0.90588842 -1.04773954  1.11469056  0.96471249\n",
      "   1.09090787  0.87030492  0.31948085  1.19819965  1.11469056  0.92945122\n",
      "   0.87573675  1.18435686]].\n",
      "360000\n",
      "current r: [ 5.66562275], current loss: 0.725853482138, gradient of loss/r is [array([-0.00013161])] and current assignments (up to sign) [[-0.85966968 -1.12364816 -1.10296836 -1.02002557 -0.93336106 -0.88193123\n",
      "  -0.92373858 -0.81264419 -0.90588842 -1.04773954  1.11469056  0.96471249\n",
      "   1.09090787  0.87030492  0.31948085  1.19819965  1.11469056  0.92945122\n",
      "   0.87573675  1.18435686]].\n",
      "370000\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='gradient_no_unitary_adjustment', l_rate=0.0001, seed_r=1000, seed = 100, mean=0.0, print_ratio=10000, iterations=400001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
