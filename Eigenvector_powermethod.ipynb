{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals\n",
    "\n",
    "    -implement spectral clustering\n",
    "    -implement belief propagation\n",
    "    -implement it with power method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla Belief propagation algorithm\n",
    "\n",
    "Input: a stochastic block model (adjaceny matrix)\n",
    "\n",
    "1) for each v, v' adajecnt, randomly draw y^1_vv' from N(0,1)\n",
    "\n",
    "2) find all cycles in G of length r or less.  To do this, we may have to multiply r times, but avoid the backtracking ones.  \n",
    "\n",
    "3) for each 1<t<m and each adjacent v, v' set y^t_vv' = sum of all the y's up to that t....since we seeded it with N(0,1) we will have the value for y^1.  Howver, if (v,v') is part of a cycle of r or less, don't do this.  Instead, \n",
    "4) if your edge is part of a r cycle, then we subtract from the previous sum, the accumulated influence of the other adjacent edge in our cycle, unless the r cycle is the same lenght as the time steps, in which case we just subtract the original randomized N(0,1)\n",
    "\n",
    "5) Let Y be the matrix composed of all y_vv' summed up with edge vertices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first try implementing power method in tensorflow\n",
    "\n",
    "#feed in graphs, as in adjaceny matrices, or just nodes and connected edges?\n",
    "\n",
    "#each layer will feed in a random vector, then multiply our matrix by the random vector\n",
    "#many times.  At the end, we subtract away the orignal vector component\n",
    "\n",
    "#next layer we just randomize another vector...\n",
    "\n",
    "#how many layers.  The depth can be learned by adding identify layers, which is essentially \n",
    "#self multiplication.  \n",
    "\n",
    "#adjaceny matrix to graph laplacian \n",
    "\n",
    "#suppose for simplicity that our graph has the same dimnension\n",
    "\n",
    "#10by10 graph, so feed adjacency matrix\n",
    "dim_graph = 10\n",
    "dim_subspace = 3\n",
    "\n",
    "x = tf.placeholder(\"float\", shape = [dim_graph, dim_graph])\n",
    "y = tf.placeholder(\"float\", shape = [None, dim_graph])\n",
    "#y is our community classification answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize the k random vectors (whcih should be tuned)\n",
    "#that we are feeding to the laplacian operator\n",
    "d = {}\n",
    "for i in range(1,dim_subspace):\n",
    "    d[\"v{}\".format(i)] = tf.Variable(tf.random_normal(shape=[1, dim_graph], \n",
    "                         mean=0.0, \n",
    "                         stddev = 1.0))\n",
    "\n",
    "#may have to define another way...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#laplacian\n",
    "\n",
    "#diffusion:\n",
    "\n",
    "def DiffusionVertex(vertex, graph, vector):\n",
    "    #assume graph is in form of adjacency matrix \n",
    "    neighbour_vector = tf.slice(graph, begin=[vertex,0], size=[1,dim_graph])\n",
    "    prod = tf.mul(vector, neighbour_vector)\n",
    "    diffusion = tf.reduce_sum(prod, 1)\n",
    "    return(diffusion)\n",
    "\n",
    "def GraphLaplacian(graph, vector):\n",
    "    #takes in graph and vector on graph indices, returns new vector\n",
    "    tmp_v = tf.constant(range(dim_graph))\n",
    "    def DiffusionVertex_1(vertex):\n",
    "        return DiffusionVertex(vertex, graph, vector)\n",
    "    diffusion_vector = tf.map_fn(DiffusionVertex_1, tmp_v) \n",
    "    \n",
    "    return tf.reduce_sum(diffusion_vector)\n",
    "\n",
    "def euclidean_norm(tensor): #need to have this for tf to work\n",
    "    square_tensor = tf.square(tensor)\n",
    "    norm_squared = tf.reduce_sum(square_tensor)\n",
    "    euclidean_norm = tf.sqrt(norm_squared)\n",
    "    return euclidean_norm\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,   4.,   9.,  16.,  25.,  36.], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elems = np.array([1, 2, 3, 4, 5, 6], dtype=\"float32\")\n",
    "squares = tf.map_fn(lambda x: x * x, elems)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(squares)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  3.,   8.,  15.],\n",
      "       [  6.,  12.,  20.],\n",
      "       [  9.,  16.,  25.]], dtype=float32), array([[ 3.,  4.,  5.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#test to see if your diffusion operator works\n",
    "\n",
    "x = tf.constant([[1,2,3], [2,3,4], [3,4,5]], dtype=tf.float32)\n",
    "y = tf.Variable(tf.random_normal(shape=[3], \n",
    "                         mean=0.0, \n",
    "                         stddev = 1.0))\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "answer = tf.mul(x,tf.slice(x, [2,0], [1,3])),tf.slice(x, [2,0], [1,3])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print sess.run(answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.58576\n"
     ]
    }
   ],
   "source": [
    "dim_graph = 3\n",
    "dim_subspace = 3\n",
    "\n",
    "x = tf.constant([[1,2,3], [2,3,4], [3,4,5]])\n",
    "x = tf.cast(x, tf.float32)\n",
    "f_x = tf.Variable(tf.random_normal(shape=[3], \n",
    "                         mean=0.0, \n",
    "                         stddev = 1.0))\n",
    "\n",
    "blah = [DiffusionVertex(i, graph=x, vector=f_x) for i in range(dim_graph)]\n",
    "blah = tf.reduce_sum(tf.pack(blah))\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print sess.run(blah)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"div_41:0\", shape=(3, 1), dtype=float32)\n",
      "[array([[ 0.38511941],\n",
      "       [ 0.5595153 ],\n",
      "       [ 0.73391116]], dtype=float32), array([[-0.38507888],\n",
      "       [-0.55950832],\n",
      "       [-0.73393774]], dtype=float32), array([[-0.38455883],\n",
      "       [-0.55941874],\n",
      "       [-0.73427862]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#now let continue with the powermethod from the above:\n",
    "#since the largest k eigenvalues is supposed to come from the adjacney matrix, \n",
    "#we leave the diffusion component for now\n",
    "\n",
    "dim_graph = 3\n",
    "dim_subspace = 3\n",
    "\n",
    "#x = tf.placeholder(tf.float32, shape=(dim_graph, dim_graph))\n",
    "x = tf.cast(x, tf.float32)\n",
    "\n",
    "answer_lst = []\n",
    "\n",
    "lmbda_lst = []\n",
    "\n",
    "f = {}\n",
    "for i in range(1,dim_subspace+1):\n",
    "    f[\"v{}\".format(i)] = tf.Variable(tf.random_normal(shape=[dim_graph, 1], \n",
    "                         mean=0.0, \n",
    "                         stddev = 1.0))\n",
    "    \n",
    "    \n",
    "\n",
    "adjacency_vector = tf.matmul(x, f[\"v1\"])\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "adjacency_vector = tf.matmul(x, adjacency_vector)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "adjacency_vector = tf.matmul(x, adjacency_vector)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "print adjacency_vector\n",
    "answer_lst.append(adjacency_vector)\n",
    "\n",
    "#lmbda_lst.append(euclidean_norm(tf.matmul(x, adjacency_vector)) #since it is already normalized\n",
    "\n",
    "                 \n",
    "adjacency_vector = tf.sub(f[\"v2\"], adjacency_vector)\n",
    "adjacency_vector = tf.matmul(x, adjacency_vector)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "adjacency_vector = tf.matmul(x, adjacency_vector)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "adjacency_vector = tf.matmul(x, adjacency_vector)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "answer_lst.append(adjacency_vector)\n",
    "#lmbda_lst.append(euclidean_norm(tf.matmul(x, adjacency_vector)) #since it is already normalized\n",
    "\n",
    "\n",
    "adjacency_vector = tf.sub(tf.sub(f[\"v3\"], adjacency_vector), answer_lst[1])\n",
    "adjacency_vector = tf.matmul(x, adjacency_vector)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "adjacency_vector = tf.matmul(x, adjacency_vector)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "adjacency_vector = tf.matmul(x, adjacency_vector)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "answer_lst.append(adjacency_vector)\n",
    "#lmbda_lst.append(euclidean_norm(tf.matmul(x, adjacency_vector)) #since it is already normalized\n",
    "\n",
    "                 \n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print sess.run(answer_lst, feed_dict = {x: [[1,2,3], [2,3,4], [3,4,5]]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 18.08835174],\n",
       "        [ 26.55926778],\n",
       "        [ 35.03018382]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matrix([[1,2,3], [2,3,4], [3,4,5]])*np.matrix([[ 2.25037885], [2.82363868], [3.39689851]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 18.0883522 ]\n",
      " [ 26.55926895]\n",
      " [ 35.0301857 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y = tf.matmul(tf.constant([[1.0,2.0,3.], [2.,3.,4.], [3.,4.,5.]], shape=[3,3]),\n",
    "          tf.constant([ 2.25037885, 2.82363868, 3.39689851], shape=[3,1]))\n",
    "with tf.Session() as sess:\n",
    "    print sess.run(y)\n",
    "    \n",
    "    #ok, they match, so at least that is not wrong....\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.67316258  0.77078715  0.84736085  0.14172805  0.08583192  0.80161512]\n",
      " [ 0.80306613  0.86274484  0.22028874  0.01986374  0.04525851  0.68746425]\n",
      " [ 0.63863501  0.21335319  0.95988464  0.52516139  0.29216087  0.15448989]\n",
      " [ 0.29623571  0.97427035  0.69251197  0.82959694  0.96867847  0.11098715]\n",
      " [ 0.47638091  0.97155361  0.53450427  0.846212    0.8239428   0.72508975]\n",
      " [ 0.63984247  0.29791923  0.17945452  0.2135168   0.97254578  0.59230027]]\n",
      "[-0.35918558+0.j -0.27768321+0.j -0.33576492+0.j -0.49008081+0.j\n",
      " -0.54394370+0.j -0.38090121+0.j]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(6,6)\n",
    "eigenValues,eigenVectors = np.linalg.eig(x)\n",
    "y = eigenVectors[:, eigenValues.argmax()]\n",
    "\n",
    "print x\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.39295191],\n",
      "       [ 0.28276429],\n",
      "       [ 0.41409135],\n",
      "       [ 0.08971775],\n",
      "       [ 0.36310893],\n",
      "       [ 0.6739912 ]], dtype=float32), array([[ 0.73673558],\n",
      "       [-0.76855552],\n",
      "       [ 0.18964009],\n",
      "       [-0.01761648],\n",
      "       [ 0.50768691],\n",
      "       [ 0.74849772]], dtype=float32)]\n",
      "[array([[ 0.38408989],\n",
      "       [ 0.28253925],\n",
      "       [ 0.3526527 ],\n",
      "       [ 0.42363718],\n",
      "       [ 0.52225506],\n",
      "       [ 0.44279084]], dtype=float32), array([[ 0.39211223],\n",
      "       [-0.01557965],\n",
      "       [ 0.33084738],\n",
      "       [ 0.28736705],\n",
      "       [ 0.53432196],\n",
      "       [ 0.52184629]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#now let try to get the first egenvector right with some backprop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dim_graph = len(x)\n",
    "dim_subspace = 1\n",
    "    \n",
    "x = tf.cast(x, tf.float32)\n",
    "y = tf.cast(y, tf.float32)\n",
    "\n",
    "f = {}\n",
    "for i in range(1,dim_subspace+1):\n",
    "    f[\"v{}\".format(i)] = tf.Variable(tf.random_uniform(shape = [dim_graph,1],\n",
    "                                                       minval=-1,\n",
    "                                                       maxval = 1,\n",
    "                                                       dtype = tf.float32))\n",
    "y = f[\"v1\"]\n",
    "   \n",
    "\n",
    "adjacency_vector = tf.matmul(x, y)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(y-adjacency_vector))                \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "train = optimizer.minimize(loss = cost, var_list = [f[\"v1\"]])\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "for i in xrange(1000):\n",
    "    sess.run(train)\n",
    "    if i%500==0:\n",
    "        print sess.run([adjacency_vector, f[\"v1\"]])\n",
    "#seems to be accurate, though not really fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.24253563  0.9701425 ]\n"
     ]
    }
   ],
   "source": [
    "#now let try to get an eigenspace with backprop\n",
    "dim_graph = 3\n",
    "dim_subspace = 2\n",
    "\n",
    "\n",
    "x = [[1,0,0], [0,1,1], [0,0,5]]\n",
    "eigenValues,eigenVectors = np.linalg.eig(x)\n",
    "y = eigenVectors[:, eigenValues.argmax()]\n",
    "    \n",
    "x = tf.cast(x, tf.float32)\n",
    "y = tf.cast(y, tf.float32)\n",
    "\n",
    "f = {}\n",
    "for i in range(1,dim_subspace+1):\n",
    "    f[\"v{}\".format(i)] = tf.Variable(tf.random_uniform(shape = [dim_graph,1],\n",
    "                                                       minval=-1,\n",
    "                                                       maxval = 1,\n",
    "                                                       dtype = tf.float32))\n",
    "y = f[\"v1\"]\n",
    "   \n",
    "\n",
    "adjacency_vector = tf.matmul(x, y)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(y-adjacency_vector))                \n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "\n",
    "train = optimizer.minimize(loss = cost, var_list = [f[\"v1\"]])\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "for i in xrange(10000):\n",
    "    sess.run(train)\n",
    "    if i%500==0:\n",
    "        print sess.run([adjacency_vector, f[\"v1\"]])\n",
    "#seems to be accurate, though not really fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above does not do well with certain vectors, but with the identity it is not totally off....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with using tf.map_fn below.  Using list instead, however this the step that is parallelizable, hence not sure that writing in terms of for loop is ideal... How will tensorflow handle it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(2,2)\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.3163\n"
     ]
    }
   ],
   "source": [
    "dim_graph = 3\n",
    "dim_subspace = 3\n",
    "\n",
    "x = tf.constant([[1,2,3], [2,3,4], [3,4,5]])\n",
    "x = tf.cast(x, tf.float32)\n",
    "y = tf.Variable(tf.random_normal(shape=[dim_graph], \n",
    "                         mean=0.0, \n",
    "                         stddev = 1.0))\n",
    "\n",
    "tmp_v = np.array(range(dim_graph), dtype=\"int\")\n",
    "\n",
    "def DiffusionVertex_1(vertex):\n",
    "    return DiffusionVertex(vertex, x, y)\n",
    "\n",
    "diffusion_vector = [DiffusionVertex_1(i) for i in range(dim_graph)]\n",
    "#diffusion_vector = tf.scan(DiffusionVertex_1, tmp_v)\n",
    "diffusion_vector = tf.pack(diffusion_vector)\n",
    "\n",
    "#answer = tf.shape(tmp_v)\n",
    "answer = diffusion_vector\n",
    "\n",
    "#answer = tf.shape(diffusion_vector)\n",
    "answer = tf.reduce_sum(diffusion_vector)\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print sess.run(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
