{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import  networkx as nx\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.],\n",
       "       [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def balanced_stochastic_blockmodel(communities=2, groupsize=3, p_in=1.0, p_out=0.0):\n",
    "    #gives dense adjacency matrix representaiton of randomly generated SBM with balanced community size\n",
    "\n",
    "    G = nx.planted_partition_graph(l=communities, k=groupsize, p_in=p_in, p_out =p_out)\n",
    "    A = nx.adjacency_matrix(G).todense()\n",
    "    \n",
    "    return A\n",
    "\n",
    "\n",
    "communities = 2 #number of communities, chance to \n",
    "group_size = 6 #number of nodes in each communitites (balanced so far)\n",
    "dim_graph = communities*group_size\n",
    "A = np.asarray(balanced_stochastic_blockmodel(communities=communities, groupsize=group_size, p_in=0.5, p_out=0.2)).astype(np.double)\n",
    "\n",
    "data = [np.asarray(balanced_stochastic_blockmodel(communities=communities, groupsize=group_size, p_in=0.5, p_out=0.2)).astype(np.float64) for i in range(100)]\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 3.05923993]), -17.582002877105602, array([[ 0.48523878, -0.67196621],\n",
      "       [-0.98420493,  1.006611  ]])]\n",
      "[array([-1.99537013]), -14.645440402921187, array([[ 0.70392115, -1.39593814],\n",
      "       [ 0.351965  ,  0.15934262]])]\n",
      "[array([-0.92830202]), -8.6270355575422322, array([[ 1.87739813, -0.60799193],\n",
      "       [ 1.78306146, -0.52507174]])]\n",
      "[array([-0.65208576]), -14.781249747021276, array([[ 0.65875418, -1.1032957 ],\n",
      "       [ 0.83589625,  1.45952181]])]\n",
      "[array([ 0.51431394]), -6.4003825869010535, array([[-0.06595127, -0.43541562],\n",
      "       [ 0.28237509,  0.78109758]])]\n",
      "[array([-0.15450143]), -14.609192468897993, array([[-0.31750946,  0.83904036],\n",
      "       [-0.87075515,  1.5013767 ]])]\n",
      "[array([-1.55449834]), -28.915727487177978, array([[-0.70682717,  2.27252048],\n",
      "       [-0.36664564,  0.84933227]])]\n",
      "[array([-0.25043337]), -39.765463118162693, array([[ 1.37487904,  3.01357896],\n",
      "       [-2.21581987, -0.49357137]])]\n",
      "[array([ 0.21729751]), -25.181655264180588, array([[-0.52254283, -1.09430227],\n",
      "       [ 1.11197007,  1.6329207 ]])]\n",
      "[array([-1.67115042]), -16.931641532407113, array([[ 0.23944986, -0.69845912],\n",
      "       [ 0.12526677,  1.78585925]])]\n"
     ]
    }
   ],
   "source": [
    "communities = 6\n",
    "Adj = tf.placeholder(tf.float64)\n",
    "\n",
    "Diag = tf.diag(tf.reduce_sum(Adj,0))\n",
    "Diag = tf.cast(Diag, tf.float64)\n",
    "\n",
    "r =  tf.Variable(tf.random_normal(shape=[1], mean=0.0,\n",
    "                                 stddev=2.0, dtype=tf.float64,\n",
    "                                 seed=None, name=None))\n",
    "\n",
    "Bethe_Hesse_neg = (tf.square(r)-1)*tf.diag(tf.ones(shape=[dim_graph], dtype=tf.float64))-tf.mul(r, Adj)+Diag \n",
    "\n",
    "eigenval, eigenvec = tf.self_adjoint_eig(Bethe_Hesse_neg)\n",
    "\n",
    "true_assignment_a = tf.concat(0, [tf.zeros([communities], dtype=tf.float64),\n",
    "                                      tf.ones([communities], dtype=tf.float64)])\n",
    "true_assignment_b = tf.concat(0, [tf.ones([communities], dtype=tf.float64),\n",
    "                                      tf.zeros([communities], dtype=tf.float64)])\n",
    "\n",
    "\n",
    "y_a = tf.pack([true_assignment_a, true_assignment_b]) \n",
    "y_b = tf.pack([true_assignment_b, true_assignment_a]) \n",
    "\n",
    "\n",
    "eigenvec_proj = tf.slice(eigenvec, [0,0], [dim_graph, 2])\n",
    "\n",
    "centers = tf.Variable(tf.random_normal(shape=[2,2],\n",
    "                                       mean=0.0, stddev=1.0, dtype=tf.float64,seed=None, name=None))\n",
    "\n",
    "expanded_vectors = tf.expand_dims(eigenvec_proj, 0)\n",
    "expanded_centers = tf.expand_dims(centers, 1)\n",
    "\n",
    "distances = tf.transpose(tf.reduce_sum(tf.square(tf.sub(expanded_vectors,\n",
    "                                                        expanded_centers)), 2))\n",
    "\n",
    "assignments = tf.nn.softmax(distances)\n",
    "\n",
    "loss_vec_a = y_a*tf.log(tf.transpose(assignments))\n",
    "loss_vec_b = y_b*tf.log(tf.transpose(assignments))\n",
    "\n",
    "loss = tf.minimum(tf.reduce_sum(loss_vec_a), tf.reduce_sum(loss_vec_a))\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "eigenvec_grad = tf.gradients(eigenvec, r)\n",
    "loss_grad = tf.gradients(loss, r)\n",
    "\n",
    "init = tf.initialize_variables([r, centers])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "        sess.run(init)\n",
    "#       sess.run(train)\n",
    "        print sess.run([r, loss, centers], feed_dict = {Adj: data[i]})\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now with training uncommentedout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.80005489]), [array([ 9.79694719])], [array([-1.98701186])], 80.859714974926163, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([-0.43659751]), [array([-4.58307354])], [array([ 1.68392781])], 77.553692330113861, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([-0.4539848]), [array([ 0.31788765])], [array([-1.11917682])], 81.061766717813185, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([ 0.12698334]), [array([ 11.42162481])], [array([-8.62615963])], 79.439033997694281, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([-0.3181855]), [array([ 6.73951226])], [array([ 5.23449881])], 82.768470799372238, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([-0.69595131]), [array([-1.93966004])], [array([ 0.28569067])], 74.435438953679238, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([ 0.23596093]), [array([-1.82086183])], [array([-3.64854591])], 83.951116555286973, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([ 1.14461804]), [array([ 0.03115408])], [array([ 0.47209244])], 76.860052868654321, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([ 1.40093678]), [array([-1.98725797])], [array([ 1.11279248])], 75.876324155799978, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([ 1.3110159]), [array([ 1.08848959])], [array([-1.0653423])], 77.552544538831427, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n"
     ]
    }
   ],
   "source": [
    "communities = 6\n",
    "Adj = tf.placeholder(tf.float64)\n",
    "Adj = tf.cast(Adj, tf.float64)\n",
    "\n",
    "Diag = tf.diag(tf.reduce_sum(Adj,0))\n",
    "Diag = tf.cast(Diag, tf.float64)\n",
    "\n",
    "#r =  tf.Variable(tf.constant(2.1), dtype = tf.float64)\n",
    "r = tf.Variable(tf.random_normal(shape=[1], mean=0.0, stddev=2.0, dtype=tf.float64,seed=None, name=None))\n",
    "\n",
    "Bethe_Hesse_neg = (tf.square(r)-1)*tf.diag(tf.ones(shape=[dim_graph], dtype=tf.float64))-tf.mul(r, Adj)+Diag \n",
    "\n",
    "eigenval, eigenvec = tf.self_adjoint_eig(Bethe_Hesse_neg)\n",
    "\n",
    "true_assignment_a = tf.concat(0, [tf.zeros([communities], dtype=tf.float64),\n",
    "                                      tf.ones([communities], dtype=tf.float64)])\n",
    "true_assignment_b = tf.concat(0, [tf.ones([communities], dtype=tf.float64),\n",
    "                                      tf.zeros([communities], dtype=tf.float64)])\n",
    "\n",
    "y_a = tf.pack([true_assignment_a, true_assignment_b]) \n",
    "\n",
    "eigenvec_proj = tf.nn.softmax(tf.slice(eigenvec, [0,0], [dim_graph, 2]))\n",
    "eigenvec_proj = tf.cast(tf.reduce_sum(eigenvec, 1), dtype = tf.float64)\n",
    "\n",
    "\n",
    "loss =  tf.minimum(tf.reduce_sum(tf.square(tf.sub(eigenvec, true_assignment_a))), \n",
    "              tf.reduce_sum(tf.square(tf.sub(eigenvec, true_assignment_b))))\n",
    "#tf.reduce_sum(tf.square(tf.sub(c, true_assignment_a)))\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "train = optimizer.minimize(loss, var_list=[r])\n",
    "\n",
    "eigenvec_grad = tf.gradients(eigenvec, r)\n",
    "loss_grad = tf.gradients(loss, r)\n",
    "\n",
    "init = tf.initialize_variables([r])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(10):\n",
    "        sess.run(train, feed_dict = {Adj: data[i]})\n",
    "        print sess.run([r, loss_grad, eigenvec_grad, loss, y_a], feed_dict = {Adj: data[i]})\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?tf.pack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tw = tf.train.SummaryWriter('./logDir',sess.graph)\n",
    "\n",
    "# Train your network\n",
    "\n",
    "tw.flush()\n",
    "tw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities = 2 #number of communities, chance to \n",
    "group_size = 5\n",
    "data = [np.asarray(balanced_stochastic_blockmodel(communities=communities, groupsize=group_size, p_in=0.8, p_out=0.3)).astype(np.float64) for i in range(1000)]\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.786448819254435, array([1, 1, 1, 1, 1, 0, 1, 1, 0, 1]), array([ 0.95610812]), 4.2000000000000002, array([[-0.15236526,  0.81405732],\n",
      "       [ 0.14181189, -0.15286577]]), array([[-0.31730845,  0.32506619],\n",
      "       [-0.31588982, -0.07618329]])]\n",
      "[5.1170808988547529, array([1, 1, 1, 1, 1, 0, 0, 0, 1, 1]), array([ 1.64406844]), 5.2000000000000002, array([[-0.15236526,  0.81405732],\n",
      "       [ 0.14181189, -0.15286577]]), array([[-0.28199102,  0.41914946],\n",
      "       [-0.3288483 , -0.15155647]])]\n",
      "[4.7725667909998837, array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0]), array([ 2.32061393]), 5.7999999999999998, array([[-0.15236526,  0.81405732],\n",
      "       [ 0.14181189, -0.15286577]]), array([[-0.32764608,  0.31990748],\n",
      "       [-0.31036049, -0.15957133]])]\n",
      "[5.3100322273663849, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([ 2.55736504]), 4.5999999999999996, array([[-0.15236526,  0.81405732],\n",
      "       [ 0.14181189, -0.15286577]]), array([[        nan,         nan],\n",
      "       [ 0.31444076, -0.01014541]])]\n",
      "[4.7497377731069506, array([1, 1, 1, 1, 1, 0, 1, 0, 0, 0]), array([ 2.75341863]), 5.2000000000000002, array([[-0.15236526,  0.81405732],\n",
      "       [ 0.14181189, -0.15286577]]), array([[-0.27421628,  0.36130056],\n",
      "       [-0.33686333, -0.17969246]])]\n",
      "[5.664707299349832, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([ 2.86778184]), 4.4000000000000004, array([[-0.15236526,  0.81405732],\n",
      "       [ 0.14181189, -0.15286577]]), array([[        nan,         nan],\n",
      "       [ 0.30769027, -0.05031229]])]\n",
      "[5.7009146885771784, array([1, 1, 1, 0, 1, 1, 0, 0, 1, 0]), array([ 2.87949377]), 3.6000000000000001, array([[-0.15236526,  0.81405732],\n",
      "       [ 0.14181189, -0.15286577]]), array([[-0.21262942,  0.42044559],\n",
      "       [-0.3616275 , -0.14572089]])]\n",
      "[5.7766127147530852, array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([ 2.92850867]), 6.2000000000000002, array([[-0.15236526,  0.81405732],\n",
      "       [ 0.14181189, -0.15286577]]), array([[ 0.28555   ,  0.47798064],\n",
      "       [ 0.31782382, -0.06599248]])]\n",
      "[5.1140831398684625, array([1, 1, 0, 1, 1, 1, 1, 1, 1, 1]), array([ 3.09480111]), 4.0, array([[-0.15236526,  0.81405732],\n",
      "       [ 0.14181189, -0.15286577]]), array([[ 0.18438365,  0.39883743],\n",
      "       [ 0.32195866, -0.01732962]])]\n",
      "[4.7972077757581966, array([1, 1, 1, 1, 1, 0, 1, 0, 0, 0]), array([ 3.12310083]), 4.4000000000000004, array([[-0.15236526,  0.81405732],\n",
      "       [ 0.14181189, -0.15286577]]), array([[-0.24786184,  0.40866024],\n",
      "       [-0.35183734, -0.184257  ]])]\n"
     ]
    }
   ],
   "source": [
    "communities = 2\n",
    "group_size=5\n",
    "dim_graph=communities*group_size\n",
    "\n",
    "Adj = tf.placeholder(tf.float64)\n",
    "Adj = tf.cast(Adj, tf.float64)\n",
    "\n",
    "Diag = tf.diag(tf.reduce_sum(Adj,0))\n",
    "Diag = tf.cast(Diag, tf.float64)\n",
    "\n",
    "average_degree = tf.reduce_sum(Adj)/dim_graph\n",
    "\n",
    "#r =  tf.Variable(tf.constant(2.1), dtype = tf.float64)\n",
    "r = tf.Variable(tf.random_normal(shape=[1], mean=0.0, stddev=2.0, dtype=tf.float64,seed=None, name=None))\n",
    "\n",
    "Bethe_Hesse_neg = (tf.square(r)-1)*tf.diag(tf.ones(shape=[dim_graph], dtype=tf.float64))-tf.mul(r, Adj)+Diag \n",
    "\n",
    "eigenval, eigenvec = tf.self_adjoint_eig(Bethe_Hesse_neg)\n",
    "\n",
    "true_assignment_a = tf.concat(0, [tf.zeros([group_size], dtype=tf.float64),\n",
    "                                      tf.ones([group_size], dtype=tf.float64)])\n",
    "true_assignment_b = tf.concat(0, [tf.ones([group_size], dtype=tf.float64),\n",
    "                                      tf.zeros([group_size], dtype=tf.float64)])\n",
    "\n",
    "y_a = tf.pack([true_assignment_a, true_assignment_b]) \n",
    "y_b = tf.pack([true_assignment_b, true_assignment_a]) \n",
    "\n",
    "\n",
    "eigenvec_proj = tf.slice(eigenvec, [0,0], [dim_graph, 2])\n",
    "\n",
    "\n",
    "center = tf.Variable(tf.truncated_normal(shape=[2,2],\n",
    "                                       mean=0.0, stddev=0.6, dtype=tf.float64,seed=None, name=None))\n",
    "\n",
    "expanded_vectors = tf.expand_dims(eigenvec_proj, 0)\n",
    "expanded_centers = tf.expand_dims(center, 1)\n",
    "\n",
    "distances = tf.reduce_sum(tf.square(tf.sub(expanded_vectors, expanded_centers)), 2)\n",
    "\n",
    "hard_assignment = tf.argmin(distances, 0)\n",
    "new_centers = tf.concat(0, [tf.reduce_mean(tf.gather(eigenvec_proj,\n",
    "                                                     tf.reshape(tf.where(tf.equal(hard_assignment, c)),\n",
    "                                                                [1,-1])), reduction_indices=[1]) for c in xrange(communities)])\n",
    "\n",
    "#a = tf.slice(new_centers, [0,0], [1,2])\n",
    "#b = tf.slice(new_centers, [1,0], [1,2])\n",
    "update_centers = tf.assign(center, new_centers)\n",
    "\n",
    "\n",
    "assignments = 1-tf.nn.softmax(tf.transpose(distances)) #this ensures that the smaller distances are actually the correct assignments as being far from the centers make it less likely it is in that cluster\n",
    "#warning, the above transform will definitely not work for more than 2 centers, so need to be more clever later...\n",
    "#like basically sum the other two entries and subtract, this will be the generalized algo.... \n",
    "\n",
    "\n",
    "loss_vec_a = y_a*tf.log(tf.transpose(assignments))\n",
    "loss_vec_b = y_b*tf.log(tf.transpose(assignments))\n",
    "\n",
    "loss = tf.minimum(-tf.reduce_sum(loss_vec_a), -tf.reduce_sum(loss_vec_b))\n",
    "\n",
    "\n",
    "#loss =  tf.minimum(tf.reduce_sum(tf.square(tf.sub(eigenvec, true_assignment_a))),tf.reduce_sum(tf.square(tf.sub(eigenvec, true_assignment_b))))\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss, var_list=[r])\n",
    "\n",
    "eigenvec_grad = tf.gradients(eigenvec, r)\n",
    "loss_grad = tf.gradients(loss, r)\n",
    "\n",
    "init = tf.initialize_variables([r, center])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(train, feed_dict = {Adj: data[i]})\n",
    "    for i in range(100):\n",
    "        sess.run([train], feed_dict = {Adj: data[i]})\n",
    "        if i%10 ==0:\n",
    "            print sess.run([loss, hard_assignment, r, average_degree, center, new_centers], feed_dict = {Adj: data[i]})\n",
    "    \n",
    "#, distances, assignments,  loss_grad, eigenvec_grad, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#does backprop not play well with update centers? \n",
    "?tf.assign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "center_1 = tf.Variable(tf.random_normal(shape=[2,1],\n",
    "                                       mean=1.0, stddev=1.0, dtype=tf.float64,seed=None, name=None))\n",
    "center_2 = tf.Variable(tf.random_normal(shape=[2,1],\n",
    "                                       mean=-1.0, stddev=1.0, dtype=tf.float64,seed=None, name=None))\n",
    "center = tf.pack([center_1, center_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([ 0.97210996]), 10.699999999999999, array([[             nan,              nan],\n",
      "       [ -2.23605209e-01,  -7.78543686e-05]]), array([[             nan,              nan],\n",
      "       [ -2.23605209e-01,  -7.78543686e-05]])]\n",
      "[nan, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([ 0.97210996]), 10.4, array([[             nan,              nan],\n",
      "       [  2.23605478e-01,   1.79839682e-04]]), array([[             nan,              nan],\n",
      "       [  2.23605478e-01,   1.79839682e-04]])]\n",
      "[nan, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([ 0.97210996]), 10.4, array([[        nan,         nan],\n",
      "       [-0.22360379,  0.00037769]]), array([[        nan,         nan],\n",
      "       [-0.22360379,  0.00037769]])]\n",
      "[nan, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([ 0.97210996]), 11.4, array([[             nan,              nan],\n",
      "       [  2.23605082e-01,   3.44556132e-05]]), array([[             nan,              nan],\n",
      "       [  2.23605082e-01,   3.44556132e-05]])]\n",
      "[nan, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([ 0.97210996]), 9.4000000000000004, array([[        nan,         nan],\n",
      "       [-0.22359913,  0.00134372]]), array([[        nan,         nan],\n",
      "       [-0.22359913,  0.00134372]])]\n",
      "[nan, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([ 0.97210996]), 10.6, array([[        nan,         nan],\n",
      "       [-0.22360375, -0.00073475]]), array([[        nan,         nan],\n",
      "       [-0.22360375, -0.00073475]])]\n",
      "[nan, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([ 0.97210996]), 9.9000000000000004, array([[        nan,         nan],\n",
      "       [ 0.22360202,  0.0010233 ]]), array([[        nan,         nan],\n",
      "       [ 0.22360202,  0.0010233 ]])]\n",
      "[nan, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([ 0.97210996]), 9.6999999999999993, array([[        nan,         nan],\n",
      "       [ 0.22360282, -0.0004133 ]]), array([[        nan,         nan],\n",
      "       [ 0.22360282, -0.0004133 ]])]\n",
      "[nan, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([ 0.97210996]), 10.300000000000001, array([[        nan,         nan],\n",
      "       [-0.22360337, -0.00087125]]), array([[        nan,         nan],\n",
      "       [-0.22360337, -0.00087125]])]\n",
      "[nan, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([ 0.97210996]), 10.0, array([[        nan,         nan],\n",
      "       [ 0.22360371,  0.00088016]]), array([[        nan,         nan],\n",
      "       [ 0.22360371,  0.00088016]])]\n"
     ]
    }
   ],
   "source": [
    "communities = 2\n",
    "group_size=10\n",
    "data = [np.asarray(balanced_stochastic_blockmodel(communities=communities, groupsize=group_size, p_in=0.8, p_out=0.3)).astype(np.float64) for i in range(1000)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dim_graph=communities*group_size\n",
    "\n",
    "Adj = tf.placeholder(tf.float64)\n",
    "Adj = tf.cast(Adj, tf.float64)\n",
    "\n",
    "Diag = tf.diag(tf.reduce_sum(Adj,0))\n",
    "Diag = tf.cast(Diag, tf.float64)\n",
    "\n",
    "average_degree = tf.reduce_sum(Adj)/dim_graph\n",
    "\n",
    "#r =  tf.Variable(tf.constant(2.1), dtype = tf.float64)\n",
    "r = tf.Variable(tf.random_normal(shape=[1], mean=0.0, stddev=2.0, dtype=tf.float64,seed=None, name=None))\n",
    "\n",
    "Bethe_Hesse_neg = (tf.square(r)-1)*tf.diag(tf.ones(shape=[dim_graph], dtype=tf.float64))-tf.mul(r, Adj)+Diag \n",
    "\n",
    "eigenval, eigenvec = tf.self_adjoint_eig(Bethe_Hesse_neg)\n",
    "\n",
    "true_assignment_a = tf.concat(0, [tf.zeros([group_size], dtype=tf.float64),\n",
    "                                      tf.ones([group_size], dtype=tf.float64)])\n",
    "true_assignment_b = tf.concat(0, [tf.ones([group_size], dtype=tf.float64),\n",
    "                                      tf.zeros([group_size], dtype=tf.float64)])\n",
    "\n",
    "y_a = tf.pack([true_assignment_a, true_assignment_b]) \n",
    "y_b = tf.pack([true_assignment_b, true_assignment_a]) \n",
    "\n",
    "\n",
    "eigenvec_proj = tf.slice(eigenvec, [0,0], [dim_graph, 2])\n",
    "\n",
    "\n",
    "center = tf.Variable(tf.truncated_normal(shape=[2,2],\n",
    "                                       mean=0.0, stddev=1.0, dtype=tf.float64,seed=None, name=None))\n",
    "\n",
    "expanded_vectors = tf.expand_dims(eigenvec_proj, 0)\n",
    "expanded_centers = tf.expand_dims(center, 1)\n",
    "\n",
    "distances = tf.reduce_sum(tf.square(tf.sub(expanded_vectors, expanded_centers)), 2)\n",
    "\n",
    "hard_assignment = tf.argmin(distances, 0)\n",
    "new_centers = tf.concat(0, [tf.reduce_mean(tf.gather(eigenvec_proj,\n",
    "                                                     tf.reshape(tf.where(tf.equal(hard_assignment, c)),\n",
    "                                                                [1,-1])), reduction_indices=[1]) for c in xrange(communities)])\n",
    "\n",
    "#a = tf.slice(new_centers, [0,0], [1,2])\n",
    "#b = tf.slice(new_centers, [1,0], [1,2])\n",
    "update_centers = tf.assign(center, new_centers)\n",
    "\n",
    "\n",
    "assignments = 1-tf.nn.softmax(tf.transpose(distances)) #this ensures that the smaller distances are actually the correct assignments as being far from the centers make it less likely it is in that cluster\n",
    "#warning, the above transform will definitely not work for more than 2 centers, so need to be more clever later...\n",
    "#like basically sum the other two entries and subtract, this will be the generalized algo.... \n",
    "\n",
    "\n",
    "loss_vec_a = y_a*tf.log(tf.transpose(assignments))\n",
    "loss_vec_b = y_b*tf.log(tf.transpose(assignments))\n",
    "\n",
    "loss = tf.minimum(-tf.reduce_sum(loss_vec_a), -tf.reduce_sum(loss_vec_b))\n",
    "\n",
    "\n",
    "#loss =  tf.minimum(tf.reduce_sum(tf.square(tf.sub(eigenvec, true_assignment_a))),tf.reduce_sum(tf.square(tf.sub(eigenvec, true_assignment_b))))\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss, var_list=[r])\n",
    "\n",
    "eigenvec_grad = tf.gradients(eigenvec, r)\n",
    "loss_grad = tf.gradients(loss, r)\n",
    "\n",
    "init = tf.initialize_variables([r, center])\n",
    "\n",
    "init_center = tf.initialize_variables([center])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(100):\n",
    "        sess.run([update_centers], feed_dict = {Adj: data[i]})\n",
    "        if i%10 ==0:\n",
    "            print sess.run([loss, hard_assignment, r, average_degree, center, new_centers], feed_dict = {Adj: data[i]})\n",
    "    \n",
    "#, distances, assignments,  loss_grad, eigenvec_grad, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gets stuck in local ins"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
