{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import  networkx as nx\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def balanced_stochastic_blockmodel(communities=2, groupsize=3, p_in=1.0, p_out=0.0):\n",
    "    #gives dense adjacency matrix representaiton of randomly generated SBM with balanced community size\n",
    "\n",
    "    G = nx.planted_partition_graph(l=communities, k=groupsize, p_in=p_in, p_out =p_out)\n",
    "    A = nx.adjacency_matrix(G).todense()\n",
    "    \n",
    "    return A\n",
    "\n",
    "\n",
    "communities = 2 #number of communities, chance to \n",
    "group_size = 6 #number of nodes in each communitites (balanced so far)\n",
    "dim_graph = communities*group_size\n",
    "A = np.asarray(balanced_stochastic_blockmodel(communities=communities, groupsize=group_size, p_in=0.5, p_out=0.2)).astype(np.double)\n",
    "\n",
    "data = [np.asarray(balanced_stochastic_blockmodel(communities=communities, groupsize=group_size, p_in=0.5, p_out=0.2)).astype(np.float64) for i in range(100)]\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 3.12317412]), -8.2351466971933238, array([[ 1.39119968, -0.26005339],\n",
      "       [-0.24372163, -1.53984271]])]\n",
      "[array([ 0.84121812]), -29.771346418053529, array([[-0.78583882,  2.88734801],\n",
      "       [ 1.61611611, -0.45196944]])]\n",
      "[array([-4.27378461]), -11.757282853953495, array([[-0.17404488,  0.27570597],\n",
      "       [ 0.62918978, -0.99172488]])]\n",
      "[array([-1.46065015]), -9.1523781351169085, array([[-0.52942376,  0.03586426],\n",
      "       [ 0.29939199, -0.62895713]])]\n",
      "[array([-1.02343599]), -8.5630697672028493, array([[ 0.08031587,  1.20777023],\n",
      "       [ 1.12336889,  0.35355371]])]\n",
      "[array([ 1.31698605]), -13.943264537005749, array([[ 0.64178432,  0.03893587],\n",
      "       [-1.08966633, -0.23000412]])]\n",
      "[array([-1.35852283]), -9.5560245273221334, array([[ 0.94398497,  0.55968787],\n",
      "       [-0.61698521, -0.91500724]])]\n",
      "[array([-0.68386662]), -11.317009505514989, array([[-0.0321034 ,  0.02024101],\n",
      "       [ 1.22038767,  0.01220293]])]\n",
      "[array([ 0.75248161]), -10.474147156943257, array([[-1.5840353 , -0.2075951 ],\n",
      "       [ 0.56167613, -1.80080067]])]\n",
      "[array([ 0.87979591]), -7.7183535789507065, array([[ 0.09668299, -0.67688134],\n",
      "       [-1.11031412,  0.96311892]])]\n"
     ]
    }
   ],
   "source": [
    "communities = 6\n",
    "Adj = tf.placeholder(tf.float64)\n",
    "\n",
    "Diag = tf.diag(tf.reduce_sum(Adj,0))\n",
    "Diag = tf.cast(Diag, tf.float64)\n",
    "\n",
    "r =  tf.Variable(tf.random_normal(shape=[1], mean=0.0,\n",
    "                                 stddev=2.0, dtype=tf.float64,\n",
    "                                 seed=None, name=None))\n",
    "\n",
    "Bethe_Hesse_neg = (tf.square(r)-1)*tf.diag(tf.ones(shape=[dim_graph], dtype=tf.float64))-tf.mul(r, Adj)+Diag \n",
    "\n",
    "eigenval, eigenvec = tf.self_adjoint_eig(Bethe_Hesse_neg)\n",
    "\n",
    "true_assignment_a = tf.concat(0, [tf.zeros([communities], dtype=tf.float64),\n",
    "                                      tf.ones([communities], dtype=tf.float64)])\n",
    "true_assignment_b = tf.concat(0, [tf.ones([communities], dtype=tf.float64),\n",
    "                                      tf.zeros([communities], dtype=tf.float64)])\n",
    "\n",
    "\n",
    "y_a = tf.pack([true_assignment_a, true_assignment_b]) \n",
    "y_b = tf.pack([true_assignment_b, true_assignment_a]) \n",
    "\n",
    "\n",
    "eigenvec_proj = tf.slice(eigenvec, [0,0], [dim_graph, 2])\n",
    "\n",
    "centers = tf.Variable(tf.random_normal(shape=[2,2],\n",
    "                                       mean=0.0, stddev=1.0, dtype=tf.float64,seed=None, name=None))\n",
    "\n",
    "expanded_vectors = tf.expand_dims(eigenvec_proj, 0)\n",
    "expanded_centers = tf.expand_dims(centers, 1)\n",
    "\n",
    "distances = tf.transpose(tf.reduce_sum(tf.square(tf.sub(expanded_vectors,\n",
    "                                                        expanded_centers)), 2))\n",
    "\n",
    "assignments = tf.nn.softmax(distances)\n",
    "\n",
    "loss_vec_a = y_a*tf.log(tf.transpose(assignments))\n",
    "loss_vec_b = y_b*tf.log(tf.transpose(assignments))\n",
    "\n",
    "loss = tf.minimum(tf.reduce_sum(loss_vec_a), tf.reduce_sum(loss_vec_a))\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "eigenvec_grad = tf.gradients(eigenvec, r)\n",
    "loss_grad = tf.gradients(loss, r)\n",
    "\n",
    "init = tf.initialize_variables([r, centers])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "        sess.run(init)\n",
    "#       sess.run(train)\n",
    "        print sess.run([r, loss, centers], feed_dict = {Adj: data[i]})\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now with training uncommentedout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.80005489]), [array([ 9.79694719])], [array([-1.98701186])], 80.859714974926163, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([-0.43659751]), [array([-4.58307354])], [array([ 1.68392781])], 77.553692330113861, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([-0.4539848]), [array([ 0.31788765])], [array([-1.11917682])], 81.061766717813185, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([ 0.12698334]), [array([ 11.42162481])], [array([-8.62615963])], 79.439033997694281, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([-0.3181855]), [array([ 6.73951226])], [array([ 5.23449881])], 82.768470799372238, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([-0.69595131]), [array([-1.93966004])], [array([ 0.28569067])], 74.435438953679238, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([ 0.23596093]), [array([-1.82086183])], [array([-3.64854591])], 83.951116555286973, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([ 1.14461804]), [array([ 0.03115408])], [array([ 0.47209244])], 76.860052868654321, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([ 1.40093678]), [array([-1.98725797])], [array([ 1.11279248])], 75.876324155799978, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n",
      "[array([ 1.3110159]), [array([ 1.08848959])], [array([-1.0653423])], 77.552544538831427, array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "       [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])]\n"
     ]
    }
   ],
   "source": [
    "communities = 6\n",
    "Adj = tf.placeholder(tf.float64)\n",
    "Adj = tf.cast(Adj, tf.float64)\n",
    "\n",
    "Diag = tf.diag(tf.reduce_sum(Adj,0))\n",
    "Diag = tf.cast(Diag, tf.float64)\n",
    "\n",
    "#r =  tf.Variable(tf.constant(2.1), dtype = tf.float64)\n",
    "r = tf.Variable(tf.random_normal(shape=[1], mean=0.0, stddev=2.0, dtype=tf.float64,seed=None, name=None))\n",
    "\n",
    "Bethe_Hesse_neg = (tf.square(r)-1)*tf.diag(tf.ones(shape=[dim_graph], dtype=tf.float64))-tf.mul(r, Adj)+Diag \n",
    "\n",
    "eigenval, eigenvec = tf.self_adjoint_eig(Bethe_Hesse_neg)\n",
    "\n",
    "true_assignment_a = tf.concat(0, [tf.zeros([communities], dtype=tf.float64),\n",
    "                                      tf.ones([communities], dtype=tf.float64)])\n",
    "true_assignment_b = tf.concat(0, [tf.ones([communities], dtype=tf.float64),\n",
    "                                      tf.zeros([communities], dtype=tf.float64)])\n",
    "\n",
    "y_a = tf.pack([true_assignment_a, true_assignment_b]) \n",
    "\n",
    "eigenvec_proj = tf.nn.softmax(tf.slice(eigenvec, [0,0], [dim_graph, 2]))\n",
    "eigenvec_proj = tf.cast(tf.reduce_sum(eigenvec, 1), dtype = tf.float64)\n",
    "\n",
    "\n",
    "loss =  tf.minimum(tf.reduce_sum(tf.square(tf.sub(eigenvec, true_assignment_a))), \n",
    "              tf.reduce_sum(tf.square(tf.sub(eigenvec, true_assignment_b))))\n",
    "#tf.reduce_sum(tf.square(tf.sub(c, true_assignment_a)))\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "train = optimizer.minimize(loss, var_list=[r])\n",
    "\n",
    "eigenvec_grad = tf.gradients(eigenvec, r)\n",
    "loss_grad = tf.gradients(loss, r)\n",
    "\n",
    "init = tf.initialize_variables([r])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(10):\n",
    "        sess.run(train, feed_dict = {Adj: data[i]})\n",
    "        print sess.run([r, loss_grad, eigenvec_grad, loss, y_a], feed_dict = {Adj: data[i]})\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?tf.pack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tw = tf.train.SummaryWriter('./logDir',sess.graph)\n",
    "\n",
    "# Train your network\n",
    "\n",
    "tw.flush()\n",
    "tw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.],\n",
       "       [ 1.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.],\n",
       "       [ 1.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,  0.,  1.,  1.],\n",
       "       [ 0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities = 2 #number of communities, chance to \n",
    "group_size = 6\n",
    "data = [np.asarray(balanced_stochastic_blockmodel(communities=communities, groupsize=group_size, p_in=0.8, p_out=0.5)).astype(np.float64) for i in range(1000)]\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2.1520866]), 6.8865252187390915, array([[ 0.41104083, -0.20884815],\n",
      "       [-0.84761614,  0.58082195]]), array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0]), array([[-0.3010572 , -0.18876819],\n",
      "       [-0.26707646,  0.29913901]])]\n",
      "[array([ 2.19829533]), 12.574520516228251, array([[ 0.41104083, -0.20884815],\n",
      "       [-0.84761614,  0.58082195]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([[ 0.28818551, -0.00571234],\n",
      "       [        nan,         nan]])]\n",
      "[array([ 2.14790093]), 10.843691687408963, array([[ 0.41104083, -0.20884815],\n",
      "       [-0.84761614,  0.58082195]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([[ 0.28490333, -0.01974335],\n",
      "       [        nan,         nan]])]\n",
      "[array([ 2.06278577]), 10.085500112919323, array([[ 0.41104083, -0.20884815],\n",
      "       [-0.84761614,  0.58082195]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([[ 0.28560289,  0.02171678],\n",
      "       [        nan,         nan]])]\n",
      "[array([ 2.00566133]), 10.967516492840904, array([[ 0.41104083, -0.20884815],\n",
      "       [-0.84761614,  0.58082195]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([[ 0.2874995 ,  0.01773295],\n",
      "       [        nan,         nan]])]\n",
      "[array([ 2.03839772]), 10.938338628611394, array([[ 0.41104083, -0.20884815],\n",
      "       [-0.84761614,  0.58082195]]), array([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]), array([[-0.2778961 , -0.25235854],\n",
      "       [-0.29626536,  0.2289669 ]])]\n",
      "[array([ 2.07680709]), 11.521899471226059, array([[ 0.41104083, -0.20884815],\n",
      "       [-0.84761614,  0.58082195]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([[ 0.28761581,  0.01460484],\n",
      "       [        nan,         nan]])]\n",
      "[array([ 2.18453987]), 8.0618670101004923, array([[ 0.41104083, -0.20884815],\n",
      "       [-0.84761614,  0.58082195]]), array([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0]), array([[-0.29866504, -0.14573208],\n",
      "       [-0.26299804,  0.33713323]])]\n",
      "[array([ 2.2262332]), 9.6666297453201295, array([[ 0.41104083, -0.20884815],\n",
      "       [-0.84761614,  0.58082195]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([[ 0.28722378,  0.00997266],\n",
      "       [        nan,         nan]])]\n",
      "[array([ 2.17087614]), 9.5986473130731174, array([[ 0.41104083, -0.20884815],\n",
      "       [-0.84761614,  0.58082195]]), array([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1]), array([[-0.28847824, -0.18632781],\n",
      "       [-0.28622475,  0.27133932]])]\n"
     ]
    }
   ],
   "source": [
    "communities = 2\n",
    "group_size=6\n",
    "Adj = tf.placeholder(tf.float64)\n",
    "Adj = tf.cast(Adj, tf.float64)\n",
    "\n",
    "Diag = tf.diag(tf.reduce_sum(Adj,0))\n",
    "Diag = tf.cast(Diag, tf.float64)\n",
    "\n",
    "#r =  tf.Variable(tf.constant(2.1), dtype = tf.float64)\n",
    "r = tf.Variable(tf.random_normal(shape=[1], mean=0.0, stddev=2.0, dtype=tf.float64,seed=None, name=None))\n",
    "\n",
    "Bethe_Hesse_neg = (tf.square(r)-1)*tf.diag(tf.ones(shape=[dim_graph], dtype=tf.float64))-tf.mul(r, Adj)+Diag \n",
    "\n",
    "eigenval, eigenvec = tf.self_adjoint_eig(Bethe_Hesse_neg)\n",
    "\n",
    "true_assignment_a = tf.concat(0, [tf.zeros([group_size], dtype=tf.float64),\n",
    "                                      tf.ones([group_size], dtype=tf.float64)])\n",
    "true_assignment_b = tf.concat(0, [tf.ones([group_size], dtype=tf.float64),\n",
    "                                      tf.zeros([group_size], dtype=tf.float64)])\n",
    "\n",
    "y_a = tf.pack([true_assignment_a, true_assignment_b]) \n",
    "y_b = tf.pack([true_assignment_b, true_assignment_a]) \n",
    "\n",
    "\n",
    "eigenvec_proj = tf.slice(eigenvec, [0,0], [dim_graph, 2])\n",
    "\n",
    "\n",
    "center = tf.Variable(tf.truncated_normal(shape=[2,2],\n",
    "                                       mean=0.0, stddev=0.6, dtype=tf.float64,seed=None, name=None))\n",
    "\n",
    "expanded_vectors = tf.expand_dims(eigenvec_proj, 0)\n",
    "expanded_centers = tf.expand_dims(center, 1)\n",
    "\n",
    "distances = tf.reduce_sum(tf.square(tf.sub(expanded_vectors, expanded_centers)), 2)\n",
    "\n",
    "hard_assignment = tf.argmin(distances, 0)\n",
    "new_centers = tf.concat(0, [tf.reduce_mean(tf.gather(eigenvec_proj,\n",
    "                                                     tf.reshape(tf.where(tf.equal(hard_assignment, c)),\n",
    "                                                                [1,-1])), reduction_indices=[1]) for c in xrange(communities)])\n",
    "\n",
    "#a = tf.slice(new_centers, [0,0], [1,2])\n",
    "#b = tf.slice(new_centers, [1,0], [1,2])\n",
    "update_centers = tf.assign(center, new_centers)\n",
    "\n",
    "\n",
    "assignments = 1-tf.nn.softmax(tf.transpose(distances)) #this ensures that the smaller distances are actually the correct assignments as being far from the centers make it less likely it is in that cluster\n",
    "#warning, the above transform will definitely not work for more than 2 centers, so need to be more clever later...\n",
    "#like basically sum the other two entries and subtract, this will be the generalized algo.... \n",
    "\n",
    "\n",
    "loss_vec_a = y_a*tf.log(tf.transpose(assignments))\n",
    "loss_vec_b = y_b*tf.log(tf.transpose(assignments))\n",
    "\n",
    "loss = tf.minimum(-tf.reduce_sum(loss_vec_a), -tf.reduce_sum(loss_vec_a))\n",
    "\n",
    "\n",
    "#loss =  tf.minimum(tf.reduce_sum(tf.square(tf.sub(eigenvec, true_assignment_a))),tf.reduce_sum(tf.square(tf.sub(eigenvec, true_assignment_b))))\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "train = optimizer.minimize(loss, var_list=[r])\n",
    "\n",
    "eigenvec_grad = tf.gradients(eigenvec, r)\n",
    "loss_grad = tf.gradients(loss, r)\n",
    "\n",
    "init = tf.initialize_variables([r, center])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(100):\n",
    "        sess.run([train], feed_dict = {Adj: data[i]})\n",
    "        if i%10 ==0:\n",
    "            print sess.run([r, loss, center, hard_assignment, new_centers], feed_dict = {Adj: data[i]})\n",
    "    \n",
    "#, distances, assignments,  loss_grad, eigenvec_grad, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#does backprop not play well with update centers? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "center_1 = tf.Variable(tf.random_normal(shape=[2,1],\n",
    "                                       mean=1.0, stddev=1.0, dtype=tf.float64,seed=None, name=None))\n",
    "center_2 = tf.Variable(tf.random_normal(shape=[2,1],\n",
    "                                       mean=-1.0, stddev=1.0, dtype=tf.float64,seed=None, name=None))\n",
    "center = tf.pack([center_1, center_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#let's try another loss: \n",
    "\n",
    "\n",
    "communities = 2\n",
    "group_size=6\n",
    "Adj = tf.placeholder(tf.float64)\n",
    "Adj = tf.cast(Adj, tf.float64)\n",
    "\n",
    "Diag = tf.diag(tf.reduce_sum(Adj,0))\n",
    "Diag = tf.cast(Diag, tf.float64)\n",
    "\n",
    "#r =  tf.Variable(tf.constant(2.1), dtype = tf.float64)\n",
    "r = tf.Variable(tf.random_normal(shape=[1], mean=0.0, stddev=2.0, dtype=tf.float64,seed=None, name=None))\n",
    "\n",
    "Bethe_Hesse_neg = (tf.square(r)-1)*tf.diag(tf.ones(shape=[dim_graph], dtype=tf.float64))-tf.mul(r, Adj)+Diag \n",
    "\n",
    "eigenval, eigenvec = tf.self_adjoint_eig(Bethe_Hesse_neg)\n",
    "\n",
    "true_assignment_a = tf.concat(0, [tf.zeros([group_size], dtype=tf.float64),\n",
    "                                      tf.ones([group_size], dtype=tf.float64)])\n",
    "true_assignment_b = tf.concat(0, [tf.ones([group_size], dtype=tf.float64),\n",
    "                                      tf.zeros([group_size], dtype=tf.float64)])\n",
    "\n",
    "y_a = tf.pack([true_assignment_a, true_assignment_b]) \n",
    "y_b = tf.pack([true_assignment_b, true_assignment_a]) \n",
    "\n",
    "\n",
    "eigenvec_proj = tf.slice(eigenvec, [0,0], [dim_graph, 2])\n",
    "\n",
    "\n",
    "center = tf.Variable(tf.truncated_normal(shape=[2,2],\n",
    "                                       mean=0.0, stddev=0.6, dtype=tf.float64,seed=None, name=None))\n",
    "\n",
    "expanded_vectors = tf.expand_dims(eigenvec_proj, 0)\n",
    "expanded_centers = tf.expand_dims(center, 1)\n",
    "\n",
    "distances = tf.reduce_sum(tf.square(tf.sub(expanded_vectors, expanded_centers)), 2)\n",
    "\n",
    "hard_assignment = tf.argmin(distances, 0)\n",
    "new_centers = tf.concat(0, [tf.reduce_mean(tf.gather(eigenvec_proj,\n",
    "                                                     tf.reshape(tf.where(tf.equal(hard_assignment, c)),\n",
    "                                                                [1,-1])), reduction_indices=[1]) for c in xrange(communities)])\n",
    "\n",
    "#a = tf.slice(new_centers, [0,0], [1,2])\n",
    "#b = tf.slice(new_centers, [1,0], [1,2])\n",
    "update_centers = tf.assign(center, new_centers)\n",
    "\n",
    "\n",
    "assignments = 1-tf.nn.softmax(tf.transpose(distances)) #this ensures that the smaller distances are actually the correct assignments as being far from the centers make it less likely it is in that cluster\n",
    "#warning, the above transform will definitely not work for more than 2 centers, so need to be more clever later...\n",
    "#like basically sum the other two entries and subtract, this will be the generalized algo.... \n",
    "\n",
    "\n",
    "loss_vec_a = y_a*tf.log(tf.transpose(assignments))\n",
    "loss_vec_b = y_b*tf.log(tf.transpose(assignments))\n",
    "\n",
    "loss = tf.minimum(-tf.reduce_sum(loss_vec_a), -tf.reduce_sum(loss_vec_a))\n",
    "\n",
    "\n",
    "#loss =  tf.minimum(tf.reduce_sum(tf.square(tf.sub(eigenvec, true_assignment_a))),tf.reduce_sum(tf.square(tf.sub(eigenvec, true_assignment_b))))\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1)\n",
    "train = optimizer.minimize(loss, var_list=[r])\n",
    "\n",
    "eigenvec_grad = tf.gradients(eigenvec, r)\n",
    "loss_grad = tf.gradients(loss, r)\n",
    "\n",
    "init = tf.initialize_variables([r, center])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(100):\n",
    "        sess.run([train], feed_dict = {Adj: data[i]})\n",
    "        if i%10 ==0:\n",
    "            print sess.run([r, loss, center, hard_assignment, new_centers], feed_dict = {Adj: data[i]})\n",
    "    \n",
    "#, distances, assignments,  loss_grad, eigenvec_grad, \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
