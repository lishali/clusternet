{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import linalg_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "import networkx as nx\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ops.RegisterGradient(\"gradient_no_unitary_adjustment\")\n",
    "def _test1(op, grad_e, grad_v):\n",
    "    \"\"\"Gradient for SelfAdjointEigV2 derived with Joan with no adjustment for subspace\"\"\"\n",
    "    e = op.outputs[0]\n",
    "    v = op.outputs[1]\n",
    "    #dim = v.get_shape()\n",
    "    with ops.control_dependencies([grad_e.op, grad_v.op]):\n",
    "        if grad_v is not None:  \n",
    "            E = array_ops.diag(e)\n",
    "            #v_proj = arrary.ops.slice(v, [0,0], [])\n",
    "            grad_grassman = grad_v# - math_ops.batch_matmul(math_ops.batch_matmul(v, array_ops.transpose(grad_v)), v)\n",
    "            grad_a = math_ops.batch_matmul(grad_grassman, math_ops.batch_matmul(E, array_ops.transpose(grad_v)))+math_ops.batch_matmul(grad_v, math_ops.batch_matmul(E, array_ops.transpose(grad_grassman)))\n",
    "    return grad_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ops.RegisterGradient(\"grassman_with_2d\")\n",
    "def _test1(op, grad_e, grad_v):\n",
    "    \"\"\"Gradient for SelfAdjointEigV2 derived with Joan with no adjustment for subspace\"\"\"\n",
    "    e = op.outputs[0]\n",
    "    v = op.outputs[1]\n",
    "    #dim = v.get_shape()\n",
    "    with ops.control_dependencies([grad_e.op, grad_v.op]):\n",
    "        if grad_v is not None:  \n",
    "            E = array_ops.diag(e)\n",
    "            v_proj = array_ops.slice(v, [0,0], [20,2])\n",
    "            grad_grassman = grad_v - math_ops.batch_matmul(math_ops.batch_matmul(v_proj, array_ops.transpose(v_proj)), grad_v)\n",
    "            grad_a = math_ops.batch_matmul(grad_grassman, math_ops.batch_matmul(E, array_ops.transpose(grad_v)))+math_ops.batch_matmul(grad_v, math_ops.batch_matmul(E, array_ops.transpose(grad_grassman)))\n",
    "    return grad_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balanced_stochastic_blockmodel(communities=2, groupsize=3, p_in=0.5, p_out=0.1, seed=None):\n",
    "    #gives dense adjacency matrix representaiton of randomly generated SBM with balanced community size\n",
    "\n",
    "    G = nx.planted_partition_graph(l=communities, k=groupsize, p_in=p_in, p_out =p_out, seed=seed)\n",
    "    A = nx.adjacency_matrix(G).todense()\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_subspace(adj, groupsize, communities, diag, dim_proj):\n",
    "    normalizer = tf.cast(2.0*groupsize*communities, dtype=tf.float64)\n",
    "    total_degree = tf.cast(tf.reduce_sum(adj), dtype=tf.float64)\n",
    "    r = tf.sqrt(total_degree/normalizer)\n",
    "    BH_op = (tf.square(r)-1)*tf.diag(tf.ones(shape=[communities*groupsize], dtype=tf.float64))-r*adj+diag \n",
    "    val, vec = tf.self_adjoint_eig(BH_op) #this is already normalized so no need to normalize\n",
    "    subspace = tf.slice(vec, [0,0], [communities*groupsize, dim_proj])\n",
    "    return r, subspace\n",
    "\n",
    "def proj_magnitude(space, vector):\n",
    "    projection_op = tf.matmul(space, tf.transpose(space))\n",
    "    projection = tf.matmul(projection_op, vector)\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(projection))) #tf.reduce_sum(tf.abs(projection))#\n",
    "\n",
    "\n",
    "def rnd_vec_normed(communities, groupsize, seed=None):\n",
    "    rnd_vec1 = tf.Variable(tf.random_normal(shape=[communities*groupsize,1], mean=0.0,stddev=1.0,\n",
    "                                                    dtype=tf.float64,\n",
    "                                                    seed=seed))\n",
    "    return normalize_vec(rnd_vec1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_svm_cluster(communities = 2, group_size = 10, seed=1, seed_r=1, p=0.8, q=0.05, name='test1', projection_dim=2, iterations=100, \n",
    "                     print_ratio=10, l_rate=0.1, mean=2.0, sd=0.4):\n",
    "    \"\"\"testing to see if the loss will decrease backproping through very simple function\"\"\"\n",
    "    B = np.asarray(balanced_stochastic_blockmodel(communities, group_size, p, q, seed)).astype(np.double)\n",
    "    B = tf.cast(B, dtype = tf.float64)\n",
    "    \n",
    "    Diag = tf.diag(tf.reduce_sum(B,0))\n",
    "    Diag = tf.cast(Diag, tf.float64)\n",
    "\n",
    "    r =  tf.Variable(tf.random_normal(shape=[1], mean=mean,\n",
    "                                 stddev=sd, dtype=tf.float64,\n",
    "                                 seed=seed_r, name=None))\n",
    "\n",
    "    \n",
    "    BH = (tf.square(r)-1)*tf.diag(tf.ones(shape=[communities*group_size], dtype=tf.float64))-tf.mul(r, B)+Diag \n",
    "    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        g = tf.get_default_graph()\n",
    "        \n",
    "        with g.gradient_override_map({'SelfAdjointEigV2': name}):\n",
    "            eigenval, eigenvec = tf.self_adjoint_eig(BH)\n",
    "            #we try to do svm in this subspace \n",
    "            #or we can project it down to 1 dimensions, do the clustering there via some threshold and check if it makes sense \n",
    "            #by computing the loss, if it is too big, we change the angle we project down to...\n",
    "            \n",
    "            \n",
    "            eigenvec_proj = tf.slice(eigenvec, [0,0], [communities*group_size, projection_dim])\n",
    "            \n",
    "            \n",
    "            \n",
    "            true_assignment_a = tf.concat(0, [-1*tf.ones([group_size], dtype=tf.float64),\n",
    "                                      tf.ones([group_size], dtype=tf.float64)])\n",
    "            true_assignment_b = -1*true_assignment_a\n",
    "            true_assignment_a = tf.expand_dims(true_assignment_a, 1)\n",
    "            true_assignment_b = tf.expand_dims(true_assignment_b, 1)\n",
    "\n",
    "            \n",
    "            projected_a = tf.matmul(tf.matmul(eigenvec_proj, tf.transpose(eigenvec_proj)), true_assignment_a)#tf.transpose(true_assignment_a))\n",
    "            projected_b = tf.matmul(tf.matmul(eigenvec_proj, tf.transpose(eigenvec_proj)), true_assignment_b)#tf.transpose(true_assignment_b))\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss = tf.minimum(tf.reduce_sum(tf.square(tf.sub(projected_a, true_assignment_a))),\n",
    "                              tf.reduce_sum(tf.square(tf.sub(projected_b, true_assignment_b))))\n",
    "            \n",
    "            optimizer = tf.train.GradientDescentOptimizer(l_rate)\n",
    "            \n",
    "            train = optimizer.minimize(loss, var_list=[r])\n",
    "\n",
    "            eigenvec_grad = tf.gradients(eigenvec, r)\n",
    "            loss_grad = tf.gradients(loss, r)\n",
    "            \n",
    "            \n",
    "            \n",
    "            r_op, target = target_subspace(adj=B, groupsize=group_size, communities=communities, diag=Diag, dim_proj=projection_dim)  \n",
    "            \n",
    "            r_op_projection_a = tf.matmul(tf.matmul(target, tf.transpose(target)), true_assignment_a)\n",
    "            r_op_projection_b = tf.matmul(tf.matmul(target, tf.transpose(target)), true_assignment_b)\n",
    "            r_op_loss = tf.minimum(tf.reduce_sum(tf.square(tf.sub(r_op_projection_a, true_assignment_a))),\n",
    "                              tf.reduce_sum(tf.square(tf.sub(r_op_projection_b, true_assignment_b))))\n",
    "            \n",
    "            init = tf.initialize_all_variables()\n",
    "            \n",
    "            \n",
    "            sess.run(init)\n",
    "            a,b,c,d= sess.run([r, r_op, r_op_loss, tf.transpose(r_op_projection_a)])\n",
    "            print \"initial r: {}. r_op = sqrt(average degree) : {} . Loss associated with r_op: {}. r_op assignments {}.\".format(a, b, c, d)\n",
    "            for i in range(iterations):           \n",
    "                if i%print_ratio==0:\n",
    "                    print i\n",
    "                    a,b,c,d = sess.run([r, loss, tf.gradients(loss, r), tf.transpose(projected_a)])\n",
    "                    print \"current r: {}, current loss: {}, gradient of loss/r is {} and current assignments (up to sign) {}.\".format(a,b,c,d)\n",
    "                    sess.run(train)\n",
    "                    \n",
    "                \n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 2.09713102]. r_op = sqrt(average degree) : 1.96214168703 . Loss associated with r_op: 0.239352945504. r_op assignments [[-0.81008947 -0.89831384 -1.06456991 -1.0936426  -1.05592124 -0.92168635\n",
      "  -0.88318609 -1.01724697 -1.0936426  -1.05086035  0.98362854  0.98397103\n",
      "   1.03054178  0.80692791  1.13586066  0.80914196  1.03176237  1.10198787\n",
      "   1.13586066  0.85180488]].\n",
      "0\n",
      "current r: [ 2.09713102], current loss: 0.244082145495 and current assignments (up to sign) [[-0.80940175 -0.9017661  -1.06406728 -1.09702595 -1.05503979 -0.91305556\n",
      "  -0.88571072 -1.0136581  -1.09702595 -1.05125222  0.97767908  0.97803705\n",
      "   1.02975475  0.80666566  1.14222167  0.80900335  1.02415991  1.10372799\n",
      "   1.14222167  0.85444331]].\n",
      "5000\n",
      "current r: [ 2.1842064], current loss: 0.247151590827 and current assignments (up to sign) [[-0.80898766 -0.90375093 -1.06375077 -1.09897039 -1.05450329 -0.90810577\n",
      "  -0.88716406 -1.01156044 -1.09897039 -1.05144598  0.97423025  0.97459683\n",
      "   1.02925282  0.80647574  1.14589764  0.80888358  1.01976638  1.10469783\n",
      "   1.14589764  0.85594003]].\n",
      "10000\n",
      "current r: [ 2.28128418], current loss: 0.250545077235 and current assignments (up to sign) [[-0.80855164 -0.90577337 -1.06340744 -1.10095103 -1.05393414 -0.90307167\n",
      "  -0.88864636 -1.00939739 -1.10095103 -1.05161955  0.97069496  0.97107007\n",
      "   1.02870375  0.80625236  1.14965726  0.80873098  1.01527287  1.10566245\n",
      "   1.14965726  0.85744937]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='gradient_no_unitary_adjustment', l_rate=0.0001, seed_r=1, seed = 1, print_ratio=5000, iterations=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 2.09713102]. r_op = sqrt(average degree) : 1.96214168703 . Loss associated with r_op: 0.239352945504. r_op assignments [[-0.81008947 -0.89831384 -1.06456991 -1.0936426  -1.05592124 -0.92168635\n",
      "  -0.88318609 -1.01724697 -1.0936426  -1.05086035  0.98362854  0.98397103\n",
      "   1.03054178  0.80692791  1.13586066  0.80914196  1.03176237  1.10198787\n",
      "   1.13586066  0.85180488]].\n",
      "0\n",
      "current r: [ 2.09713102], current loss: 0.244082145495 and current assignments (up to sign) [[-0.80940175 -0.9017661  -1.06406728 -1.09702595 -1.05503979 -0.91305556\n",
      "  -0.88571072 -1.0136581  -1.09702595 -1.05125222  0.97767908  0.97803705\n",
      "   1.02975475  0.80666566  1.14222167  0.80900335  1.02415991  1.10372799\n",
      "   1.14222167  0.85444331]].\n",
      "5000\n",
      "current r: [ 2.1842064], current loss: 0.247151590827 and current assignments (up to sign) [[-0.80898766 -0.90375093 -1.06375077 -1.09897039 -1.05450329 -0.90810577\n",
      "  -0.88716406 -1.01156044 -1.09897039 -1.05144598  0.97423025  0.97459683\n",
      "   1.02925282  0.80647574  1.14589764  0.80888358  1.01976638  1.10469783\n",
      "   1.14589764  0.85594003]].\n",
      "10000\n",
      "current r: [ 2.28128418], current loss: 0.250545077235 and current assignments (up to sign) [[-0.80855164 -0.90577337 -1.06340744 -1.10095103 -1.05393414 -0.90307167\n",
      "  -0.88864636 -1.00939739 -1.10095103 -1.05161955  0.97069496  0.97107007\n",
      "   1.02870375  0.80625236  1.14965726  0.80873098  1.01527287  1.10566245\n",
      "   1.14965726  0.85744937]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='grassman_with_2d', l_rate=0.0001, seed_r=1, seed = 1, print_ratio=5000, iterations=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 2.09713102]. r_op = sqrt(average degree) : 1.96214168703 . Loss associated with r_op: 0.239352945504. r_op assignments [[-0.81008947 -0.89831384 -1.06456991 -1.0936426  -1.05592124 -0.92168635\n",
      "  -0.88318609 -1.01724697 -1.0936426  -1.05086035  0.98362854  0.98397103\n",
      "   1.03054178  0.80692791  1.13586066  0.80914196  1.03176237  1.10198787\n",
      "   1.13586066  0.85180488]].\n",
      "0\n",
      "current r: [ 2.09713102], current loss: 0.244082145495 and current assignments (up to sign) [[-0.80940175 -0.9017661  -1.06406728 -1.09702595 -1.05503979 -0.91305556\n",
      "  -0.88571072 -1.0136581  -1.09702595 -1.05125222  0.97767908  0.97803705\n",
      "   1.02975475  0.80666566  1.14222167  0.80900335  1.02415991  1.10372799\n",
      "   1.14222167  0.85444331]].\n",
      "5000\n",
      "current r: [ 2.09712749], current loss: 0.244082021042 and current assignments (up to sign) [[-0.80940177 -0.90176601 -1.0640673  -1.09702587 -1.05503982 -0.91305577\n",
      "  -0.88571066 -1.01365819 -1.09702587 -1.05125221  0.97767922  0.9780372\n",
      "   1.02975477  0.80666567  1.14222151  0.80900336  1.0241601   1.10372795\n",
      "   1.14222151  0.85444325]].\n",
      "10000\n",
      "current r: [ 2.09712396], current loss: 0.24408189659 and current assignments (up to sign) [[-0.80940178 -0.90176593 -1.06406731 -1.09702579 -1.05503984 -0.91305598\n",
      "  -0.88571059 -1.01365828 -1.09702579 -1.0512522   0.97767937  0.97803734\n",
      "   1.02975479  0.80666568  1.14222136  0.80900336  1.02416028  1.10372791\n",
      "   1.14222136  0.85444318]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='SelfAdjointEigV2', l_rate=0.0001, seed_r=1, seed = 1, print_ratio=5000, iterations=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 0.09713102]. r_op = sqrt(average degree) : 1.96214168703 . Loss associated with r_op: 0.239352945504. r_op assignments [[-0.81008947 -0.89831384 -1.06456991 -1.0936426  -1.05592124 -0.92168635\n",
      "  -0.88318609 -1.01724697 -1.0936426  -1.05086035  0.98362854  0.98397103\n",
      "   1.03054178  0.80692791  1.13586066  0.80914196  1.03176237  1.10198787\n",
      "   1.13586066  0.85180488]].\n",
      "0\n",
      "current r: [ 0.09713102], current loss: 13.4317194769 and current assignments (up to sign) [[-0.0704722  -0.00936025 -0.0777787  -0.05508386 -0.07880056 -1.32332547\n",
      "  -0.04820161 -0.0327515  -0.05508386 -0.02054714  1.12260788  1.10486448\n",
      "   0.19324849  0.1558531   0.14156616  0.15051068  1.45895059  0.19786074\n",
      "   0.14156616  0.12984709]].\n",
      "5000\n",
      "current r: [ 0.10066948], current loss: 13.3054706136 and current assignments (up to sign) [[-0.07429522 -0.01036222 -0.08230116 -0.0584075  -0.08341059 -1.33888232\n",
      "  -0.05090684 -0.03599611 -0.0584075  -0.02259806  1.13176911  1.11357712\n",
      "   0.20321737  0.1642456   0.14863961  0.15851154  1.46671506  0.20719663\n",
      "   0.14863961  0.13645023]].\n",
      "10000\n",
      "current r: [ 0.10426678], current loss: 13.1749923075 and current assignments (up to sign) [[-0.07832097 -0.01146866 -0.08708293 -0.06193093 -0.08828607 -1.35501733\n",
      "  -0.05376752 -0.03953272 -0.06193093 -0.0248368   1.14091862  1.12228909\n",
      "   0.21352363  0.17292843  0.15595587  0.16678338  1.47439144  0.2168096\n",
      "   0.15595587  0.1432769 ]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='SelfAdjointEigV2', l_rate=0.0001, seed_r=1, seed = 1, print_ratio=5000, iterations=10001, mean = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 2.20383543]. r_op = sqrt(average degree) : 1.97484176581 . Loss associated with r_op: 0.662781930024. r_op assignments [[-0.84520184 -1.04351574 -1.09783797 -1.0543809  -1.01460308 -0.94833992\n",
      "  -0.83218621 -0.8432221  -0.98416898 -1.00625452  1.0952479   0.95841708\n",
      "   1.12949094  0.9512771   0.32414146  1.14211636  1.0952479   0.92739904\n",
      "   0.87003141  1.17413761]].\n",
      "0\n",
      "current r: [ 2.20383543], current loss: 0.666961367887 and current assignments (up to sign) [[-0.84855647 -1.05652956 -1.09976991 -1.04980963 -1.00161017 -0.93801109\n",
      "  -0.84630963 -0.839108   -0.97165024 -1.01364658  1.09876804  0.9595762\n",
      "   1.12354594  0.93764335  0.32311382  1.15104672  1.09876804  0.928076\n",
      "   0.87132582  1.17617342]].\n",
      "5000\n",
      "current r: [ 2.20383335], current loss: 0.66696132456 and current assignments (up to sign) [[-0.84855645 -1.05652945 -1.09976989 -1.04980967 -1.00161027 -0.93801117\n",
      "  -0.84630952 -0.83910804 -0.97165035 -1.01364652  1.09876801  0.95957619\n",
      "   1.12354599  0.93764346  0.32311383  1.15104665  1.09876801  0.92807599\n",
      "   0.87132581  1.17617341]].\n",
      "10000\n",
      "current r: [ 2.20383127], current loss: 0.666961281234 and current assignments (up to sign) [[-0.84855642 -1.05652935 -1.09976988 -1.04980971 -1.00161038 -0.93801126\n",
      "  -0.8463094  -0.83910807 -0.97165045 -1.01364646  1.09876798  0.95957618\n",
      "   1.12354604  0.93764357  0.32311383  1.15104658  1.09876798  0.92807599\n",
      "   0.8713258   1.17617339]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='SelfAdjointEigV2', l_rate=0.0001, seed_r=100, seed =100, print_ratio=5000, iterations=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 2.1347376]. r_op = sqrt(average degree) : 1.97484176581 . Loss associated with r_op: 0.662781930024. r_op assignments [[-0.84520184 -1.04351574 -1.09783797 -1.0543809  -1.01460308 -0.94833992\n",
      "  -0.83218621 -0.8432221  -0.98416898 -1.00625452  1.0952479   0.95841708\n",
      "   1.12949094  0.9512771   0.32414146  1.14211636  1.0952479   0.92739904\n",
      "   0.87003141  1.17413761]].\n",
      "0\n",
      "current r: [ 2.1347376], current loss: 0.665565074398 and current assignments (up to sign) [[-0.84765907 -1.0529035  -1.09927242 -1.05112167 -1.00523673 -0.94090613\n",
      "  -0.84234876 -0.84028683 -0.97514447 -1.01161039  1.09780201  0.95925908\n",
      "   1.12521453  0.94142082  0.32339089  1.14855368  1.09780201  0.9279009\n",
      "   0.87097998  1.17562105]].\n",
      "5000\n",
      "current r: [ 2.29598877], current loss: 0.668938341386 and current assignments (up to sign) [[-0.84962699 -1.06101813 -1.1003416  -1.04814432 -0.99711386 -0.93440902\n",
      "  -0.8512409  -0.83761388 -0.96731799 -1.01614153  1.09994822  0.95996263\n",
      "   1.12146742  0.93298953  0.32278108  1.1541384   1.09994822  0.92827835\n",
      "   0.87173821  1.17684137]].\n",
      "10000\n",
      "current r: [ 2.48897366], current loss: 0.673319842502 and current assignments (up to sign) [[-0.85148744 -1.06931695 -1.10126871 -1.04494481 -0.98877913 -0.9276953\n",
      "  -0.86044271 -0.83474938 -0.9592873  -1.02067844  1.10208521  0.96065973\n",
      "   1.11758641  0.92444808  0.32219661  1.15987238  1.10208521  0.92861063\n",
      "   0.87245532  1.17803042]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='grassman_with_2d', l_rate=0.0001, seed_r=1000, seed = 100, print_ratio=5000, iterations=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
