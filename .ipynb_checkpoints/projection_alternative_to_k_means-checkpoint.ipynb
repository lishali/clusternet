{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import linalg_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "import networkx as nx\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ops.RegisterGradient(\"gradient_no_unitary_adjustment\")\n",
    "def _test1(op, grad_e, grad_v):\n",
    "    \"\"\"Gradient for SelfAdjointEigV2 derived with Joan with no adjustment for subspace\"\"\"\n",
    "    e = op.outputs[0]\n",
    "    v = op.outputs[1]\n",
    "    #dim = v.get_shape()\n",
    "    with ops.control_dependencies([grad_e.op, grad_v.op]):\n",
    "        if grad_v is not None:  \n",
    "            E = array_ops.diag(e)\n",
    "            #v_proj = arrary.ops.slice(v, [0,0], [])\n",
    "            grad_grassman = grad_v# - math_ops.batch_matmul(math_ops.batch_matmul(v, array_ops.transpose(grad_v)), v)\n",
    "            grad_a = math_ops.batch_matmul(grad_grassman, math_ops.batch_matmul(E, array_ops.transpose(grad_v)))+math_ops.batch_matmul(grad_v, math_ops.batch_matmul(E, array_ops.transpose(grad_grassman)))\n",
    "    return grad_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ops.RegisterGradient(\"grassman_with_2d\")\n",
    "def _test1(op, grad_e, grad_v):\n",
    "    \"\"\"Gradient for SelfAdjointEigV2 derived with Joan with no adjustment for subspace\"\"\"\n",
    "    e = op.outputs[0]\n",
    "    v = op.outputs[1]\n",
    "    #dim = v.get_shape()\n",
    "    with ops.control_dependencies([grad_e.op, grad_v.op]):\n",
    "        if grad_v is not None:  \n",
    "            E = array_ops.diag(e)\n",
    "            v_proj = array_ops.slice(v, [0,0], [20,2])\n",
    "            grad_grassman = grad_v - math_ops.batch_matmul(math_ops.batch_matmul(v_proj, array_ops.transpose(v_proj)), grad_v)\n",
    "            grad_a = math_ops.batch_matmul(grad_grassman, math_ops.batch_matmul(E, array_ops.transpose(grad_v)))+math_ops.batch_matmul(grad_v, math_ops.batch_matmul(E, array_ops.transpose(grad_grassman)))\n",
    "    return grad_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balanced_stochastic_blockmodel(communities=2, groupsize=3, p_in=0.5, p_out=0.1, seed=None):\n",
    "    #gives dense adjacency matrix representaiton of randomly generated SBM with balanced community size\n",
    "\n",
    "    G = nx.planted_partition_graph(l=communities, k=groupsize, p_in=p_in, p_out =p_out, seed=seed)\n",
    "    A = nx.adjacency_matrix(G).todense()\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_subspace(adj, groupsize, communities, diag, dim_proj):\n",
    "    normalizer = tf.cast(2.0*groupsize*communities, dtype=tf.float64)\n",
    "    total_degree = tf.cast(tf.reduce_sum(adj), dtype=tf.float64)\n",
    "    r = tf.sqrt(total_degree/normalizer)\n",
    "    BH_op = (tf.square(r)-1)*tf.diag(tf.ones(shape=[communities*groupsize], dtype=tf.float64))-r*adj+diag \n",
    "    val, vec = tf.self_adjoint_eig(BH_op) #this is already normalized so no need to normalize\n",
    "    subspace = tf.slice(vec, [0,0], [communities*groupsize, dim_proj])\n",
    "    return r, subspace\n",
    "\n",
    "def proj_magnitude(space, vector):\n",
    "    projection_op = tf.matmul(space, tf.transpose(space))\n",
    "    projection = tf.matmul(projection_op, vector)\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(projection))) #tf.reduce_sum(tf.abs(projection))#\n",
    "\n",
    "\n",
    "def rnd_vec_normed(communities, groupsize, seed=None):\n",
    "    rnd_vec1 = tf.Variable(tf.random_normal(shape=[communities*groupsize,1], mean=0.0,stddev=1.0,\n",
    "                                                    dtype=tf.float64,\n",
    "                                                    seed=seed))\n",
    "    return normalize_vec(rnd_vec1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_svm_cluster(communities = 2, group_size = 10, seed=1, seed_r=1, p=0.8, q=0.05, name='test1', projection_dim=2, iterations=100, \n",
    "                     print_ratio=10, l_rate=0.1):\n",
    "    \"\"\"testing to see if the loss will decrease backproping through very simple function\"\"\"\n",
    "    B = np.asarray(balanced_stochastic_blockmodel(communities, group_size, p, q, seed)).astype(np.double)\n",
    "    B = tf.cast(B, dtype = tf.float64)\n",
    "    \n",
    "    Diag = tf.diag(tf.reduce_sum(B,0))\n",
    "    Diag = tf.cast(Diag, tf.float64)\n",
    "\n",
    "    r =  tf.Variable(tf.random_normal(shape=[1], mean=2.0,\n",
    "                                 stddev=0.4, dtype=tf.float64,\n",
    "                                 seed=seed_r, name=None))\n",
    "\n",
    "    \n",
    "    BH = (tf.square(r)-1)*tf.diag(tf.ones(shape=[communities*group_size], dtype=tf.float64))-tf.mul(r, B)+Diag \n",
    "    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        g = tf.get_default_graph()\n",
    "        \n",
    "        with g.gradient_override_map({'SelfAdjointEigV2': name}):\n",
    "            eigenval, eigenvec = tf.self_adjoint_eig(BH)\n",
    "            #we try to do svm in this subspace \n",
    "            #or we can project it down to 1 dimensions, do the clustering there via some threshold and check if it makes sense \n",
    "            #by computing the loss, if it is too big, we change the angle we project down to...\n",
    "            \n",
    "            \n",
    "            eigenvec_proj = tf.slice(eigenvec, [0,0], [communities*group_size, projection_dim])\n",
    "            \n",
    "            \n",
    "            \n",
    "            true_assignment_a = tf.concat(0, [-1*tf.ones([group_size], dtype=tf.float64),\n",
    "                                      tf.ones([group_size], dtype=tf.float64)])\n",
    "            true_assignment_b = -1*true_assignment_a\n",
    "            true_assignment_a = tf.expand_dims(true_assignment_a, 1)\n",
    "            true_assignment_b = tf.expand_dims(true_assignment_b, 1)\n",
    "\n",
    "            \n",
    "            projected_a = tf.matmul(tf.matmul(eigenvec_proj, tf.transpose(eigenvec_proj)), true_assignment_a)#tf.transpose(true_assignment_a))\n",
    "            projected_b = tf.matmul(tf.matmul(eigenvec_proj, tf.transpose(eigenvec_proj)), true_assignment_b)#tf.transpose(true_assignment_b))\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss = tf.minimum(tf.reduce_sum(tf.square(tf.sub(projected_a, true_assignment_a))),\n",
    "                              tf.reduce_sum(tf.square(tf.sub(projected_b, true_assignment_b))))\n",
    "            \n",
    "            optimizer = tf.train.GradientDescentOptimizer(l_rate)\n",
    "            \n",
    "            train = optimizer.minimize(loss, var_list=[r])\n",
    "\n",
    "            eigenvec_grad = tf.gradients(eigenvec, r)\n",
    "            loss_grad = tf.gradients(loss, r)\n",
    "            \n",
    "            \n",
    "            \n",
    "            r_op, target = target_subspace(adj=B, groupsize=group_size, communities=communities, diag=Diag, dim_proj=projection_dim)  \n",
    "            \n",
    "            r_op_projection_a = tf.matmul(tf.matmul(target, tf.transpose(target)), true_assignment_a)\n",
    "            r_op_projection_b = tf.matmul(tf.matmul(target, tf.transpose(target)), true_assignment_b)\n",
    "            r_op_loss = tf.minimum(tf.reduce_sum(tf.square(tf.sub(r_op_projection_a, true_assignment_a))),\n",
    "                              tf.reduce_sum(tf.square(tf.sub(r_op_projection_b, true_assignment_b))))\n",
    "            \n",
    "            init = tf.initialize_all_variables()\n",
    "            \n",
    "            \n",
    "            sess.run(init)\n",
    "            a,b,c,d= sess.run([r, r_op, r_op_loss, tf.transpose(r_op_projection_a)])\n",
    "            print \"initial r: {}. r_op = sqrt(average degree) : {} . Loss associated with r_op: {}. r_op assignments {}.\".format(a, b, c, d)\n",
    "            for i in range(iterations):           \n",
    "                if i%print_ratio==0:\n",
    "                    print i\n",
    "                    a,b,c = sess.run([r, loss, tf.transpose(projected_a)])\n",
    "                    print \"current r: {}, current loss: {} and current assignments (up to sign) {}.\".format(a,b,c)\n",
    "                    sess.run(train)\n",
    "                    \n",
    "                \n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 2.0390076]. r_op = sqrt(average degree) : 1.96214168703 . Loss associated with r_op: 0.239352945504. r_op assignments [[-0.81008947 -0.89831384 -1.06456991 -1.0936426  -1.05592124 -0.92168635\n",
      "  -0.88318609 -1.01724697 -1.0936426  -1.05086035  0.98362854  0.98397103\n",
      "   1.03054178  0.80692791  1.13586066  0.80914196  1.03176237  1.10198787\n",
      "   1.13586066  0.85180488]].\n",
      "0\n",
      "current r: [ 2.0390076], current loss: 0.242034941282 and current assignments (up to sign) [[-0.80969087 -0.90033984 -1.06428228 -1.09562837 -1.05541189 -0.91661797\n",
      "  -0.88466722 -1.01514999 -1.09562837 -1.05109873  0.9801446   0.98049625\n",
      "   1.03009306  0.80678441  1.13958865  0.80907127  1.02730685  1.10301719\n",
      "   1.13958865  0.8533586 ]].\n",
      "5000\n",
      "current r: [ 2.1195467], current loss: 0.244873027222 and current assignments (up to sign) [[-0.809293   -0.90229375 -1.06398511 -1.09754292 -1.05489929 -0.91173881\n",
      "  -0.88609694 -1.01310289 -1.09754292 -1.05130598  0.97676424  0.97712452\n",
      "   1.02962486  0.80661799  1.14319756  0.80897439  1.02299353  1.10398802\n",
      "   1.14319756  0.85484267]].\n",
      "10000\n",
      "current r: [ 2.20917426], current loss: 0.248028505245 and current assignments (up to sign) [[-0.80887299 -0.90428922 -1.06366145 -1.09949762 -1.05435402 -0.90676494\n",
      "  -0.88755845 -1.01098724 -1.09949762 -1.05149454  0.97329138  0.97366024\n",
      "   1.0291104   0.80641924  1.14689693  0.80884599  1.01857202  1.10495692\n",
      "   1.14689693  0.85634333]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='gradient_no_unitary_adjustment', l_rate=0.0001, seed_r=1, seed = 1, print_ratio=5000, iterations=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial r: [ 2.67157745]. r_op = sqrt(average degree) : 1.96214168703 . Loss associated with r_op: 0.239352945504. r_op assignments [[-0.81008947 -0.89831384 -1.06456991 -1.0936426  -1.05592124 -0.92168635\n",
      "  -0.88318609 -1.01724697 -1.0936426  -1.05086035  0.98362854  0.98397103\n",
      "   1.03054178  0.80692791  1.13586066  0.80914196  1.03176237  1.10198787\n",
      "   1.13586066  0.85180488]].\n",
      "0\n",
      "current r: [ 2.67157745], current loss: 0.26343783925 and current assignments (up to sign) [[-0.80703805 -0.91233871 -1.06214638 -1.10737575 -1.05192861 -0.88679737\n",
      "  -0.8934686  -1.00219836 -1.10737575 -1.05201496  0.95907246  0.95947373\n",
      "   1.02665465  0.8053149   1.16195814  0.80801834  1.00057164  1.10862377\n",
      "   1.16195814  0.86223384]].\n",
      "5000\n",
      "current r: [ 2.82714156], current loss: 0.26814192594 and current assignments (up to sign) [[-0.8065258  -0.91442394 -1.06169848 -1.10941457 -1.05124062 -0.88165065\n",
      "  -0.89500364 -0.99985554 -1.10941457 -1.05208616  0.95533462  0.9557437\n",
      "   1.02591695  0.80494778  1.16589539  0.80772102  0.99586661  1.10950795\n",
      "   1.16589539  0.86371469]].\n",
      "10000\n",
      "current r: [ 3.00136343], current loss: 0.273096994072 and current assignments (up to sign) [[-0.80600321 -0.91649013 -1.06123189 -1.11143386 -1.0505344  -0.87656171\n",
      "  -0.89652635 -0.99750751 -1.11143386 -1.05213055  0.95160899  0.9520256\n",
      "   1.02514392  0.80455033  1.16981092  0.80739196  0.99118789  1.11035631\n",
      "   1.16981092  0.86516268]].\n"
     ]
    }
   ],
   "source": [
    "test_svm_cluster(name='grassman_with_2d', l_rate=0.0001, seed_r=1, seed = 1, print_ratio=5000, iterations=10001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
