{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals\n",
    "\n",
    "    -implement spectral clustering\n",
    "    -implement belief propagation\n",
    "    -implement it with power method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla Belief propagation algorithm\n",
    "\n",
    "Input: a stochastic block model (adjaceny matrix)\n",
    "\n",
    "1) for each v, v' adajecnt, randomly draw y^1_vv' from N(0,1)\n",
    "\n",
    "2) find all cycles in G of length r or less.  To do this, we may have to multiply r times, but avoid the backtracking ones.  \n",
    "\n",
    "3) for each 1<t<m and each adjacent v, v' set y^t_vv' = sum of all the y's up to that t....since we seeded it with N(0,1) we will have the value for y^1.  Howver, if (v,v') is part of a cycle of r or less, don't do this.  Instead, \n",
    "4) if your edge is part of a r cycle, then we subtract from the previous sum, the accumulated influence of the other adjacent edge in our cycle, unless the r cycle is the same lenght as the time steps, in which case we just subtract the original randomized N(0,1)\n",
    "\n",
    "5) Let Y be the matrix composed of all y_vv' summed up with edge vertices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first try implementing power method in tensorflow\n",
    "\n",
    "#feed in graphs, as in adjaceny matrices, or just nodes and connected edges?\n",
    "\n",
    "#each layer will feed in a random vector, then multiply our matrix by the random vector\n",
    "#many times.  At the end, we subtract away the orignal vector component\n",
    "\n",
    "#next layer we just randomize another vector...\n",
    "\n",
    "#how many layers.  The depth can be learned by adding identify layers, which is essentially \n",
    "#self multiplication.  \n",
    "\n",
    "#adjaceny matrix to graph laplacian \n",
    "\n",
    "#suppose for simplicity that our graph has the same dimnension\n",
    "\n",
    "#10by10 graph, so feed adjacency matrix\n",
    "dim_graph = 10\n",
    "dim_subspace = 3\n",
    "\n",
    "x = tf.placeholder(\"float\", shape = [dim_graph, dim_graph])\n",
    "y = tf.placeholder(\"float\", shape = [None, dim_graph])\n",
    "#y is our community classification answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.variables.Variable at 0x1142f5690>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize the k random vectors (whcih should be tuned)\n",
    "#that we are feeding to the laplacian operator\n",
    "d = {}\n",
    "for i in range(1,dim_subspace):\n",
    "    d[\"v{}\".format(i)] = tf.Variable(tf.random_normal(shape=[1, dim_graph], \n",
    "                         mean=0.0, \n",
    "                         stddev = 1.0))\n",
    "\n",
    "#may have to define another way...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#laplacian\n",
    "\n",
    "#diffusion:\n",
    "\n",
    "def DiffusionVertex(vertex, graph, vector):\n",
    "    #assume graph is in form of adjacency matrix \n",
    "    neighbour_vector = tf.slice(graph, begin=[vertex,0], size=[1,dim_graph])\n",
    "    prod = tf.mul(vector, neighbour_vector)\n",
    "    diffusion = tf.reduce_sum(prod, 1)\n",
    "    return(diffusion)\n",
    "\n",
    "def GraphLaplacian(graph, vector):\n",
    "    #takes in graph and vector on graph indices, returns new vector\n",
    "    tmp_v = tf.constant(range(dim_graph))\n",
    "    def DiffusionVertex_1(vertex):\n",
    "        return DiffusionVertex(vertex, graph, vector)\n",
    "    diffusion_vector = tf.map_fn(DiffusionVertex_1, tmp_v) \n",
    "    \n",
    "    return tf.reduce_sum(diffusion_vector)\n",
    "\n",
    "def euclidean_norm(tensor): #need to have this for tf to work\n",
    "    squareroot_tensor = tf.square(tensor)\n",
    "    euclidean_norm = tf.reduce_sum(squareroot_tensor)\n",
    "    return euclidean_norm\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,   4.,   9.,  16.,  25.,  36.], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elems = np.array([1, 2, 3, 4, 5, 6], dtype=\"float32\")\n",
    "squares = tf.map_fn(lambda x: x * x, elems)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(squares)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  3.,   8.,  15.],\n",
      "       [  6.,  12.,  20.],\n",
      "       [  9.,  16.,  25.]], dtype=float32), array([[ 3.,  4.,  5.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#test to see if your diffusion operator works\n",
    "\n",
    "x = tf.constant([[1,2,3], [2,3,4], [3,4,5]], dtype=tf.float32)\n",
    "y = tf.Variable(tf.random_normal(shape=[3], \n",
    "                         mean=0.0, \n",
    "                         stddev = 1.0))\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "answer = tf.mul(x,tf.slice(x, [2,0], [1,3])),tf.slice(x, [2,0], [1,3])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print sess.run(answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.408573\n"
     ]
    }
   ],
   "source": [
    "dim_graph = 3\n",
    "dim_subspace = 3\n",
    "\n",
    "x = tf.constant([[1,2,3], [2,3,4], [3,4,5]])\n",
    "x = tf.cast(x, tf.float32)\n",
    "f_x = tf.Variable(tf.random_normal(shape=[3], \n",
    "                         mean=0.0, \n",
    "                         stddev = 1.0))\n",
    "\n",
    "blah = [DiffusionVertex(i, graph=x, vector=f_x) for i in range(dim_graph)]\n",
    "blah = tf.reduce_sum(tf.pack(blah))\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print sess.run(blah)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-66-7953dbfaee7d>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-66-7953dbfaee7d>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    new_vector = f[\"v2\"] - adjacency_vector\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#now let continue with the powermethod from the above:\n",
    "#since the largest k eigenvalues is supposed to come from the adjacney matrix, \n",
    "#we leave the diffusion component for now\n",
    "\n",
    "dim_graph = 3\n",
    "dim_subspace = 3\n",
    "\n",
    "#x = tf.placeholder(tf.float32, shape=(dim_graph, dim_graph))\n",
    "x = tf.cast(x, tf.float32)\n",
    "\n",
    "answer_lst = []\n",
    "\n",
    "lmbda_lst = []\n",
    "\n",
    "f = {}\n",
    "for i in range(1,dim_subspace):\n",
    "    f[\"v{}\".format(i)] = tf.Variable(tf.random_normal(shape=[dim_graph, 1], \n",
    "                         mean=0.0, \n",
    "                         stddev = 1.0))\n",
    "\n",
    "adjacency_vector = tf.matmul(x, f[\"v1\"])\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "adjacency_vector = tf.matmul(x, adjacency_vector)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "adjacency_vector = tf.matmul(x, adjacency_vector)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "answer_lst.append(adjacency_vector)\n",
    "lmbda_lst.append(euclidean_norm(tf.matmul(x, adjacency_vector)) #since it is already normalized\n",
    "\n",
    "new_vector = f[\"v2\"] - adjacency_vector\n",
    "adjacency_vector = tf.malmul(x, new_vector)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "adjacency_vector = tf.matmul(x, adjacency_vector)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "adjacency_vector = tf.matmul(x, adjacency_vector)\n",
    "adjacency_vector = adjacency_vector/euclidean_norm(adjacency_vector)\n",
    "\n",
    "answer_lst.append(adjacency_vector)\n",
    "lmbda_lst.append(euclidean_norm(tf.matmul(x, adjacency_vector)) #since it is already normalized\n",
    "\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print sess.run(answer_lst, feed_dict = {x: [[1,2,3], [2,3,4], [3,4,5]]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_lst = [0]*dim_subspace\n",
    "answer_lst[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with using tf.map_fn below.  Using list instead, however this the step that is parallelizable, hence not sure that writing in terms of for loop is ideal... How will tensorflow handle it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.3163\n"
     ]
    }
   ],
   "source": [
    "dim_graph = 3\n",
    "dim_subspace = 3\n",
    "\n",
    "x = tf.constant([[1,2,3], [2,3,4], [3,4,5]])\n",
    "x = tf.cast(x, tf.float32)\n",
    "y = tf.Variable(tf.random_normal(shape=[dim_graph], \n",
    "                         mean=0.0, \n",
    "                         stddev = 1.0))\n",
    "\n",
    "tmp_v = np.array(range(dim_graph), dtype=\"int\")\n",
    "\n",
    "def DiffusionVertex_1(vertex):\n",
    "    return DiffusionVertex(vertex, x, y)\n",
    "\n",
    "diffusion_vector = [DiffusionVertex_1(i) for i in range(dim_graph)]\n",
    "#diffusion_vector = tf.scan(DiffusionVertex_1, tmp_v)\n",
    "diffusion_vector = tf.pack(diffusion_vector)\n",
    "\n",
    "#answer = tf.shape(tmp_v)\n",
    "answer = diffusion_vector\n",
    "\n",
    "#answer = tf.shape(diffusion_vector)\n",
    "answer = tf.reduce_sum(diffusion_vector)\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print sess.run(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
