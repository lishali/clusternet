{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import gen_nn_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import linalg_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-aeaa3c06242a>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-aeaa3c06242a>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    with ops.control_de pendencies([grad_e.op, grad_v.op]):\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "@ops.RegisterGradient(\"gradient_no_unitary_adjustment\")\n",
    "def _test1(op, grad_e, grad_v):\n",
    "    \"\"\"Gradient for SelfAdjointEigV2 derived with Joan with no adjustment for subspace\"\"\"\n",
    "    e = op.outputs[0]\n",
    "    v = op.outputs[1]\n",
    "    #dim = v.get_shape()\n",
    "    with ops.control_de pendencies([grad_e.op, grad_v.op]):\n",
    "        if grad_v is not None:  \n",
    "            E = array_ops.diag(e)\n",
    "            #v_proj = arrary.ops.slice(v, [0,0], [])\n",
    "            grad_grassman = grad_v# - math_ops.batch_matmul(math_ops.batch_matmul(v, array_ops.transpose(grad_v)), v)\n",
    "            grad_a = math_ops.batch_matmul(grad_grassman, math_ops.batch_matmul(E, array_ops.transpose(grad_v)))+math_ops.batch_matmul(grad_v, math_ops.batch_matmul(E, array_ops.transpose(grad_grassman)))\n",
    "    return grad_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@ops.RegisterGradient(\"grassman_with_2d\")\n",
    "def _test1(op, grad_e, grad_v):\n",
    "    \"\"\"Gradient for SelfAdjointEigV2 derived with Joan with no adjustment for subspace\"\"\"\n",
    "    e = op.outputs[0]\n",
    "    v = op.outputs[1]\n",
    "    #dim = v.get_shape()\n",
    "    with ops.control_dependencies([grad_e.op, grad_v.op]):\n",
    "        if grad_v is not None:  \n",
    "            E = array_ops.diag(e)\n",
    "            v_proj = array_ops.slice(v, [0,0], [20,2])\n",
    "            grad_grassman = grad_v - math_ops.batch_matmul(math_ops.batch_matmul(v_proj, array_ops.transpose(v_proj)), grad_v)\n",
    "            grad_a = math_ops.batch_matmul(grad_grassman, math_ops.batch_matmul(E, array_ops.transpose(grad_v)))+math_ops.batch_matmul(grad_v, math_ops.batch_matmul(E, array_ops.transpose(grad_grassman)))\n",
    "    return grad_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balanced_stochastic_blockmodel(communities=2, groupsize=3, p_in=0.5, p_out=0.1, seed=None):\n",
    "    #gives dense adjacency matrix representaiton of randomly generated SBM with balanced community size\n",
    "\n",
    "    G = nx.planted_partition_graph(l=communities, k=groupsize, p_in=p_in, p_out =p_out, seed=seed)\n",
    "    A = nx.adjacency_matrix(G).todense()\n",
    "    \n",
    "    return A\n",
    "def target_subspace(adj, groupsize, communities, diag, dim_proj):\n",
    "    normalizer = tf.cast(2.0*groupsize*communities, dtype=tf.float64)\n",
    "    total_degree = tf.cast(tf.reduce_sum(adj), dtype=tf.float64)\n",
    "    r = tf.sqrt(total_degree/normalizer)\n",
    "    BH_op = (tf.square(r)-1)*tf.diag(tf.ones(shape=[communities*groupsize], dtype=tf.float64))-r*adj+diag \n",
    "    val, vec = tf.self_adjoint_eig(BH_op) #this is already normalized so no need to normalize\n",
    "    subspace = tf.slice(vec, [0,0], [communities*groupsize, dim_proj])\n",
    "    return r, subspace\n",
    "\n",
    "def proj_magnitude(space, vector):\n",
    "    projection_op = tf.matmul(space, tf.transpose(space))\n",
    "    projection = tf.matmul(projection_op, vector)\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(projection))) #tf.reduce_sum(tf.abs(projection))#\n",
    "\n",
    "\n",
    "def rnd_vec_normed(communities, groupsize, seed=None):\n",
    "    rnd_vec1 = tf.Variable(tf.random_normal(shape=[communities*groupsize,1], mean=0.0,stddev=1.0,\n",
    "                                                    dtype=tf.float64,\n",
    "                                                    seed=seed))\n",
    "    return normalize_vec(rnd_vec1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's create a dataset\n",
    "communities = 2\n",
    "group_size = 10\n",
    "\n",
    "data_easy = [np.asarray(balanced_stochastic_blockmodel(communities, group_size, p, 0.1*p)).astype(np.double) for p in np.linspace(0.2, 0.4,70)]\n",
    "data_hard = [np.asarray(balanced_stochastic_blockmodel(communities, group_size, p, 0.5*p)).astype(np.double) for q in np.linspace(0.2, 0.4,7)]\n",
    "\n",
    "data = data_easy+data_hard\n",
    "np.random.shuffle(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,\n",
       "         1.,  1.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,\n",
       "         0.,  0.,  1.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  1.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 1.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,\n",
       "         1.,  1.,  1.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,\n",
       "         1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,\n",
       "         0.,  1.,  1.,  0.,  0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         1.,  0.,  1.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         1.,  1.,  0.,  0.,  1.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  1.,  1.,  0.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  1.,  0.,  1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         1.,  1.,  0.,  0.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we need to do online training for now as I cannot write eigendecomposition in batch\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn_average_deg_variable(communities = 2, group_size = 10, seed_v=None, name='grassman_with_2d', projection_dim=2, \n",
    "                     print_ratio=10, l_rate=0.1, mean=0.3, sd=1.0):\n",
    "    \"\"\"testing to see if the loss will decrease backproping through very simple function\"\"\"\n",
    "    \n",
    "    #now p and q will be generated from a range of \n",
    "    \n",
    "    X = tf.placeholder(dtype=tf.float64, shape=[communities*group_size, communities*group_size])\n",
    "    \n",
    "    B = tf.cast(X, dtype = tf.float64)\n",
    "    \n",
    "    Diag = tf.diag(tf.reduce_sum(B,0))\n",
    "    Diag = tf.cast(Diag, tf.float64)\n",
    "    \n",
    "    #by symmetry I should make this a bit more constrained.  so\n",
    "\n",
    "    v =  tf.Variable(tf.random_normal(shape=[communities*group_size,1], mean=mean,\n",
    "                                 stddev=sd, dtype=tf.float64,\n",
    "                                 seed=seed_v, name=None))\n",
    "    \n",
    "     \n",
    "    \n",
    "    degree = tf.cast(communities*group_size, dtype=tf.float64)\n",
    "    r_param = tf.div(tf.cast(1.0, dtype=tf.float64), degree)*tf.matmul(tf.transpose(v), tf.matmul(Diag, v))\n",
    "\n",
    "    \n",
    "    BH = (tf.square(r_param)-1)*tf.diag(tf.ones(shape=[communities*group_size], dtype=tf.float64))-tf.mul(r_param, B)+Diag \n",
    "    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        g = tf.get_default_graph()\n",
    "        \n",
    "        with g.gradient_override_map({'SelfAdjointEigV2': name}):\n",
    "            eigenval, eigenvec = tf.self_adjoint_eig(BH)\n",
    "            #we try to do svm in this subspace \n",
    "            #or we can project it down to 1 dimensions, do the clustering there via some threshold and check if it makes sense \n",
    "            #by computing the loss, if it is too big, we change the angle we project down to...\n",
    "            \n",
    "            \n",
    "            eigenvec_proj = tf.slice(eigenvec, [0,0], [communities*group_size, projection_dim])\n",
    "            \n",
    "            \n",
    "            \n",
    "            true_assignment_a = tf.concat(0, [-1*tf.ones([group_size], dtype=tf.float64),\n",
    "                                      tf.ones([group_size], dtype=tf.float64)])\n",
    "            true_assignment_b = -1*true_assignment_a\n",
    "            true_assignment_a = tf.expand_dims(true_assignment_a, 1)\n",
    "            true_assignment_b = tf.expand_dims(true_assignment_b, 1)\n",
    "\n",
    "            \n",
    "            projected_a = tf.matmul(tf.matmul(eigenvec_proj, tf.transpose(eigenvec_proj)), true_assignment_a)#tf.transpose(true_assignment_a))\n",
    "            projected_b = tf.matmul(tf.matmul(eigenvec_proj, tf.transpose(eigenvec_proj)), true_assignment_b)#tf.transpose(true_assignment_b))\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss = tf.minimum(tf.reduce_sum(tf.square(tf.sub(projected_a, true_assignment_a))),\n",
    "                              tf.reduce_sum(tf.square(tf.sub(projected_b, true_assignment_b))))\n",
    "            \n",
    "            optimizer = tf.train.GradientDescentOptimizer(l_rate)\n",
    "            \n",
    "            train = optimizer.minimize(loss, var_list=[v])\n",
    "\n",
    "            eigenvec_grad = tf.gradients(eigenvec, v)\n",
    "            loss_grad = tf.gradients(loss, v)\n",
    "            \n",
    "            \n",
    "            \n",
    "            r_op, target = target_subspace(adj=B, groupsize=group_size, communities=communities, diag=Diag, dim_proj=projection_dim)  \n",
    "            \n",
    "            r_op_projection_a = tf.matmul(tf.matmul(target, tf.transpose(target)), true_assignment_a)\n",
    "            r_op_projection_b = tf.matmul(tf.matmul(target, tf.transpose(target)), true_assignment_b)\n",
    "            r_op_loss = tf.minimum(tf.reduce_sum(tf.square(tf.sub(r_op_projection_a, true_assignment_a))),\n",
    "                              tf.reduce_sum(tf.square(tf.sub(r_op_projection_b, true_assignment_b))))\n",
    "            \n",
    "            init = tf.initialize_all_variables()\n",
    "            \n",
    "            \n",
    "            sess.run(init)\n",
    "            sess.run(train, feed_dict={X:data[0]})\n",
    "            a,r, b,c,d= sess.run([v, r_param, r_op_loss, r_op, tf.transpose(r_op_projection_a)], feed_dict={X:data[0]})\n",
    "            a_lst = []\n",
    "            r_lst = []\n",
    "            b_lst = []\n",
    "            c_lst = []\n",
    "            d_lst = []\n",
    "            \n",
    "            a_lst.append(a)\n",
    "            r_lst.append(r)\n",
    "            b_lst.append(b)\n",
    "            c_lst.append(None)\n",
    "            d_lst.append(d)\n",
    "            \n",
    "            print \"initial v: {}. r_param: {}. r_op = sqrt(average degree) : {} . Loss associated with r_op: {}. r_op assignments {}.\".format(a, r, c, b, d)\n",
    "            for i in range(len(data)):   \n",
    "                try:\n",
    "                    sess.run(feed_dict={X:data[i]})\n",
    "                    sess.run(train)\n",
    "                except: \n",
    "                    pass\n",
    "                \n",
    "                if i%print_ratio==0:  \n",
    "                    #print i\n",
    "                    try:\n",
    "                        a,r, b,c,d = sess.run([v, r_param, loss, tf.gradients(loss, v), tf.transpose(projected_a)], feed_dict={X:data[i]}) \n",
    "                        a_lst.append(a)\n",
    "                        r_lst.append(r)\n",
    "                        b_lst.append(b)\n",
    "                        c_lst.append(c)\n",
    "                        d_lst.append(d)\n",
    "                    except:\n",
    "                        a,r, b,c,d = None, None, None, None, None \n",
    "                        a_lst.append(a)\n",
    "                        r_lst.append(r)\n",
    "                        b_lst.append(b)\n",
    "                        c_lst.append(c)\n",
    "                        d_lst.append(d)\n",
    "                    #print \"current r: {}, current loss: {}, gradient of loss/r is {} and current assignments (up to sign) {}.\".format(a,b,c,d)  \n",
    "\n",
    "    d = {\"v\": a_lst, \"r_param\": r_lst, \"loss\": b_lst, \"gradient_loss_v\": c_lst, \"projection\": d_lst}\n",
    "    d = pd.DataFrame(d)\n",
    "    d.to_csv(\"/Users/xiangli/Desktop/clusternet/Learning_r_matrix_data/r{}rate{}p{}q{}step{}.csv\".format(mean, l_rate, p, q, print_ratio))\n",
    "    return  d\n",
    "                \n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-ef734265e48d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn_average_deg_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'grassman_with_2d'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0ml_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-371a578ab198>\u001b[0m in \u001b[0;36mlearn_average_deg_variable\u001b[0;34m(communities, group_size, seed, seed_v, name, projection_dim, print_ratio, l_rate, mean, sd)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_op_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_op_projection_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0ma_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m                 run_metadata):\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    992\u001b[0m         graph_def, self._current_version = self._graph._as_graph_def(\n\u001b[1;32m    993\u001b[0m             \u001b[0mfrom_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m             add_shapes=self._add_shapes)\n\u001b[0m\u001b[1;32m    995\u001b[0m         \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_as_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   2204\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfrom_version\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mop_id\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfrom_version\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m           \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2207\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0madd_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m\"_output_shapes\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/google/protobuf/internal/containers.pyc\u001b[0m in \u001b[0;36mextend\u001b[0;34m(self, elem_seq)\u001b[0m\n\u001b[1;32m    387\u001b[0m       \u001b[0mnew_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m       \u001b[0mnew_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SetListener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mnew_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0mlistener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mMergeFrom\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m   1249\u001b[0m           \u001b[0mfield_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m           \u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m         \u001b[0mfield_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCPPTYPE_MESSAGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_present_in_parent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/google/protobuf/internal/containers.pyc\u001b[0m in \u001b[0;36mMergeFrom\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    595\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m     \u001b[0;31m# self._message_listener.Modified() not required here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;31m# mutations to submessages already propagate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/google/protobuf/internal/containers.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m       \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key_checker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m       \u001b[0mnew_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m       \u001b[0mnew_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SetListener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_listener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_present_in_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage_listener_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNullMessageListener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listener_for_children\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Listener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfield_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn_average_deg_variable(print_ratio=1, name='grassman_with_2d',  l_rate=0.00001, mean=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's create a dataset\n",
    "communities = 2\n",
    "group_size = 10\n",
    "\n",
    "data_easy = [np.asarray(balanced_stochastic_blockmodel(communities, group_size, p, 0.1*p)).astype(np.double) for p in np.linspace(0.1, 0.3,50)]\n",
    "data_hard = [np.asarray(balanced_stochastic_blockmodel(communities, group_size, p, 0.5*p)).astype(np.double) for q in np.linspace(0.1, 0.3,5)]\n",
    "\n",
    "#data = data_easy+data_hard\n",
    "np.random.shuffle(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn_average_deg_variable(communities = 2, group_size = 10, seed_v=None, projection_dim=2, print_ratio=10, l_rate=0.00000001, mean=0.3, sd=0.1):\n",
    "    \"\"\"testing to see if the loss will decrease backproping through very simple function\"\"\"\n",
    "    \n",
    "    #now p and q will be generated from a range of \n",
    "    \n",
    "    X = tf.placeholder(dtype=tf.float64, shape=[communities*group_size, communities*group_size])\n",
    "    \n",
    "    B = tf.cast(X, dtype = tf.float64)\n",
    "    \n",
    "    Diag = tf.diag(tf.reduce_sum(B,0))\n",
    "    Diag = tf.cast(Diag, tf.float64)\n",
    "    \n",
    "    #by symmetry I should make this a bit more constrained.  so\n",
    "\n",
    "    v =  tf.Variable(tf.random_normal(shape=[communities*group_size,1], mean=mean,\n",
    "                                 stddev=sd, dtype=tf.float64,\n",
    "                                 seed=seed_v, name=None))\n",
    "    v_sqr = tf.square(v)\n",
    "    \n",
    "     \n",
    "    \n",
    "    degree = tf.cast(communities*group_size, dtype=tf.float64)\n",
    "    r_param = tf.div(tf.cast(1.0, dtype=tf.float64), degree)*tf.matmul(tf.transpose(v_sqr), tf.matmul(Diag, v_sqr))\n",
    "\n",
    "    \n",
    "    BH = (tf.square(r_param)-1)*tf.diag(tf.ones(shape=[communities*group_size], dtype=tf.float64))-tf.mul(r_param, B)+Diag \n",
    "    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        g = tf.get_default_graph()\n",
    "        \n",
    "        with g.gradient_override_map({'SelfAdjointEigV2': 'grassman_with_2d'}):\n",
    "            eigenval, eigenvec = tf.self_adjoint_eig(BH)\n",
    "            #we try to do svm in this subspace \n",
    "            #or we can project it down to 1 dimensions, do the clustering there via some threshold and check if it makes sense \n",
    "            #by computing the loss, if it is too big, we change the angle we project down to...\n",
    "            \n",
    "            \n",
    "            eigenvec_proj = tf.slice(eigenvec, [0,0], [communities*group_size, projection_dim])\n",
    "            \n",
    "            \n",
    "            \n",
    "            true_assignment_a = tf.concat(0, [-1*tf.ones([group_size], dtype=tf.float64),\n",
    "                                      tf.ones([group_size], dtype=tf.float64)])\n",
    "            true_assignment_b = -1*true_assignment_a\n",
    "            true_assignment_a = tf.expand_dims(true_assignment_a, 1)\n",
    "            true_assignment_b = tf.expand_dims(true_assignment_b, 1)\n",
    "\n",
    "            \n",
    "            projected_a = tf.matmul(tf.matmul(eigenvec_proj, tf.transpose(eigenvec_proj)), true_assignment_a)#tf.transpose(true_assignment_a))\n",
    "            projected_b = tf.matmul(tf.matmul(eigenvec_proj, tf.transpose(eigenvec_proj)), true_assignment_b)#tf.transpose(true_assignment_b))\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss = tf.minimum(tf.reduce_sum(tf.square(tf.sub(projected_a, true_assignment_a))),\n",
    "                              tf.reduce_sum(tf.square(tf.sub(projected_b, true_assignment_b))))\n",
    "            \n",
    "            optimizer = tf.train.AdamOptimizer(l_rate)\n",
    "            \n",
    "            train = optimizer.minimize(loss, var_list=[v])\n",
    "\n",
    "            eigenvec_grad = tf.gradients(eigenvec, v)\n",
    "            loss_grad = tf.gradients(loss, v)\n",
    "            \n",
    "            r_op, target = target_subspace(adj=B, groupsize=group_size, communities=communities, diag=Diag, dim_proj=projection_dim)\n",
    "            r_diff = (r_op-r_param) #difference between r_op and r_param is how close we are to the average degree\n",
    "            \n",
    "            \n",
    "            init = tf.initialize_all_variables()\n",
    "            \n",
    "            \n",
    "            sess.run(init)\n",
    "            a,r, b= sess.run([v, r_param, r_diff], feed_dict={X:data[0]})\n",
    "            a_lst = []\n",
    "            r_lst = []\n",
    "            r_diff_list = []\n",
    "            b_lst = []\n",
    "            c_lst = []\n",
    "            d_lst = []\n",
    "            \n",
    "            a_lst.append(a)\n",
    "            r_lst.append(r)\n",
    "            r_diff_list.append(b)\n",
    "            b_lst.append(None)\n",
    "            c_lst.append(None)\n",
    "            d_lst.append(None)\n",
    "            \n",
    "            print \"initial v: {}. r_param: {}, difference between r_param and sqrt average deg {}.\".format(a, r, b)\n",
    "            for i in range(len(data)):   \n",
    "                try:\n",
    "                    sess.run(train, feed_dict={X:data[i]})\n",
    "                    #if i%print_ratio==0:  \n",
    "                    #print i\n",
    "                        #try:\n",
    "                    a,r, k, b,c,d = sess.run([v, r_param, r_diff, loss, tf.gradients(loss, v), tf.transpose(projected_a)], feed_dict={X:data[i]}) \n",
    "                    a_lst.append(a)\n",
    "                    r_lst.append(r)\n",
    "                    r_diff_list.append(k)\n",
    "                    b_lst.append(b)\n",
    "                    c_lst.append(c)\n",
    "                    d_lst.append(d)\n",
    "                    \n",
    "                            \n",
    "                except: \n",
    "                    a,r, k, b,c,d = None, None, None, None, None, None \n",
    "                    a_lst.append(a)\n",
    "                    r_lst.append(r)\n",
    "                    r_diff_list.append(k)\n",
    "                    b_lst.append(b)\n",
    "                    c_lst.append(c)\n",
    "                    d_lst.append(d)\n",
    "                    pass\n",
    "                \n",
    "\n",
    "                    #print \"current r: {}, current loss: {}, gradient of loss/r is {} and current assignments (up to sign) {}.\".format(a,b,c,d)  \n",
    "\n",
    "    d = {\"v\": a_lst, \"r_param\": r_lst, \"r_diff\": r_diff_list, \"loss\": b_lst, \"gradient_loss_v\": c_lst, \"projection\": d_lst}\n",
    "    d = pd.DataFrame(d)\n",
    "    easy_size = len(data)\n",
    "    d.to_csv(\"/Users/xiangli/Desktop/clusternet/Learning_r_matrix_data/mean{}l_rate{}step{}data_size{}.csv\".format(mean, l_rate, print_ratio, easy_size))\n",
    "    return  d\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data = data+data+data+data\n",
    "#np.random.shuffle(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument is not a tf.Variable: Tensor(\"Square_54:0\", shape=(20, 1), dtype=float64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-9ded158d77f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn_average_deg_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-09b2ceaabad5>\u001b[0m in \u001b[0;36mlearn_average_deg_variable\u001b[0;34m(communities, group_size, seed_v, projection_dim, print_ratio, l_rate, mean, sd)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0meigenvec_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meigenvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    197\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[1;32m    198\u001b[0m                                 name=name)\n",
      "\u001b[0;32m/Users/xiangli/anaconda/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Argument is not a tf.Variable: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to optimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument is not a tf.Variable: Tensor(\"Square_54:0\", shape=(20, 1), dtype=float64)"
     ]
    }
   ],
   "source": [
    "learn_average_deg_variable(print_ratio=1, l_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "                \n",
    "mean_list = [i for i in np.linspace(-0.5, 0.5, 10)]\n",
    "l_rate_lst = [10**(-i)/3 for i in range(4, 10, 1)]\n",
    "\n",
    "\n",
    "\n",
    "for l in range(len(l_rate_lst)):\n",
    "    for k in range(len(mean_list)):\n",
    "            learn_average_deg_variable(communities = 2, group_size = 10, projection_dim=2, print_ratio=1, l_rate=l_rate_lst[l], mean=mean_list[k], sd=0.2)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.3333333333333335e-05, 3.3333333333333337e-06]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
